{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Building Solutions with Cloudera Data Services","text":"<p>Version 1.0.0</p> <p>Welcome to the Building Solutions with Cloudera Data Services. Follow along with your instructor to complete each section.</p>"},{"location":"index.html#synopsis","title":"Synopsis","text":"<p>This three-day course provides participants with a comprehensive understanding of the Cloudera platform and its integrated services, including Cloudera Data Warehouse, Cloudera Data Engineering, Cloudera Data Flow, and Cloudera AI. Participants will gain hands-on experience in designing, implementing, and optimizing data workflows and analytics solutions within the Cloudera ecosystem. </p> <p>The course emphasizes practical strategies for building scalable, secure, and efficient data-driven solutions tailored to enterprise needs. Key topics include data ingestion and processing, stream management, query optimization, machine learning integration, and managing resource performance in production environments.</p> <p></p>"},{"location":"index.html#who-should-take-this-course","title":"Who Should Take This Course?","text":"<p>This course is designed for data engineers, data analysts, application developers, and machine learning engineers who want a deeper understanding of how the Cloudera platform and its data services support solution development. This course assumes a foundational knowledge of data engineering principles (e.g., ETL concepts, data warehousing), analytics concepts (e.g., basic statistical analysis, data visualization), and cloud services (e.g., basic cloud computing models, service deployment). Basic familiarity with Linux environments (e.g., navigating the file system, using basic commands) and SQL (e.g., writing basic queries, understanding relational database concepts) is required. While some programming experience is helpful, this course focuses on practical application and does not require extensive coding skills. Prior experience with ETL, big data, and streaming technologies will greatly benefit participants.</p>"},{"location":"index.html#course-details","title":"Course Details","text":"<ul> <li> <p>Cloudera Data Flow</p> <ul> <li>Introduction to data ingestion and streaming capabilities</li> <li>Overview of NiFi, Kafka, and stream processing</li> <li>Hands-on Session: Creating and managing data flows</li> </ul> </li> <li> <p>Cloudera Data Engineering</p> <ul> <li>Introduction to Cloudera Data Engineering and Airflow</li> <li>Troubleshooting jobs and reviewing use cases</li> <li>Hands-on Session: Building Airflow DAGs</li> </ul> </li> <li> <p>Cloudera Data Warehouse</p> <ul> <li>Understanding Cloudera Data Warehouse for large-scale data analytics</li> <li>Introduction to Iceberg</li> <li>Hands-on Session: Building a data lakehouse</li> <li>Performance optimization and lakehouse maintenance</li> <li>Data visualization</li> </ul> </li> <li> <p>Cloudera AI &amp; Machine Learning</p> <ul> <li>Introduction to Cloudera Machine Learning</li> <li>Automating ML workflows and deploying models at scale</li> <li>Hands-on Session: Training and deploying a model using Cloudera AI</li> <li>MLOps pipeline and model monitoring</li> </ul> </li> <li> <p>Workshop: Stock Market Analysis with Alpha Vantage</p> <ul> <li>Participants will use Alpha Vantage APIs to fetch and analyze stock market data.</li> <li>Data Ingestion and Streaming: Using Cloudera Data Flow and Cloudera Data Engineering to process real-time stock data.</li> <li>Global Data Access: Storing and querying stock data with Cloudera Data Warehouse.</li> <li>Data Visualization: Leveraging Cloudera Data Visualization to create insightful dashboards and reports.</li> </ul> </li> </ul>"},{"location":"guide/index.html","title":"Table of Contents","text":"<p>01-01 Get Aboard 02-01 Using Cloudera Data Flow 03-01 Using Cloudera Data Engineering - Airflow 04-01 Using Cloudera Data Warehouse 05-01 Using Cloudera AI 06-01 Integrating Cloudera Data Services 07-01 Summary</p>"},{"location":"guide/01-01/index.html","title":"01-01 Get Aboard","text":"<p>The purpose of this module is introduce the course and the classroom environment.</p>"},{"location":"guide/01-01/index.html#1-introduction","title":"1. Introduction","text":""},{"location":"guide/01-01/index.html#2-course-objectives","title":"2. Course Objectives","text":""},{"location":"guide/01-01/index.html#3-student-introductions","title":"3. Student Introductions","text":""},{"location":"guide/01-01/index.html#4-introducing-cloudera","title":"4. Introducing Cloudera","text":""},{"location":"guide/01-01/index.html#5-introducing-cloudera-educational-services","title":"5. Introducing Cloudera Educational Services","text":""},{"location":"guide/01-01/index.html#6-gain-access-to-the-courseware","title":"6. Gain Access to the Courseware","text":""},{"location":"guide/01-01/index.html#7-exercises","title":"7. Exercises","text":""},{"location":"guide/01-01/index.html#8-summary","title":"8. Summary","text":""},{"location":"guide/01-02/index.html","title":"01-02 Getting Aboard","text":""},{"location":"guide/01-02/index.html#1-accessing-cloudera-management-console","title":"1. Accessing Cloudera Management Console","text":""},{"location":"guide/01-02/index.html#11-login-to-edu-keycloak","title":"1.1. Login to edu-keycloak","text":"<p>Get the edu-keycloak URL from the instructor, and log in using the provided username and a password.</p> <p></p>"},{"location":"guide/01-02/index.html#12-click-maybe-later","title":"1.2. Click 'Maybe Later'","text":"<p>You might be prompted to try out the new UI; however, the exercises were created based on the old UI. To ensure a consistent hands-on experience, please select the 'Maybe Later' button.</p> <p>You can also switch between the old and new UI using the toggle button in the top right corner.</p> <p></p>"},{"location":"guide/01-02/index.html#13-review-console","title":"1.3. Review Console","text":"<p>You will be presented with all the Cloudera Data Services tiles in the Cloudera Management Console homepage.</p> <p></p>"},{"location":"guide/01-02/index.html#2-access-the-cloudera-data-services-documentation","title":"2. Access the Cloudera Data Services Documentation","text":""},{"location":"guide/01-02/index.html#21-cloudera-data-services-docs","title":"2.1. Cloudera Data Services Docs","text":"<p>The Cloudera Data Services Documentation is located at https://docs.cloudera.com/?tab=cdp-public-cloud</p> <p></p>"},{"location":"guide/01-02/index.html#3-access-the-cloudera-blog-main-page","title":"3. Access the Cloudera Blog Main Page","text":"<p>In this part of the exercise, you will access the Cloudera Blog</p>"},{"location":"guide/01-02/index.html#31-the-cloudera-blog-main-page","title":"3.1. The Cloudera Blog Main Page","text":"<p>The Cloudera Blog is located at https://blog.cloudera.com. Here, you will find relevant and timely articles, videos, a blog search function, and the Cloudera Twitter feed.</p> <p></p>"},{"location":"guide/01-02/index.html#32-the-category-and-blog-filter-function","title":"3.2. The Category and Blog Filter Function","text":"<p>First, select a category from Business, Technical, or Culture and then use the Cloudera Blog filter function to narrow the scope of the articles you see. As you'll see in the next two slides, the content varies depending on which category you select.</p> <p></p>"},{"location":"guide/01-02/index.html#33-category-and-filter-functions","title":"3.3. Category and Filter Functions","text":"<p>Note the differences in articles retrieved by selecting the Technical category and Cloudera Data Services in the Filter By section that you saw in the previous slide.</p> <p></p>"},{"location":"guide/01-02/index.html#34-blog-category-and-filter-results","title":"3.4. Blog Category and Filter Results","text":"<p>This slide shows that selecting the Partners category and filtering articles by different Data Services.</p> <p></p>"},{"location":"guide/01-02/index.html#4-the-cloudera-community","title":"4. The Cloudera Community","text":""},{"location":"guide/01-02/index.html#41-the-cloudera-community","title":"4.1. The Cloudera Community","text":"<p>You can begin your Cloudera Community experience by scrolling the homepage. </p> <p></p>"},{"location":"guide/01-02/index.html#42-asking-questions-and-searching-for-answers","title":"4.2. Asking Questions and Searching for Answers","text":"<p>You can ask a question of the community or search for an answer that might already exist. Try the Advanced Search if you need to narrow your search.</p> <p></p>"},{"location":"guide/01-02/index.html#43-get-started","title":"4.3. Get Started","text":"<p>Click the Get Started link on the Cloudera Community main page to view some general information or to register as a new user.</p> <p></p>"},{"location":"guide/01-02/index.html#44-the-cloudera-support-portal-main-page","title":"4.4. The Cloudera Support Portal Main Page","text":"<p>From this Cloudera Support Portal main page, you may access documentation, downloads, training opportunities, and create and manage your support cases (Login required).</p> <p></p>"},{"location":"guide/01-02/index.html#5-end-of-exercise","title":"5. End of Exercise","text":""},{"location":"guide/02-01/index.html","title":"02-01 Using Cloudera Data Flow","text":"<p>The purpose of this module is to install Cloudera Data Platform Runtime.</p>"},{"location":"guide/02-01/index.html#1-introduction","title":"1. Introduction","text":""},{"location":"guide/02-01/index.html#2-cloudera-data-flow","title":"2. Cloudera Data Flow","text":""},{"location":"guide/02-01/index.html#3-apache-flink","title":"3. Apache Flink","text":""},{"location":"guide/02-01/index.html#4-nifi","title":"4. Nifi","text":""},{"location":"guide/02-01/index.html#5-concepts","title":"5. Concepts","text":""},{"location":"guide/02-01/index.html#6-architecture","title":"6. Architecture","text":""},{"location":"guide/02-01/index.html#7-use-cases","title":"7. Use Cases","text":""},{"location":"guide/02-01/index.html#8-exercises","title":"8. Exercises","text":""},{"location":"guide/02-01/index.html#9-summary","title":"9. Summary","text":""},{"location":"guide/02-02/index.html","title":"02-02 Accessing Environment","text":"<p>Get aboard the classroom environment for Cloudera DataFlow. This exercise will walk you through how to acquire the login URL and required credentials.</p>"},{"location":"guide/02-02/index.html#1-student-credentials","title":"1. Student Credentials","text":""},{"location":"guide/02-02/index.html#11-review-login-details","title":"1.1. Review login details","text":"<p>Review the student email for the log in credentials and the URL to the login page.</p> <p>Make a note of the workload username and password as you will need it from time to time throughout the exercises. </p> <p></p>"},{"location":"guide/02-02/index.html#12-lab-environment","title":"1.2. Lab Environment","text":"<p>This course provides a lab environment which is a Cloudera Public Cloud in which the following environments are pre-created:</p> <p>(1) Environment should be enabled as part of the CDF Data Service - [devops-570-class-{class-id}]  </p> <p>(2) Streams Messaging Data Hub Cluster should be created and running - [edu-ds-messaging-{class-id}]</p> <p>(3) Stream analytics Data Hub cluster should be created and running - [edu-ds-analytics-{class-id}]</p>"},{"location":"guide/02-02/index.html#2-download-zip-files","title":"2. Download Zip files","text":"<p>This module will need you to have access to a log data file. </p> <p>Click here to download the Syslog to Kafka json file.</p>"},{"location":"guide/02-02/index.html#3-accessing-cloudera-data-flow","title":"3. Accessing Cloudera Data Flow","text":""},{"location":"guide/02-02/index.html#31-login-to-edu-keycloak","title":"3.1. Login to edu-keycloak","text":"<p>Get the edu-keycloak URL from the instructor, and log in using the provided username and a password. You will be able to copy and paste in both the user ID and the password.</p> <p></p>"},{"location":"guide/02-02/index.html#32-review-home-page","title":"3.2. Review Home Page","text":"<p>You should be able to get the following home page of Cloudera Public Cloud.</p> <p>Select the Data Flow tile from the Cloudera Public Cloud Enterprise Data web interface.</p> <p></p>"},{"location":"guide/02-02/index.html#4-tour-the-home-page","title":"4. Tour the Home Page","text":""},{"location":"guide/02-02/index.html#41-view-the-navigation-panel","title":"4.1. View the Navigation Panel","text":"<p>The Main navigation panel enables access to the following Cloudera DataFlow functions:</p> <ul> <li>Deployments</li> <li>Catalog</li> <li>ReadyFlow Gallery</li> <li>Flow Design</li> <li>Projects</li> <li>Resources</li> <li>Functions</li> <li>Environments</li> </ul> <p></p>"},{"location":"guide/02-02/index.html#42-review-overview-page","title":"4.2. Review Overview page","text":"<p>The Overview page lists access to various functions, including:</p> <ul> <li>Build Flows</li> <li>Organize &amp; Control Access</li> <li>Import existing flows</li> <li>Deploy a ReadyFlow</li> </ul> <p></p>"},{"location":"guide/02-02/index.html#5-end-of-exercise","title":"5. End of Exercise","text":""},{"location":"guide/02-03/index.html","title":"02-03 Define Workload Password","text":"<p>Note</p> <p>You will need to define your workload password that will be used to access non-SSO interfaces. Please keep a note of this workload password. If you have forgotten it, you will be able to repeat this process and define another one.</p> <p>You may read more about workload passwords here.</p>"},{"location":"guide/02-03/index.html#1-set-workload-password","title":"1. Set Workload Password","text":"<p>Let's set the workload password</p>"},{"location":"guide/02-03/index.html#11-click-on-username","title":"1.1. Click on Username","text":"<p>From main menu, select your Username option from the panel.</p> <p></p>"},{"location":"guide/02-03/index.html#12-select-profile","title":"1.2. Select Profile","text":"<p>Click on Profile option under the username. </p> <p></p>"},{"location":"guide/02-03/index.html#13-click-set-workload-password","title":"1.3. Click Set Workload Password","text":"<p>Review the profile page for your user. </p> <p>Click on the Set Workload Password option</p> <p></p>"},{"location":"guide/02-03/index.html#14-enter-password","title":"1.4. Enter Password","text":"<p>Enter your workload password received from the instructor and noted in the previous exercise.</p> <p>Click on Set Workload Password button.</p> <p></p>"},{"location":"guide/02-03/index.html#15-verify-password-set","title":"1.5. Verify password set","text":"<p>A success message appears confirming that Workload password is updated and it is being synced to environments. </p> <p>Alternatively, look for a message next to Set <code>Workload Password</code> which says <code>(Workload password is currently set).</code></p> <p></p>"},{"location":"guide/02-03/index.html#2-end-of-the-exercise","title":"2. End of the Exercise","text":""},{"location":"guide/02-04/index.html","title":"02-04 Verify permissions in Apache Ranger","text":"<p>Note</p> <p>THESE STEPS HAVE ALREADY BEEN DONE FOR YOU. This section will walk you through how Permissions/Policies are managed in Ranger. </p>"},{"location":"guide/02-04/index.html#_1","title":"02-04 Verify permissions in Apache Ranger","text":"<p>Caution</p> <p>PLEASE DO NOT EXECUTE THE STEPS IN THIS SECTION OR CHANGE ANYTHING.</p> <p>Important</p> <p>Since we could not grant the required privileges to the workload users to implement the steps in the environment, this exercise has been written as a read-only guide for your future reference. Please go through all the steps, screenshots, policies, permissions in detail. You won't be able to view the policies in your environment. You may request your instructor to provide a short demo.  </p>"},{"location":"guide/02-04/index.html#1-accessing-apache-ranger","title":"1. Accessing Apache Ranger","text":""},{"location":"guide/02-04/index.html#11-click-on-environments","title":"1.1. Click on Environments","text":"<p>Click on the <code>Environments</code> tab on the left pane in Cloudera Management Console Main panel. </p> <p></p>"},{"location":"guide/02-04/index.html#12-navigate-to-management-console","title":"1.2. Navigate to Management Console","text":"<p>Select the environment that is shared by the instructor (Ex: <code>devops-570-class-250204</code>).</p> <p>The environment assigned to you would be named similar except the class ID. </p> <p></p>"},{"location":"guide/02-04/index.html#13-access-ranger-ui","title":"1.3. Access Ranger UI","text":"<p>Click on the Ranger quick link to access the Ranger UI.</p> <p>Also, make a note of the 3 Data Hubs pre-created for the exercises. We will need them in later steps.</p> <p>(1) Environment should be enabled as part of the CDF Data Service - [devops-570-class-250204]</p> <p>(2) Streams Messaging Data Hub Cluster should be created and running - [edu-ds-messaging-250204]</p> <p>(3) Stream analytics Data Hub cluster should be created and running - [edu-ds-analytics-250204]</p> <p>(4) A gateway node to access the services - [training-gateway-250204]</p> <p>In your case, the class ID would be different:</p> <ul> <li>[devops-570-class-{class-id}]   </li> <li>[edu-ds-messaging-{class-id}]</li> <li>[edu-ds-analytics-{class-id}]</li> <li>[training-gateway-{class-id}]</li> </ul> <p></p>"},{"location":"guide/02-04/index.html#14-review-ranger-ui","title":"1.4. Review Ranger UI","text":"<p>Notice the two data hubs are populated under KAFKA and KAFKA-CONNECT. </p> <p></p>"},{"location":"guide/02-04/index.html#2-kafka-permissions","title":"2. Kafka Permissions","text":""},{"location":"guide/02-04/index.html#21-select-the-kafka-repository","title":"2.1. Select the Kafka repository","text":"<p>In Ranger UI, select the Kafka repository that\u2019s associated with the stream messaging datahub.</p> <p>In this case, it will be [edu-ds-messaging 250204_kafka_716a]</p> <p></p>"},{"location":"guide/02-04/index.html#22-verify-user","title":"2.2. Verify user","text":"<p>Verify if the user who will be performing the exercise is present in both <code>all-consumergroup</code> and <code>all-topic</code>.</p> <p>A variable {USER} with all the required permissions in Apache Ranger has been added. This variable {USER} is used to represent each student in this environment in a Ranger Policy. </p> <p>The below image reflects the variable {USER} being part of <code>all-consumergroup</code>.</p> <p></p>"},{"location":"guide/02-04/index.html#23-verify-user","title":"2.3. Verify user","text":"<p>The below image reflects the variable {USER} being part of <code>all-topic</code>.</p> <p></p>"},{"location":"guide/02-04/index.html#3-schema-registry-permissions","title":"3. Schema Registry Permissions","text":"<p>Schema Registry Permissions</p>"},{"location":"guide/02-04/index.html#31-navigate-to-ranger-home-page","title":"3.1. Navigate to Ranger Home Page","text":"<p>Click on the Ranger icon in the top left corner to navigate back to the Ranger home page. </p> <p></p>"},{"location":"guide/02-04/index.html#32-select-schema-registry","title":"3.2. Select SCHEMA-REGISTRY","text":"<p>In Ranger, select the <code>SCHEMA-REGISTRY</code> repository that\u2019s associated with the stream messaging datahub.</p> <p>In this case, it will be [edu_ds_messaging_250204_schemaregistry]</p> <p></p>"},{"location":"guide/02-04/index.html#33-verify-user","title":"3.3. Verify user","text":"<p>Verify if the user who will be performing the exercise is present in the Policy: all - <code>schema-group, schema-metadata, schema-branch, schema-version</code>.</p> <p>A variable {USER} with all the required permissions in Apache Ranger has been added. This variable {USER} is used to represent each student in this environment in a Ranger Policy.</p> <p></p>"},{"location":"guide/02-04/index.html#34-click-on-edit","title":"3.4. Click on Edit","text":"<p>Click on the Edit button under the Actions column to edit the policy. </p> <p></p>"},{"location":"guide/02-04/index.html#35-review-policy-details","title":"3.5. Review Policy Details","text":"<p>Review the policy details. </p> <p></p>"},{"location":"guide/02-04/index.html#36-review-allow-conditions","title":"3.6. Review Allow Conditions","text":"<p>Verify if the user who will be performing the exercise is present in the Selected Users column. </p> <p>Review the policy permissions. </p> <p>Exit the Ranger UI by closing the tab. </p> <p></p>"},{"location":"guide/02-04/index.html#4-end-of-the-exercise","title":"4. End of the Exercise","text":""},{"location":"guide/02-05/index.html","title":"02-05 Create a Flow using Flow Designer","text":"<p>Creating a data flow for CDF-PC is the same process as creating any data flow within Nifi with 3 very important steps.</p> <p>(a) The data flow that would be used for CDF-PC must be self-contained within a process group.</p> <p>(b) Data flows for CDF-PC must use parameters for any property on a processor that is modifiable, e.g. user names, Kafka topics, etc.</p> <p>(c) All queues need to have meaningful names (instead of Success, Fail, and Retry). These names will be used to define Key Performance Indicators in CDF-PC.</p> <p>Let's build a data flow using flow designer.</p>"},{"location":"guide/02-05/index.html#1-create-canvas","title":"1. Create Canvas","text":""},{"location":"guide/02-05/index.html#we-will-start-by-creating-the-canvas-to-design-the-flow","title":"We will start by creating the canvas to design the flow","text":""},{"location":"guide/02-05/index.html#11-select-dataflow","title":"1.1. Select DataFlow","text":"<p>Access the <code>DataFlow</code> data service from the Management Console.</p> <p></p>"},{"location":"guide/02-05/index.html#12-click-on-flow-design","title":"1.2. Click on Flow Design","text":"<p>From the main navigation panel, select the Flow Design option. </p> <p></p>"},{"location":"guide/02-05/index.html#13-click-on-create-draft","title":"1.3. Click on Create Draft","text":"<p>Click on <code>Create Draft.</code> This will be the main process group for the flow that you\u2019ll create.</p> <p></p>"},{"location":"guide/02-05/index.html#14-name-the-draft","title":"1.4. Name the Draft","text":"<ul> <li> <p>Select the appropriate environment as part of the Target Workshop name (Ex: <code>devops-570-class-250204</code>).</p> </li> <li> <p>Let the Target Project be Unassigned.</p> </li> <li> <p>Give your flow a name with your username as prefix (Ex: <code>dse_2_250204_datadump_flow</code>).</p> </li> <li> <p>Click on Create button.</p> </li> </ul> <p></p>"},{"location":"guide/02-05/index.html#15-review-canvas","title":"1.5. Review Canvas","text":"<p>On successful creation of the Draft, you should now be redirected to the canvas on which you can design your flow.</p> <p></p>"},{"location":"guide/02-05/index.html#2-adding-new-parameters","title":"2. Adding new parameters","text":"<p>Configure Parameters: Parameters are reused within the flow multiple times and will also be configurable at the time of deployment. </p> <p>There are 2 options available: <code>Add Parameter</code>, which is used for specifying non-sensitive values and <code>Add Sensitive Parameter</code>, which is used for specifying sensitive parameters like password.</p>"},{"location":"guide/02-05/index.html#21-click-on-the-flow-options","title":"2.1. Click on the Flow Options","text":"<p>Click on the <code>Flow Options</code> on the top right corner of your canvas. and then select <code>Parameters</code>.</p> <p></p>"},{"location":"guide/02-05/index.html#22-click-on-add-parameter","title":"2.2. Click on Add Parameter","text":""},{"location":"guide/02-05/index.html#23-create-first-parameter","title":"2.3. Create first parameter","text":"<p>Create a variable with the following entries:</p> <ul> <li>Name: <code>S3 Directory</code>.</li> <li>Value: <code>LabData</code>.</li> </ul> <p>Click Save</p> <p></p>"},{"location":"guide/02-05/index.html#24-review-first-parameter","title":"2.4. Review first parameter","text":""},{"location":"guide/02-05/index.html#25-click-on-add-parameter","title":"2.5. Click on Add Parameter","text":""},{"location":"guide/02-05/index.html#26-create-second-parameter","title":"2.6. Create second parameter","text":"<p>Create a variable with the following entries:</p> <ul> <li>Name: <code>CDP Workload User</code>.</li> <li>Value: <code>The username assigned to you Ex: dse_2_250204</code>.</li> </ul> <p>Click Save</p> <p></p>"},{"location":"guide/02-05/index.html#27-review-second-parameter","title":"2.7. Review second parameter","text":""},{"location":"guide/02-05/index.html#28-click-on-add-sensitive-parameter","title":"2.8. Click on Add Sensitive Parameter","text":""},{"location":"guide/02-05/index.html#29-create-third-parameter","title":"2.9. Create third parameter","text":"<p>Create a variable with the following entries:</p> <ul> <li>Name: <code>CDP Workload User Password</code>.</li> <li>Value: <code>Workload User password set by you earlier in exercise 02-03 Define Workload Password</code>.</li> </ul> <p>Click Save</p> <p></p>"},{"location":"guide/02-05/index.html#210-review-third-parameter","title":"2.10. Review third parameter","text":""},{"location":"guide/02-05/index.html#211-click-apply-changes","title":"2.11. Click Apply Changes","text":""},{"location":"guide/02-05/index.html#212-click-ok","title":"2.12. Click OK","text":""},{"location":"guide/02-05/index.html#213-review-all-parameters","title":"2.13. Review all Parameters","text":"<p>All the parameters are applied and listed under the All Parameters tab.</p> <p>Now that we have created these parameters, we can easily search and reuse them within our dataflow. This is useful for <code>CDP Workload User</code> and <code>CDP Workload User Password</code>.</p> <p></p>"},{"location":"guide/02-05/index.html#3-create-the-flow","title":"3. Create the flow","text":"<p>Let\u2019s go back to the canvas to start designing our flow. This flow will contain 2 Processors:</p> <ul> <li><code>GenerateFlowFile</code>: Generates random data.</li> <li><code>PutCDPObjectStore</code>: Loads data into HDFS(S3).</li> </ul>"},{"location":"guide/02-05/index.html#31-navigate-to-flow-designer","title":"3.1. Navigate to Flow Designer","text":""},{"location":"guide/02-05/index.html#32-pull-the-processor","title":"3.2. Pull the Processor","text":"<p>Click on Processor and drag it onto the canvas</p> <p></p>"},{"location":"guide/02-05/index.html#33-add-generateflowfile-processor","title":"3.3. Add GenerateFlowFile processor","text":"<p>Type <code>GenerateFlowFile</code> in the text box, and once the processor appears click on Add.</p> <p></p>"},{"location":"guide/02-05/index.html#34-review-processor","title":"3.4. Review processor","text":"<p>The <code>GenerateFlowFile</code> Processor will now be on your canvas. </p> <p>Reposition the processor appropriately if required by dragging it over the canvas.</p> <p></p>"},{"location":"guide/02-05/index.html#35-configure-generateflowfile-processor","title":"3.5. Configure GenerateFlowFile processor","text":"<p>Right click on the processor and select Configuration. </p> <p></p>"},{"location":"guide/02-05/index.html#36-fill-in-the-values","title":"3.6. Fill in the values","text":"<p>Fill in the values in the right window pane to configure the processor in the following way.</p> <p><code>Processor Name</code>: <code>DataGenerator</code></p> <p><code>Scheduling Strategy</code>: <code>Timer Driven</code></p> <p></p>"},{"location":"guide/02-05/index.html#37-fill-in-the-values","title":"3.7. Fill in the values","text":"<p><code>Run Duration</code>: <code>0ms</code></p> <p><code>Run Schedule</code>: <code>1 min</code></p> <p><code>Execution</code>: <code>All Nodes</code></p> <p><code>Properties</code>: <code>Custom Text</code></p> <p></p>"},{"location":"guide/02-05/index.html#38-fill-in-the-values","title":"3.8. Fill in the values","text":"<pre><code>&lt;26&gt;1 2021-09-21T21:32:43.967Z host1.example.com application4 3064 ID42 [exampleSDID@873 iut=\"4\" eventSource=\"application\" eventId=\"58\"] application4 has\nstopped unexpectedly\n</code></pre> <p>The above represents a syslog out in RFC5424 format. Subsequent portions of these exercises under this module will leverage this same syslog format.</p> <p>Click Apply</p> <p></p>"},{"location":"guide/02-05/index.html#39-review-datagenerator-processor","title":"3.9. Review DataGenerator Processor","text":"<p>Review DataGenerator Processor on the canvas. </p> <p></p>"},{"location":"guide/02-05/index.html#310-pull-second-processor","title":"3.10. Pull second Processor","text":"<p>Pull a new <code>Processor</code> onto the canvas </p> <p></p>"},{"location":"guide/02-05/index.html#311-add-putcdpobjectstore-processor","title":"3.11. Add PutCDPObjectStore processor","text":"<p>Type <code>PutCDPObjectStore</code> in the text box, and once the processor appears click on <code>Add</code>.</p> <p></p>"},{"location":"guide/02-05/index.html#312-review-processor","title":"3.12. Review processor","text":"<p>The <code>PutCDPObjectStore</code> processor will now be on your canvas. </p> <p>Reposition the processor appropriately if required by dragging it over the canvas.</p> <p></p>"},{"location":"guide/02-05/index.html#313-configure-putcdpobjectstore-processor","title":"3.13. Configure PutCDPObjectStore processor","text":"<p>Right click on the processor and select Configuration. </p> <p>Fill in the values in the right window pane to configure the processor in the following way.</p> <p><code>Processor Name</code> : <code>Move2S3</code></p> <p><code>Scheduling Strategy</code> : <code>Timer Driven</code></p> <p><code>Run Duration</code> : <code>0ms</code></p> <p><code>Run Schedule</code> : <code>0 sec</code></p> <p><code>Execution</code> : <code>All Nodes</code></p> <p></p>"},{"location":"guide/02-05/index.html#314-fill-in-the-values","title":"3.14. Fill in the values","text":"<p>Click on No Value set for in front of Directory under Property.</p> <p></p>"},{"location":"guide/02-05/index.html#315-fill-directory","title":"3.15. Fill Directory","text":"<p><code>Directory</code> : #{S3 Directory}</p> <p></p>"},{"location":"guide/02-05/index.html#316-fill-properties","title":"3.16. Fill properties","text":"<p>Similarly, fill the below properties</p> <p><code>CDP Username</code> : #{CDP Workload User}</p> <p><code>CDP Password</code> : #{CDP Workload User Password}</p> <p><code>Relationships</code>: Check the <code>Terminate</code> box under <code>success</code>.</p> <p>Click on **Apply. **</p> <p></p>"},{"location":"guide/02-05/index.html#317-create-connection-between-processors","title":"3.17. Create connection between processors","text":"<p>Review <code>Move2S3</code> processor on the canvas. </p> <p>Connect the two processors by dragging the arrow from <code>DataGenerator</code> processor to the <code>Move2S3</code> processor. </p> <p></p>"},{"location":"guide/02-05/index.html#318-drag-the-arrow","title":"3.18. Drag the arrow","text":""},{"location":"guide/02-05/index.html#319-connect-arrow","title":"3.19. Connect arrow","text":""},{"location":"guide/02-05/index.html#320-select-success","title":"3.20. Select Success","text":"<p>Select on <code>success</code> relationship checkbox. </p> <p>Click Add.</p> <p></p>"},{"location":"guide/02-05/index.html#321-review-flow","title":"3.21. Review Flow","text":"<p>Your flow should look something like this. </p> <p></p>"},{"location":"guide/02-05/index.html#322-add-a-queue","title":"3.22. Add a queue","text":"<p>The <code>Move2S3</code> processor does not know what to do in case of a failure. Let\u2019s add a retry queue to it. This can be done by dragging the arrow on the <code>Move2S3</code> processor outwards then back to itself, as shown below.</p> <p></p>"},{"location":"guide/02-05/index.html#323-select-failure","title":"3.23. Select Failure","text":"<p>Select on <code>failure</code> relationship checkbox.</p> <p>Click Add.</p> <p></p>"},{"location":"guide/02-05/index.html#324-review-flow","title":"3.24. Review flow","text":""},{"location":"guide/02-05/index.html#4-renaming-the-queues","title":"4. Renaming the queues","text":"<p>Info</p> <p>Naming the queue: Providing unique names to all queues is very important as they are used to define Key Performance Indicators (KPI) upon which CDF-PC will auto scale. To name a queue, double-click the queue and give it a unique name. A best practice here is to start the existing queue name (i.e. success, failure, retry, etc\u2026) and add the source and destination processor information.</p>"},{"location":"guide/02-05/index.html#41-rename-success-queue","title":"4.1. Rename Success Queue","text":"<p>Click on Success Queue. In the Connection Name field, rename the queue as <code>success_Move2S3.\ufeff</code></p> <p>Click **Apply. **</p> <p></p>"},{"location":"guide/02-05/index.html#42-rename-failure-queue","title":"4.2. Rename Failure Queue","text":"<p>Click on failure Queue. In the Connection Name field, rename the queue as <code>failure_Move2S3.</code></p> <p>Click **Apply. **</p> <p></p>"},{"location":"guide/02-05/index.html#43-review-flow","title":"4.3. Review flow","text":""},{"location":"guide/02-05/index.html#5-end-of-the-exercise","title":"5. End of the Exercise","text":""},{"location":"guide/02-06/index.html","title":"02-06 Testing the flow","text":"<p>Testing the Data Flow: To test the flow we need to first start the test session.</p>"},{"location":"guide/02-06/index.html#1-testing-the-data-flow","title":"1. Testing the Data Flow","text":""},{"location":"guide/02-06/index.html#11-click-on-flow-options","title":"1.1. Click on Flow Options","text":"<p>Click on <code>Flow Options</code> on the top right corner and then click <code>Start</code> under <code>Test Session</code> section.</p> <p></p>"},{"location":"guide/02-06/index.html#12-click-start-test-session","title":"1.2. Click Start Test Session","text":"<p>In the next window, click <code>Start Test Session</code>.</p> <p></p>"},{"location":"guide/02-06/index.html#13-wait-a-couple-of-mins","title":"1.3. Wait a couple of mins","text":""},{"location":"guide/02-06/index.html#14-initializing-test-session","title":"1.4. Initializing Test Session","text":"<p>The activation should take about a couple of minutes. While this happens, you will see this at the top right corner of your screen.</p> <p></p>"},{"location":"guide/02-06/index.html#15-active-test-session","title":"1.5. Active Test Session","text":"<p>Once the Test Session is ready you will see the Active Test Session in green button. </p> <p></p>"},{"location":"guide/02-06/index.html#2-setup-s3-dir-access","title":"2. Setup S3 dir access","text":"<p>Setup the command line access for the S3 LadData directory. </p>"},{"location":"guide/02-06/index.html#21-click-on-menu","title":"2.1. Click on Menu","text":"<p>Select the Menu option by clicking on the 9 dots. </p> <p></p>"},{"location":"guide/02-06/index.html#22-select-management-console","title":"2.2. Select Management Console","text":"<p>Navigate to the Management Console page by clicking the Management Console tile.</p> <p></p>"},{"location":"guide/02-06/index.html#23-select-environment","title":"2.3. Select Environment","text":"<p>Click on the environment. </p> <p></p>"},{"location":"guide/02-06/index.html#24-select-gateway","title":"2.4. Select Gateway","text":"<p>Click on training gateway</p> <p></p>"},{"location":"guide/02-06/index.html#25-review-page","title":"2.5. Review page","text":"<p>Review the details for training gateway. </p> <p>Scroll down the page. </p> <p></p>"},{"location":"guide/02-06/index.html#26-click-on-nodes","title":"2.6. Click on Nodes","text":"<p>Click on the Nodes option in the left hand side pane. </p> <p></p>"},{"location":"guide/02-06/index.html#27-copy-public-ip","title":"2.7. Copy Public IP","text":"<p>Click on the Copy to Clipboard button next to Public IP. </p> <p></p>"},{"location":"guide/02-06/index.html#28-access-the-gateway","title":"2.8. Access the gateway","text":"<p>Open a terminal on your device and run the ssh command to access the gateway. </p> <pre><code>ssh username@&lt;training-gateway-public-up&gt;\n</code></pre> <p></p>"},{"location":"guide/02-06/index.html#29-enter-password","title":"2.9. Enter password","text":"<p>Enter password for your username. </p> <p></p>"},{"location":"guide/02-06/index.html#210-login-successful","title":"2.10. Login Successful","text":"<p>After the password, you should be able to successfully access the gateway. </p> <p></p>"},{"location":"guide/02-06/index.html#3-fetching-location","title":"3. Fetching location","text":""},{"location":"guide/02-06/index.html#31-navigate-to-training-gateway","title":"3.1. Navigate to training gateway","text":"<p>Navigate back to training gateway page. </p> <p>Click on Environment under Environment Details. </p> <p></p>"},{"location":"guide/02-06/index.html#32-click-on-summary","title":"3.2. Click on Summary","text":"<p>Scroll down on the Summary Tab</p> <p></p>"},{"location":"guide/02-06/index.html#33-copy-the-location","title":"3.3. Copy the location","text":"<p>Copy the location until datalake</p> <p>For example: s3a://cdp-storage-devops-570-class-250204</p> <p></p>"},{"location":"guide/02-06/index.html#34-list-files","title":"3.4. List files","text":"<p>Run the following command to list the files under your user. You should not see LabData directory just yet.</p> <pre><code>hdfs dfs -ls s3a://cdp-storage-devops-570-class-250204/user/dse_2_250204/\n</code></pre> <pre><code>Syntax: hdfs dfs -ls &lt;storage-location&gt;/user/&lt;username&gt;/\n</code></pre> <p></p>"},{"location":"guide/02-06/index.html#4-run-the-flow","title":"4. Run the flow","text":""},{"location":"guide/02-06/index.html#41-click-on-menu","title":"4.1. Click on Menu","text":"<p>Select the Menu option by clicking on the 9 dots. </p> <p></p>"},{"location":"guide/02-06/index.html#42-select-dataflow","title":"4.2. Select DataFlow","text":"<p>Navigate to the Cloudera DataFlow page by clicking the DataFlow tile in the Cloudera management console.</p> <p></p>"},{"location":"guide/02-06/index.html#43-select-flow-design","title":"4.3. Select Flow Design","text":"<p>Select the Flow Design option. </p> <p></p>"},{"location":"guide/02-06/index.html#44-select-draft","title":"4.4. Select Draft","text":"<p>Click on the Draft Name</p> <p></p>"},{"location":"guide/02-06/index.html#45-run-the-flow","title":"4.5. Run the flow","text":"<p>Run the flow by right clicking the <code>empty part</code> of the canvas and selecting <code>Start</code>.</p> <p></p>"},{"location":"guide/02-06/index.html#46-review-the-processor-state","title":"4.6. Review the processor state","text":"<p>Both the processors should now be in the <code>Start</code> state. This can be confirmed by looking at the green play button against each processor.</p> <p></p>"},{"location":"guide/02-06/index.html#5-verify-contents-under-labdata","title":"5. Verify contents under /LabData","text":""},{"location":"guide/02-06/index.html#51-list-the-files","title":"5.1. List the files","text":"<p>Run the same command as executed earlier to list the files under your user. You should now see LabData directory</p> <pre><code>hdfs dfs -ls s3a://cdp-storage-devops-570-class-250204/user/dse_2_250204/\n</code></pre> <pre><code>Syntax: hdfs dfs -ls &lt;storage-location&gt;/user/&lt;username&gt;/\n</code></pre> <pre><code>hdfs dfs -ls s3a://cdp-storage-devops-570-class-250204/user/dse_2_250204/LabData\n</code></pre> <pre><code>Syntax: hdfs dfs -ls &lt;storage-location&gt;/user/&lt;username&gt;/LabData/\n</code></pre> <p></p>"},{"location":"guide/02-06/index.html#6-end-of-the-exercise","title":"6. End of the Exercise","text":""},{"location":"guide/02-07/index.html","title":"02-07 Moving the flow to the flow catalog","text":"<p>After the flow has been created and tested, we can now Publish the flow to the Flow Catalog.</p>"},{"location":"guide/02-07/index.html#1-publish-the-flow","title":"1. Publish the Flow","text":""},{"location":"guide/02-07/index.html#11-stop-test-session","title":"1.1. Stop Test Session","text":"<p>Stop the current test session by clicking on the green tab on top right corner indicating <code>Active Test Session</code>. </p> <p></p>"},{"location":"guide/02-07/index.html#12-click-on-end","title":"1.2. Click on End.","text":""},{"location":"guide/02-07/index.html#13-review-session-status","title":"1.3. Review Session Status","text":"<p>Keep a check on the Test Session status as it turns red. </p> <p></p>"},{"location":"guide/02-07/index.html#14-publish-flow","title":"1.4. Publish Flow","text":"<p>Once the session stops click on <code>Flow Options</code> on the top right corner of your screen and click on <code>Publish</code>.</p> <p></p>"},{"location":"guide/02-07/index.html#15-name-the-flow","title":"1.5. Name the flow","text":"<p>Give your flow a unique name and click on <code>Publish</code>.</p> <p><code>Flow Name</code>: <code>{user_id}_datadump_flow</code> (Ex: <code>dse_2_250204_datadump_flow</code>).</p> <p></p>"},{"location":"guide/02-07/index.html#16-review-success-message","title":"1.6. Review Success Message","text":"<p>The flow will now be visible on the Flow <code>Catalog</code> and is ready to be deployed.</p> <p></p>"},{"location":"guide/02-07/index.html#2-deploying-the-flow","title":"2. Deploying the flow","text":""},{"location":"guide/02-07/index.html#21-click-on-menu","title":"2.1. Click on Menu","text":"<p>Select the Catalog option. </p> <p></p>"},{"location":"guide/02-07/index.html#22-search-for-the-flow-catalog","title":"2.2. Search for the Flow Catalog","text":"<p>Search for the <code>Flow Catalog</code> by typing the name of the flow that you just now published. </p> <p>Click on the flow. </p> <p></p>"},{"location":"guide/02-07/index.html#23-deploy-flow","title":"2.3. Deploy Flow","text":"<p>You should see the option to <code>Deploy</code>. Verify 'Version 1' and then click on <code>Deploy</code>. </p> <p></p>"},{"location":"guide/02-07/index.html#24-click-continue","title":"2.4. Click Continue","text":"<p>The target workspace should be pre-selected for you. </p> <p>Click Continue.</p> <p></p>"},{"location":"guide/02-07/index.html#25-name-the-deployment","title":"2.5. Name the Deployment","text":"<p>Give a unique name to the deployment.</p> <p>Deployment Name: <code>{user_id}_flow_prod</code></p> <p>(Ex: <code>dse_2_250204_flow_prod</code>).</p> <p>Click Next</p> <p></p>"},{"location":"guide/02-07/index.html#26-set-nifi-configuration","title":"2.6. Set Nifi Configuration","text":"<p>In this step we let everything be the default and click Next.</p> <p></p>"},{"location":"guide/02-07/index.html#27-set-the-parameters","title":"2.7. Set the Parameters","text":"<p>Set the parameters are following:</p> <ul> <li>CDP Workload User: <code>The username assigned to you</code>. </li> <li>Ex: <code>dse_2_250204</code>.</li> <li>CDP Workload User Password: <code>Workload User password set by you earlier in exercise 02-03 'Define Workload Password'</code>.</li> <li>S3 Directory: <code>LabData</code></li> </ul> <p></p>"},{"location":"guide/02-07/index.html#28-set-the-cluster-size","title":"2.8. Set the cluster size","text":"<p>Select the <code>Extra Small</code> size. In this step you can configure how your flow will auto scale, but keep it disabled for this lab. Select the standard storage selection. </p> <p>Click Next.</p> <p></p>"},{"location":"guide/02-07/index.html#29-add-key-performance-indicators","title":"2.9. Add Key Performance indicators","text":"<p>Set up KPIs to track specific performance metrics of a deployed flow.</p> <p>Click on Add New KPI.</p> <p></p>"},{"location":"guide/02-07/index.html#210-fill-in-the-details","title":"2.10. Fill in the details","text":"<p>In the <code>Add New KPI</code> window, fill in the details as below.</p> <ul> <li>KPI Scope: <code>Connection</code></li> <li>Connection Name: <code>failure_Move2S3</code></li> <li>Metric to Track: <code>Percent Full</code></li> <li>Check box against <code>Trigger alert when metric is greater than</code>: <code>50Percent</code>.</li> </ul> <p><code>Alert will be triggered when metric is outside the boundary(s) for</code>: <code>2Minutes</code>.</p> <p>Click Add.</p> <p></p>"},{"location":"guide/02-07/index.html#211-review-kpi","title":"2.11. Review KPI","text":"<p>Click Next. </p> <p></p>"},{"location":"guide/02-07/index.html#212-click-deploy","title":"2.12. Click Deploy","text":""},{"location":"guide/02-07/index.html#213-review-deployment-initiated-message","title":"2.13. Review Deployment Initiated message","text":"<p>The Deployment Initiated message will be displayed. Wait until the flow deployment is completed, which might take a few minutes. </p> <p></p>"},{"location":"guide/02-07/index.html#214-review-deployment","title":"2.14. Review Deployment","text":"<p>When deployed, the flow will show up on the Data flow dashboard, as below.</p> <p></p>"},{"location":"guide/02-07/index.html#215-list-the-files","title":"2.15. List the files","text":"<p>The data gets populated in the S3 bucket. </p> <p>Run the same command as executed earlier to list the files under your user. </p> <pre><code>hdfs dfs -ls s3a://cdp-storage-devops-570-class-250204/user/dse_2_250204/LabData\n</code></pre> <pre><code>Syntax: hdfs dfs -ls &lt;storage-location&gt;/user/&lt;username&gt;/LabData/\n</code></pre> <p></p>"},{"location":"guide/02-07/index.html#216-click-on-flow","title":"2.16. Click on Flow","text":"<p>Click in the empty area in the left hand side to take a look at the flow. </p> <p></p>"},{"location":"guide/02-07/index.html#217-review-flow","title":"2.17. Review Flow","text":"<p>After a while you will see the flow something like below for the flow you just deployed.</p> <p></p>"},{"location":"guide/02-07/index.html#3-verifying-flow-deployment","title":"3. Verifying flow deployment","text":""},{"location":"guide/02-07/index.html#31-select-manage-deployment","title":"3.1. Select Manage Deployment","text":"<p>Click on the flow in the Dashboard and select <code>Manage Deployment</code> under Actions button. </p> <p></p>"},{"location":"guide/02-07/index.html#32-review-deployment","title":"3.2. Review Deployment","text":""},{"location":"guide/02-07/index.html#33-review-kpi-and-alerts","title":"3.3. Review KPI and Alerts","text":"<p>Click on the <code>KPI and Alerts</code> tab under <code>Deployment Settings</code> to get the list of KPIs that have been set. You also have an option to modify or add more KPIs to your flow here.</p> <p></p>"},{"location":"guide/02-07/index.html#34-review-sizing-and-scaling","title":"3.4. Review Sizing and Scaling","text":"<p>Click on the <code>Sizing and Scaling</code> tab to get detailed information.</p> <p></p>"},{"location":"guide/02-07/index.html#35-review-parameters","title":"3.5. Review Parameters","text":"<p>The parameters that we earlier created can be managed from the Parameters tab. Click on <code>Parameters</code>.</p> <p></p>"},{"location":"guide/02-07/index.html#36-review-nifi-configurations","title":"3.6. Review NiFi Configurations","text":"<p>If you have set any configuration w.r.t to Nifi they will show up on the <code>NiFi Configuration</code> tab.</p> <p></p>"},{"location":"guide/02-07/index.html#37-view-in-nifi","title":"3.7. View in NiFi","text":"<p>Click on <code>Actions</code> and then click on <code>View in NiFi</code>. This will open the flow in the Nifi UI.</p> <p></p>"},{"location":"guide/02-07/index.html#38-review-flow-in-nifi-ui","title":"3.8. Review flow in Nifi UI","text":""},{"location":"guide/02-07/index.html#4-suspend-flow","title":"4. Suspend flow","text":"<p>We will now suspend this flow. </p>"},{"location":"guide/02-07/index.html#41-click-on-suspend-deployment","title":"4.1. Click on Suspend Deployment","text":"<p>Navigate back to the Manage Deployment page. Click on <code>Actions</code> and then <code>Suspend Deployment</code>.</p> <p></p>"},{"location":"guide/02-07/index.html#42-click-on-suspend-flow","title":"4.2. Click on Suspend Flow","text":"<p>Click on the verification <code>Suspend Flow</code>.</p> <p></p>"},{"location":"guide/02-07/index.html#43-observe-the-status","title":"4.3. Observe the status","text":"<p>Observe the change in the status of the flow.</p> <p></p>"},{"location":"guide/02-07/index.html#44-review-deployment-state","title":"4.4. Review Deployment State","text":"<p>After a few minutes, the status of the deployment has been changed to Suspended. </p> <p></p>"},{"location":"guide/02-07/index.html#5-end-of-the-exercise","title":"5. End of the Exercise","text":""},{"location":"guide/02-08/index.html","title":"02-08 Migrating Existing Data Flows to CDF-PC","text":"<p>Info</p> <p>The purpose of this exercise is to demonstrate how existing NiFi flows externally developed (e,g. on local laptops of developers, or pushed from a code repo) can be migrated to the Data Flow. This workshop will leverage an existing NiFi flow template that has been designed with the best practices for CDF-PC flow deployment.</p> <p>The existing NiFi Flow will perform the following actions. - Generate random syslogs in 5424 Format.</p> <ul> <li> <p>Convert the incoming data to a JSON using record writers.</p> </li> <li> <p>Apply a SQL filter to the JSON records.</p> </li> <li> <p>Send the transformed syslog messages to Kafka.</p> </li> </ul> <p>Note</p> <p>A parameter context has already been defined in the flow and the queues have been uniquely named.</p> <p>For this we will be leveraging the DataHubs which have already been created - <code>edu-ds-messaging-250204</code>, <code>edu-ds-analytics-250204.</code></p> <p>Note that the above names might be different depending upon your environment.</p>"},{"location":"guide/02-08/index.html#1-create-a-kafka-topic","title":"1. Create a Kafka Topic","text":""},{"location":"guide/02-08/index.html#11-click-on-menu","title":"1.1. Click on Menu","text":"<p>Select the Menu option by clicking on the 9 dots. </p> <p></p>"},{"location":"guide/02-08/index.html#12-select-management-console","title":"1.2. Select Management Console","text":"<p>Navigate to the Management Console page by clicking the Management Console tile.</p> <p></p>"},{"location":"guide/02-08/index.html#13-select-environment","title":"1.3. Select Environment","text":"<p>Click on the environment. </p> <p></p>"},{"location":"guide/02-08/index.html#14-select-messaging-data-hub","title":"1.4. Select Messaging Data Hub","text":"<p>Click on the Data Hub for Stream Messaging. For ex: edu-ds-messaging-250204</p> <p></p>"},{"location":"guide/02-08/index.html#15-note-hostname-of-master","title":"1.5. Note Hostname of master","text":"<p>Note the hostname of the master server in the Kafka Datahub. This will be used while created a new deployment later in this exercise. </p> <pre><code>edu-ds-messaging-250204-master0.devops-5.fc0b-8n9t.a6.cloudera.site\n</code></pre> <p></p>"},{"location":"guide/02-08/index.html#16-login-to-streams-messaging-manager","title":"1.6. Login to Streams Messaging Manager","text":"<p>Login to <code>Streams Messaging Manager</code> by clicking the appropriate hyperlink in the Streams Messaging Datahub</p> <p></p>"},{"location":"guide/02-08/index.html#17-click-on-topics","title":"1.7. Click on Topics","text":"<p>Click on <code>Topics</code> in the left tab.</p> <p></p>"},{"location":"guide/02-08/index.html#18-click-on-add-new","title":"1.8. Click on Add New.","text":""},{"location":"guide/02-08/index.html#19-create-a-topic","title":"1.9. Create a Topic","text":"<p>Create a Topic with the following parameters. </p> <ul> <li>Name: <code>&lt;username&gt;_syslog</code>. </li> <li>Ex: <code>wuser00_syslog</code>.</li> <li>Partitions: <code>1</code></li> <li>Availability: <code>MODERATE</code></li> <li>Cleanup Policy: <code>delete</code></li> </ul> <p>Note</p> <p>The Flow will not work if you set the Cleanup Policy to anything other than <code>Delete</code>. This is because we are not specifying keys when writing to Kafka</p> <p>Click Save.</p> <p></p>"},{"location":"guide/02-08/index.html#110-review-message","title":"1.10. Review Message","text":"<p>A pop-up message will appear confirming topic has been added.</p> <p></p>"},{"location":"guide/02-08/index.html#111-search-for-the-topic","title":"1.11. Search for the topic","text":"<p>You can search for the topic that you created now and look for it as shown here.</p> <p></p>"},{"location":"guide/02-08/index.html#2-obtain-the-kafka-broker-list","title":"2. Obtain the Kafka Broker List","text":"<p>We will require the broker list to configure our processors to connect to our Kafka brokers which allows consumers to connect and fetch messages by partition, topic or offset. This information can be found in the Datahub cluster associated to the Streams Messaging Manager. Later in the lab, we will need to have at hand the list of kafka brokers - already configured in this environment- so to be able to our dataflow to publish to our Kafka topics.</p>"},{"location":"guide/02-08/index.html#21-select-brokers","title":"2.1. Select Brokers","text":"<p>Select <code>Brokers</code> from the left tab.</p> <p></p>"},{"location":"guide/02-08/index.html#22-save-broker-list","title":"2.2. Save Broker List","text":"<p>Save the name of the broker list in a notepad.</p> <p>Example: </p> <ul> <li>edu-ds-messaging-250204-corebroker2.devops-5.fc0b-8n9t.a6.cloudera.site:9093</li> <li>edu-ds-messaging-250204-corebroker0.devops-5.fc0b-8n9t.a6.cloudera.site:9093</li> <li>edu-ds-messaging-250204-corebroker1.devops-5.fc0b-8n9t.a6.cloudera.site:9093</li> </ul> <p></p>"},{"location":"guide/02-08/index.html#3-create-a-schema-in-schema-registry","title":"3. Create a Schema in Schema Registry","text":"<p>You need to now work on <code>Schema Registry</code>. </p>"},{"location":"guide/02-08/index.html#31-login-to-schema-registry","title":"3.1. Login to Schema Registry","text":"<p>Navigate back to Data Hub for Stream Messaging. Login to <code>Schema Registry</code> by clicking the appropriate hyperlink in the Streams Messaging Datahub</p> <p></p>"},{"location":"guide/02-08/index.html#32-create-a-new-schema","title":"3.2. Create a new schema","text":"<p>Click on the <code>+</code> button on the top right to create a new schema.</p> <p></p>"},{"location":"guide/02-08/index.html#33-fill-in-the-details","title":"3.3. Fill in the details","text":"<p>Create a new schema with the following information.</p> <ul> <li>Name: <code>&lt;username&gt;_syslog</code></li> <li>Ex: <code>dse_2_250204_syslog</code></li> <li>Description: <code>syslog schema for dataflow workshop</code></li> <li>Type: <code>Avro schema provider</code></li> <li>Schema Group: <code>Kafka</code></li> <li>Compatibility: <code>Backward</code></li> <li>Evolve: <code>True</code></li> <li>Schema Text: Copy and paste the below schema text below into the <code>Schema Text</code> field.</li> </ul> <pre><code>{\n \"name\": \"syslog\",\n \"type\": \"record\",\n \"namespace\": \"com.cloudera\",\n \"fields\": [\n  {\n   \"name\": \"priority\",\n   \"type\": \"int\"\n  },\n  {\n   \"name\": \"severity\",\n   \"type\": \"int\"\n  },\n  {\n   \"name\": \"facility\",\n   \"type\": \"int\"\n  },\n  {\n   \"name\": \"version\",\n   \"type\": \"int\"\n  },\n  {\n   \"name\": \"timestamp\",\n   \"type\": \"long\"\n  },\n  {\n   \"name\": \"hostname\",\n   \"type\": \"string\"\n  },\n  {\n   \"name\": \"body\",\n   \"type\": \"string\"\n  },\n  {\n   \"name\": \"appName\",\n   \"type\": \"string\"\n  },\n  {\n   \"name\": \"procid\",\n   \"type\": \"string\"\n  },\n  {\n   \"name\": \"messageid\",\n   \"type\": \"string\"\n  },\n  {\n   \"name\": \"structuredData\",\n   \"type\": {\n    \"name\": \"structuredData\",\n    \"type\": \"record\",\n    \"fields\": [\n     {\n      \"name\": \"SDID\",\n      \"type\": {\n       \"name\": \"SDID\",\n       \"type\": \"record\",\n       \"fields\": [\n        {\n         \"name\": \"eventId\",\n         \"type\": \"string\"\n        },\n        {\n         \"name\": \"eventSource\",\n         \"type\": \"string\"\n        },\n        {\n         \"name\": \"iut\",\n         \"type\": \"string\"\n        }\n       ]\n      }\n     }\n    ]\n   }\n  }\n ]\n}\n</code></pre> <p>Note:</p> <p>The name of the Kafka Topic (Ex: <code>dse_2_250204_syslog</code>) you previously created and the Schema Name must be the same.</p> <p>Click Save.</p> <p></p>"},{"location":"guide/02-08/index.html#34-review-message","title":"3.4. Review Message","text":"<p>A pop-up message will appear confirming Schema has been added. </p> <p></p>"},{"location":"guide/02-08/index.html#4-operationalizing-externally-developed-data-flows-with-cdf-pc","title":"4. Operationalizing Externally Developed Data Flows with CDF-PC","text":"<p>Import the Flow into the CDF-PC Catalog</p>"},{"location":"guide/02-08/index.html#41-click-on-menu","title":"4.1. Click on Menu","text":"<p>Select the Menu option by clicking on the 9 dots. </p> <p></p>"},{"location":"guide/02-08/index.html#42-select-dataflow","title":"4.2. Select DataFlow","text":"<p>Navigate to the Cloudera DataFlow page by clicking the DataFlow tile in the Cloudera management console.</p> <p></p>"},{"location":"guide/02-08/index.html#43-select-catalog","title":"4.3. Select Catalog","text":"<p>Select the Catalog option. </p> <p></p>"},{"location":"guide/02-08/index.html#44-select-import-flow-definition","title":"4.4. Select Import Flow Definition","text":"<p>Select <code>Import Flow Definition</code> on the Top Right.</p> <p></p>"},{"location":"guide/02-08/index.html#45-fill-in-the-details","title":"4.5. Fill in the details","text":"<p>Add the following information.</p> <ul> <li><code>Flow Name</code>: _syslog_to_kafka. (Ex: <code>wuser00_syslog_to_kafka</code>) <li><code>Flow Description</code>: <code>Reads Syslog in RFC 5424 format, applies a SQL filter, transforms the data into JSON records, and publishes to Kafka.</code></li> <li><code>NiFi Flow Configuration</code>: syslog-to-kafka.json (From the resources downloaded earlier).</li> <li><code>Version Comments</code>: Initial Version.</li> <p></p>"},{"location":"guide/02-08/index.html#46-upload-file","title":"4.6. Upload File","text":"<p>Upload the syslog-to-kafka.json script downloaded earlier in step 2 of exercise 02-02 Accessing Environment. </p> <p>Click Import.</p> <p></p>"},{"location":"guide/02-08/index.html#47-review-message","title":"4.7. Review Message","text":"<p>A pop-up message will appear confirming flow definition has been imported successfully. </p> <p></p>"},{"location":"guide/02-08/index.html#5-deploy-the-flow-in-cdf-pc","title":"5. Deploy the Flow in CDF-PC","text":""},{"location":"guide/02-08/index.html#51-search-for-the-flow","title":"5.1. Search for the flow","text":"<p>Search for the flow in the Flow Catalog by typing the flow name that you created in the previous step.</p> <p></p>"},{"location":"guide/02-08/index.html#52-deploy-the-flow","title":"5.2. Deploy the Flow","text":"<p>Click on the Flow, you should see the following. You should see a <code>Deploy</code> Option appear shortly. Then click on <code>Deploy</code>.</p> <p></p>"},{"location":"guide/02-08/index.html#53-select-target-environment","title":"5.3. Select Target Environment","text":"<p>Select the CDP <code>Target Environment</code> (Ex: <code>emeaworkshop-environ</code>) where this flow will be deployed. </p> <p>Click Continue.</p> <p></p>"},{"location":"guide/02-08/index.html#54-name-the-deployment","title":"5.4. Name the Deployment","text":"<p>Give the deployment a unique name (Ex: <code>{user_id}_syslog_to_kafka</code>). </p> <p>In case the character limit exceeds, you may shorten the name as shown in the next step. </p> <p></p>"},{"location":"guide/02-08/index.html#55-name-the-deployment","title":"5.5. Name the Deployment","text":"<p>Removed extra '_' from the name so as to meet the character limit for the deployment name.</p> <p>Click Next.</p> <p></p>"},{"location":"guide/02-08/index.html#56-click-next","title":"5.6. Click Next","text":"<p>In the NiFi Configuration screen, click Next to take the default parameters.</p> <p></p>"},{"location":"guide/02-08/index.html#57-add-the-flow-parameters","title":"5.7. Add the Flow Parameters","text":"<p>Add the Flow Parameters as below.</p> <p>Note that you might have to navigate to multiple screens to fill it. Then click <code>Next</code>.</p> <ul> <li><code>CDP Workload User</code>: The workload username for the current user. (Ex: <code>dse_2_250204</code>)</li> <li><code>CDP Workload Password</code>: The workload password for the current user (This password was set by you earlier).</li> <li><code>Filter Rule</code>: <code>SELECT * FROM FLOWFILE</code>.</li> </ul> <p></p>"},{"location":"guide/02-08/index.html#58-add-the-flow-parameters","title":"5.8. Add the Flow Parameters","text":"<ul> <li><code>Kafka Broker Endpoint</code>: The list of Kafka Brokers previously noted, which is comma separated as shown below.</li> <li>Example: edu-ds-messaging-250204-corebroker2.devops-5.fc0b-8n9t.a6.cloudera.site:9093,edu-ds-messaging-250204-corebroker0.devops-5.fc0b-8n9t.a6.cloudera.site:9093,edu-ds-messaging-250204-corebroker1.devops-5.fc0b-8n9t.a6.cloudera.site:9093</li> <li><code>Kafka Destination Topic</code>: username_syslog (Ex: <code>dse_2_250204_syslog</code>)</li> <li><code>Kafka Producer ID</code>: nifi_dfx_p1</li> <li><code>Schema Name</code>: username-syslog (Ex: <code>dse_2_250204_syslog</code>)</li> <li><code>Schema Registry Hostname</code>: The hostname of the master server in the Kafka Datahub (You have noted the hostname earlier in Step 1.6 in this exercise).</li> <li>Example: edu-ds-messaging-250204-master0.devops-5.fc0b-8n9t.a6.cloudera.site</li> </ul> <p>Click Next.</p> <p></p>"},{"location":"guide/02-08/index.html#59-define-sizing-and-scaling","title":"5.9. Define sizing and scaling","text":"<p>On the next page, define sizing and scaling details. </p> <ul> <li><code>Size</code>: <code>Extra Small</code></li> <li><code>Auto Scaling</code>: <code>Enabled</code></li> <li><code>Min Nodes</code>: <code>1</code></li> <li><code>Max Nodes</code>: <code>3</code></li> </ul> <p>Click Next.</p> <p></p>"},{"location":"guide/02-08/index.html#510-skip-the-kpi","title":"5.10. Skip the KPI","text":"<p>Skip the KPI page by clicking Next</p> <p></p>"},{"location":"guide/02-08/index.html#511-review-deployment","title":"5.11. Review deployment","text":"<p>Review your deployment. Then Click Deploy.</p> <p></p>"},{"location":"guide/02-08/index.html#512-proceed-to-the-cdf-pc-dashboard","title":"5.12. Proceed to the CDF-PC Dashboard","text":"<p>Proceed to the CDF-PC Dashboard and wait for your flow deployment to complete. A Green Check Mark will appear once complete, which might take a few minutes.</p> <p></p>"},{"location":"guide/02-08/index.html#513-review-deployment","title":"5.13. Review Deployment","text":"<p>When deployed, the flow will show up on the Data flow dashboard, as below.</p> <p></p>"},{"location":"guide/02-08/index.html#514-view-system-metrics","title":"5.14. View System Metrics","text":"<p>Click on System Metrics tab to view system metrics</p> <p></p>"},{"location":"guide/02-08/index.html#6-end-of-the-exercise","title":"6. End of the Exercise","text":""},{"location":"guide/02-09/index.html","title":"02-09 Managing KeyTabs","text":"<p>To run queries on the <code>SQL Stream Builder</code> you need to have your KeyTab <code>unlocked</code>. This is mainly for <code>authentication</code> purposes. As the credential you are using is sometimes reused as part of other people doing the same lab it is possible that your KeyTab is <code>already unlocked</code>. We have shared the steps for both the scenarios.</p>"},{"location":"guide/02-09/index.html#1-unlock-your-keytab","title":"1. Unlock your KeyTab","text":""},{"location":"guide/02-09/index.html#11-click-on-menu","title":"1.1. Click on Menu","text":"<p>Select the Menu option by clicking on the 9 dots. </p> <p></p>"},{"location":"guide/02-09/index.html#12-select-management-console","title":"1.2. Select Management Console","text":"<p>Navigate to the Management Console page by clicking the Management Console tile.</p> <p></p>"},{"location":"guide/02-09/index.html#13-select-environment","title":"1.3. Select Environment","text":"<p>Click on the environment. </p> <p></p>"},{"location":"guide/02-09/index.html#14-select-analytics-data-hub","title":"1.4. Select Analytics Data Hub","text":"<p>Click on the Data Hub cluster for stream analytics. (Ex: edu-ds-analytics-250204) </p> <p></p>"},{"location":"guide/02-09/index.html#15-click-streaming-sql-console","title":"1.5. Click Streaming SQL Console.","text":"<p>Open the SSB UI by clicking on <code>Streaming SQL Console</code>.</p> <p></p>"},{"location":"guide/02-09/index.html#16-click-on-the-user-name","title":"1.6. Click on the User name","text":"<p>Click on the User name (Ex: <code>dse_2_250204</code>) at the bottom left of the screen and select <code>Manage Keytab</code>. Make sure you are logged in as the username that was assigned to you.</p> <p></p>"},{"location":"guide/02-09/index.html#17-enter-credentials","title":"1.7. Enter Credentials","text":"<p>Enter your Workload Username under <code>Principal Name *</code> and workload password that you had set earlier in the <code>Password *</code> field.</p> <p>Click on Unlock Keytab.</p> <p></p>"},{"location":"guide/02-09/index.html#18-close","title":"1.8. Close","text":"<p>A message appears confirming 'Success KeyTab has been unclocked'. </p> <p>Click on X to close the window. </p> <p></p>"},{"location":"guide/02-09/index.html#2-reset-your-keytab","title":"2. Reset your KeyTab","text":""},{"location":"guide/02-09/index.html#21-click-on-the-user-name","title":"2.1. Click on the User name","text":"<p>Click on the User name (Ex: <code>dse_2_250204</code>) at the bottom left of the screen and select <code>Manage Keytab</code>. Make sure you are logged in as the username that was assigned to you.</p> <p></p>"},{"location":"guide/02-09/index.html#22-verify-keyab","title":"2.2. Verify Keyab","text":"<p>If you get the following dialog box it means that your Keytab is already <code>UNLOCKED</code>. </p> <p>Hence, it would be necessary to reset here by locking it and unlocking it again using your newly set workload password.</p> <p>Click on Lock Keytab.</p> <p></p>"},{"location":"guide/02-09/index.html#23-review-message","title":"2.3. Review message","text":"<p>A success message appears confirming Keytab has been locked.</p> <p>Click on X to close the window. </p> <p></p>"},{"location":"guide/02-09/index.html#24-click-on-the-user-name","title":"2.4. Click on the User name","text":"<p>Click on the User name (Ex: <code>dse_2_250204</code>) at the bottom left of the screen and select <code>Manage Keytab</code>.</p> <p></p>"},{"location":"guide/02-09/index.html#25-enter-credentials","title":"2.5. Enter Credentials","text":"<p>Enter your Workload Username under <code>Principal Name *</code> and workload password that you had set earlier in the <code>Password *</code> field.</p> <p>Click on Unlock Keytab.</p> <p></p>"},{"location":"guide/02-09/index.html#26-close","title":"2.6. Close","text":"<p>A message appears confirming 'Success KeyTab has been unclocked'.</p> <p>Click on X to close the window. </p> <p></p>"},{"location":"guide/02-09/index.html#3-end-of-the-exercise","title":"3. End of the Exercise","text":""},{"location":"guide/02-10/index.html","title":"02-10 Working on SQL Stream Builder Project","text":"<p>Note</p> <p>The purpose of this workshop is to demonstrate streaming analytic capabilities using SQL Stream Builder. We will leverage the NiFi Flow deployed in CDF-PC from the previous step and demonstrate how to query live data and subsequently sink it to another location. The SQL query will leverage the existing syslog schema in Schema Registry.</p>"},{"location":"guide/02-10/index.html#1-create-a-sql-stream-builder-ssb-project","title":"1. Create a SQL Stream Builder (SSB) Project","text":""},{"location":"guide/02-10/index.html#11-click-on-menu","title":"1.1. Click on Menu","text":"<p>Select the Menu option by clicking on the 9 dots. </p> <p></p>"},{"location":"guide/02-10/index.html#12-select-management-console","title":"1.2. Select Management Console","text":"<p>Navigate to the Management Console page by clicking the Management Console tile.</p> <p></p>"},{"location":"guide/02-10/index.html#13-select-environment","title":"1.3. Select Environment","text":"<p>Click on the environment. </p> <p></p>"},{"location":"guide/02-10/index.html#14-select-analytics-data-hub","title":"1.4. Select Analytics Data Hub","text":"<p>Click on the Data Hub cluster for stream analytics. (Ex: edu-ds-analytics-250204) </p> <p></p>"},{"location":"guide/02-10/index.html#15-click-streaming-sql-console","title":"1.5. Click Streaming SQL Console.","text":"<p>Open the SSB UI by clicking on <code>Streaming SQL Console</code>.</p> <p></p>"},{"location":"guide/02-10/index.html#16-create-a-new-project","title":"1.6. Create a new project","text":"<p>Create a SQL Stream Builder (SSB) Project by clicking New Project.</p> <p></p>"},{"location":"guide/02-10/index.html#17-fill-in-the-details","title":"1.7. Fill in the details","text":"<p>Use the following details.</p> <ul> <li><code>Name</code>: <code>{user_id}_hol_data_services</code>.</li> <li>(Ex: <code>dse_2_250204_data_services</code>).</li> <li><code>Description</code>: SSB Project to analyze streaming data. </li> </ul> <p>Click Create.</p> <p></p>"},{"location":"guide/02-10/index.html#18-review-project","title":"1.8. Review project","text":"<p>A success message appears confirming 'Project has been successfully created'. </p> <p></p>"},{"location":"guide/02-10/index.html#19-switch-project","title":"1.9. Switch project","text":"<p>Switch to the created project (Ex: <code>dse_2_250204_hol_data_services</code>). </p> <p>Click on Switch.</p> <p></p>"},{"location":"guide/02-10/index.html#110-select-switch-project","title":"1.10. Select Switch Project","text":"<p>If pop up comes select <code>Switch Project</code>.</p> <p></p>"},{"location":"guide/02-10/index.html#111-review-project","title":"1.11. Review Project","text":"<p>The project page appears. </p> <p></p>"},{"location":"guide/02-10/index.html#2-create-kafka-data-store","title":"2. Create Kafka Data Store","text":""},{"location":"guide/02-10/index.html#21-select-data-sources","title":"2.1. Select Data Sources","text":"<p>Create Kafka Data Store by selecting <code>Data Sources</code> in the left pane.</p> <p></p>"},{"location":"guide/02-10/index.html#22-click-on-ellipsis","title":"2.2. Click on ellipsis","text":"<p>Click on the three-dotted icon next to <code>Kafka</code></p> <p></p>"},{"location":"guide/02-10/index.html#23-select-new-kafka-data-source","title":"2.3. Select New Kafka Data Source.","text":""},{"location":"guide/02-10/index.html#24-fill-in-the-details","title":"2.4. Fill in the details","text":"<p>Add the Flow Parameters as below.</p> <ul> <li><code>Name</code>: <code>{user-id}_cdp_kafka</code>.</li> <li>(Ex: <code>dse_2_250204_cdp_kafka</code>)</li> <li><code>Kafka Broker Endpoint</code>: The list of Kafka Brokers previously noted, which is comma separated as shown below.</li> <li>Example: edu-ds-messaging-250204-corebroker2.devops-5.fc0b-8n9t.a6.cloudera.site:9093,edu-ds-messaging-250204-corebroker0.devops-5.fc0b-8n9t.a6.cloudera.site:9093,edu-ds-messaging-250204-corebroker1.devops-5.fc0b-8n9t.a6.cloudera.site:9093</li> <li><code>Protocol</code>: <code>SASL/SSL</code></li> </ul> <p></p>"},{"location":"guide/02-10/index.html#25-fill-in-the-details","title":"2.5. Fill in the details","text":"<ul> <li><code>SASL Username</code>: <code>workload-username</code>.</li> <li>(Ex: dse_2_250204).</li> <li><code>SASL Mechanism</code>: <code>PLAIN</code>.</li> <li><code>SASL Password</code>: Workload User password set by you earlier in exercise 02-03 Define Workload Password.</li> </ul> <p>Click on Validate to test the connections.</p> <p></p>"},{"location":"guide/02-10/index.html#26-click-create","title":"2.6. Click Create","text":"<p>A message appears confirming Data Source is Valid. </p> <p>Click Create.</p> <p></p>"},{"location":"guide/02-10/index.html#27-review-message","title":"2.7. Review message","text":"<p>A success message appears confirming the data source has been saved. </p> <p></p>"},{"location":"guide/02-10/index.html#3-create-kafka-table","title":"3. Create Kafka Table","text":""},{"location":"guide/02-10/index.html#31-click-on-new-kafka-table","title":"3.1. Click on New Kafka Table","text":"<p>Create Kafka Table, by selecting <code>Virtual Tables</code> in the left pane by clicking on the three-dotted icon (ellipsis) next to it.</p>"},{"location":"guide/02-10/index.html#_1","title":"02-10 Working on SQL Steam Builder Project","text":"<p>Then click on New Kafka Table.</p> <p></p>"},{"location":"guide/02-10/index.html#32-configure-kafka-table","title":"3.2. Configure Kafka Table","text":"<p>Configure the Kafka Table using the details below.</p> <ul> <li><code>Table Name</code>: {user-id}_syslog_data.</li> <li>(Ex: <code>dse_2_250204_syslog_data</code>)</li> <li><code>Kafka Cluster</code>: <code>select the Kafka data source you created previously</code>.</li> <li>(Ex: <code>dse_2_250204_cdp_kafka</code>)</li> <li><code>Data Format</code>: <code>JSON</code>.</li> <li><code>Topic Name</code>: <code>select the topic created in Schema Registry</code>.</li> </ul> <p>When you select Data Format as AVRO, you must provide the correct Schema Definition when creating the table for SSB to be able to successfully process the topic data. For JSON tables, though, SSB can look at the data flowing through the topic and try to infer the schema automatically, which is quite handy at times. Obviously, there must be data in the topic already for this feature to work correctly.</p> <p>Note</p> <p>SSB tries its best to infer the schema correctly, but this is not always possible and sometimes data types are inferred incorrectly. You should always review the inferred schemas to check if it\u2019s correctly inferred and make the necessary adjustments.</p> <p>Since you are reading data from a JSON topic, go ahead and click on <code>Detect Schema</code> to get the schema inferred. You should see the schema be updated in the <code>Schema Definition</code> tab.</p> <p>Click on **Detect Schema. **</p> <p></p>"},{"location":"guide/02-10/index.html#33-review-message","title":"3.3. Review message","text":"<p>Click OK to close the window.</p> <p></p>"},{"location":"guide/02-10/index.html#34-schema-is-invalid","title":"3.4. Schema is invalid","text":"<p>You will also notice that a \"Schema is invalid\" message appears upon the schema detection.</p> <p></p>"},{"location":"guide/02-10/index.html#35-review-info","title":"3.5. Review info","text":"<p>If you hover the mouse over the message, it shows the reason.</p> <p>We will fix this in the next step.</p> <p></p>"},{"location":"guide/02-10/index.html#36-enter-properties","title":"3.6. Enter Properties","text":"<p>Info</p> <p>Each record read from Kafka by SSB has an associated timestamp column of data type TIMESTAMP ROWTIME. By default, this timestamp is sourced from the internal timestamp of the Kafka message and is exposed through a column called eventTimestamp. However, if your message payload already contains a timestamp associated with the event (event time), you may want to use that instead of the Kafka internal timestamp.</p> <p>In this case, the syslog message has a field called <code>timestamp</code> that contains the timestamp you should use. You want to expose this field as the table\u2019s <code>event_time</code> column. To do this, click on the Event Time tab and enter the following properties.</p> <ul> <li><code>Use Kafka Timestamps</code>: <code>Disable</code>.</li> <li><code>Input Timestamp Column</code>: <code>timestamp</code>.</li> <li><code>Event Time Column</code>: <code>event_time</code>.</li> <li><code>Watermark Seconds</code>: <code>3</code>.</li> </ul> <p>Now that you have configured the event time column, click on Detect Schema again. </p> <p></p>"},{"location":"guide/02-10/index.html#37-review-message","title":"3.7. Review message","text":"<p>Click OK to close the window.</p> <p></p>"},{"location":"guide/02-10/index.html#38-schema-is-valid","title":"3.8. Schema is valid","text":"<p>You should see the schema turn valid.</p> <p></p>"},{"location":"guide/02-10/index.html#39-create-table","title":"3.9. Create Table","text":"<p>Click the Create and Review button to create the table.</p> <p></p>"},{"location":"guide/02-10/index.html#310-review-the-tables-ddl","title":"3.10. Review the table\u2019s DDL","text":"<p>A success message appears confirming table has been created. </p> <p>Review the table\u2019s DDL and click Close. </p> <p></p>"},{"location":"guide/02-10/index.html#4-create-a-flink-job","title":"4. Create a Flink Job","text":""},{"location":"guide/02-10/index.html#41-click-on-new-job","title":"4.1. Click on New Job","text":"<p>Create a Flink Job, by selecting</p> <p><code>Jobs</code> in the left pane, clicking on the three-dotted icon (ellipsis) next to it. </p> <p>Then click on New Job.</p> <p></p>"},{"location":"guide/02-10/index.html#42-name-the-job","title":"4.2. Name the Job","text":"<p>Give a unique job name (Ex: <code>dse_2_250204_flink_job</code>) </p> <p>Click Create.</p> <p></p>"},{"location":"guide/02-10/index.html#43-review-job","title":"4.3. Review job","text":"<p>A success message appears confirming 'Job has been created'. </p> <p></p>"},{"location":"guide/02-10/index.html#44-execute-select-query","title":"4.4. Execute SELECT query","text":"<p>Add the following SQL Statement in the Editor.</p> <pre><code>SELECT * FROM {user-id}_syslog_data WHERE severity &lt;=3\n</code></pre> <p>Replace user-id with your username.</p> <p>For ex: SELECT * FROM dse_2_250204_syslog_data WHERE severity &lt;=3</p> <p>Run the Streaming SQL Job by clicking Execute.</p> <p></p>"},{"location":"guide/02-10/index.html#45-view-output","title":"4.5. View output","text":"<p>Give. it a couple of minutes for the query to run. </p> <p></p>"},{"location":"guide/02-10/index.html#46-review-message","title":"4.6. Review message","text":"<p>A success message appears confirming 'Job has been started'. </p> <p></p>"},{"location":"guide/02-10/index.html#47-review-output","title":"4.7. Review Output","text":"<p>In the <code>Results</code> tab, you should see syslog messages with severity levels \u21d03.</p> <p></p>"},{"location":"guide/02-10/index.html#5-end-of-the-exercise","title":"5. End of the Exercise","text":""},{"location":"guide/03-01/index.html","title":"03-01 Using Cloudera Data Engineering - Airflow","text":"<p>The purpose of this module is to install Cloudera Data Platform Runtime.</p>"},{"location":"guide/03-01/index.html#1-introduction","title":"1. Introduction","text":""},{"location":"guide/03-01/index.html#2-cloudera-data-engineering","title":"2. Cloudera Data Engineering","text":""},{"location":"guide/03-01/index.html#3-airflow-concepts","title":"3. Airflow Concepts","text":""},{"location":"guide/03-01/index.html#4-troubleshoot-jobs","title":"4. Troubleshoot Jobs","text":""},{"location":"guide/03-01/index.html#5-use-cases","title":"5. Use Cases","text":""},{"location":"guide/03-01/index.html#6-cde-airflow-faqs","title":"6. CDE Airflow FAQs","text":""},{"location":"guide/03-01/index.html#7-requirements","title":"7. Requirements","text":""},{"location":"guide/03-01/index.html#8-exercises","title":"8. Exercises","text":""},{"location":"guide/03-01/index.html#9-summary","title":"9. Summary","text":""},{"location":"guide/03-01/index.html#10-resources","title":"10. Resources","text":""},{"location":"guide/03-02/index.html","title":"03-02 Accessing Environment","text":"<p>Get aboard the classroom environment for Cloudera Data Engineering. This exercise will walk you through how to acquire the login URL and required credentials.</p>"},{"location":"guide/03-02/index.html#1-student-credentials","title":"1. Student Credentials","text":""},{"location":"guide/03-02/index.html#11-review-login-details","title":"1.1. Review login details","text":"<p>Review the student email for the log in credentials and the URL to the login page.</p> <p></p>"},{"location":"guide/03-02/index.html#12-lab-environment","title":"1.2. Lab Environment","text":"<p>This course provides a lab environment which is a Cloudera Public Cloud in which the following environments are pre-created:</p> <ul> <li>Data Service environment [CDE-{class-id}]</li> <li>Virtual Cluster [VC-{class-id}]</li> <li>Virtual Warehouse [VW-hive-{class-id}]</li> </ul>"},{"location":"guide/03-02/index.html#2-download-zip-files","title":"2. Download Zip files","text":"<p>We will work with the following artifacts:</p> <ul> <li>Python scripts containing DAGs and Jobs.</li> <li>These scripts are under provide under cde_jobs directory in a zipped format.</li> </ul> <p>Every exercise will ask you to upload a python script to create a DAG. Click here to download</p>"},{"location":"guide/03-02/index.html#3-accessing-cde-airflow-ui","title":"3. Accessing CDE Airflow UI","text":""},{"location":"guide/03-02/index.html#31-login-to-edu-keycloak","title":"3.1. Login to edu-keycloak","text":"<p>Get the edu-keycloak URL from the instructor, and log in using the provided username and a password. You will be able to copy and paste in both the user ID and the password.</p> <p></p>"},{"location":"guide/03-02/index.html#32-select-the-data-engineering-tile","title":"3.2. Select the Data Engineering Tile","text":"<p>Select the Cloudera Data Engineering tile from the Cloudera Public Cloud Enterprise Data web interface.</p> <p></p>"},{"location":"guide/03-02/index.html#4-tour-the-home-page","title":"4. Tour the Home Page","text":""},{"location":"guide/03-02/index.html#41-view-the-navigation-panel","title":"4.1. View the Navigation Panel","text":"<p>The Main navigation panel enables access to the following Cloudera Data Engineering functions:</p> <ul> <li>Jobs</li> <li>Jobs Runs</li> <li>Sessions</li> <li>Repositories</li> <li>Resources</li> <li>Administration</li> </ul> <p></p>"},{"location":"guide/03-02/index.html#42-review-list-of-quick-links","title":"4.2. Review list of Quick Links","text":"<p>The Environments page that lists your environments, including:</p> <p>\u2022 New Session</p> <p>\u2022 New Spark Job</p> <p>\u2022 New Pipeline</p> <p>\u2022 Add Resource</p> <p>\u2022 View Job Runs</p> <p>\u2022 View Clusters</p> <p></p>"},{"location":"guide/03-02/index.html#5-end-of-exercise","title":"5. End of Exercise","text":""},{"location":"guide/03-03/index.html","title":"03-03 Building Your First CDE Airflow DAG","text":"<p>We will start with a simple Airflow DAG that executes a Spark CDE Job and a shell command.</p> <p>We will work with the following artifacts:</p> <ul> <li>A python file containing Airflow DAG. This is provided under cde_jobs/firstdag.py.</li> <li>A pythong file containing a PySpark job. This is provided under cde_jobs/sql.py.</li> </ul>"},{"location":"guide/03-03/index.html#1-create-a-spark-cde-job","title":"1. Create a Spark CDE Job","text":"<p>Let's create the first CDE Job. </p>"},{"location":"guide/03-03/index.html#11-click-on-administration","title":"1.1. Click on Administration","text":"<p>Navigate to the main menu and select Administration option from the panel. </p> <p></p>"},{"location":"guide/03-03/index.html#12-review-the-environment","title":"1.2. Review the environment","text":"<p>A CDE service [CDE-251701] and a virtual cluster [VC-251701]has been created for you. </p> <p>In your case, the name would reflect your class ID [CDE-(class ID), VC-(class ID)]. Please make a note of the environment names. </p> <p></p>"},{"location":"guide/03-03/index.html#13-click-on-view-jobs","title":"1.3. Click on View Jobs","text":"<p>Click on the View Jobs button in the Virtual Cluster assigned to you. </p> <p></p>"},{"location":"guide/03-03/index.html#14-click-on-create-jobs","title":"1.4. Click on Create Jobs","text":"<p>Let's create our first job. Click on Create Job button. </p> <p></p>"},{"location":"guide/03-03/index.html#15-fill-in-the-details","title":"1.5. Fill in the details","text":"<p>A Create job page opens. In the Create Job page, select the Job Type as Spark 3.2.3.</p> <p>Name the job as sparksql_YourCDPUsername.</p> <p>Attention</p> <p>Replace username with your actual username.</p> <p>For example: sparksql_dse_1_250204</p> <p>Select File option under Application File.</p> <p></p>"},{"location":"guide/03-03/index.html#16-click-on-upload","title":"1.6. Click on Upload","text":"<p>Click on the Upload button to furnish the script</p> <p></p>"},{"location":"guide/03-03/index.html#17-browse-sqlpy","title":"1.7. Browse sql.py","text":"<p>From the cde_job zip file downloaded in the previous exercise, browse and upload sql.pyscript as your Spark job file.</p> <p></p>"},{"location":"guide/03-03/index.html#18-create-a-resource","title":"1.8. Create a Resource","text":"<p>You will be prompted to select a resource. Create a new resource with name firstdag_YourCDPUsername.</p> <p>Attention</p> <p>Replace username with your actual username.</p> <p>For example: firstdag_dse_1_250204</p> <p>Resources are repositories inside the Virtual Cluster where you can store files, dependencies, and manage python environments.</p> <p>For more info on resources, please visit the CDE Documentation.</p> <p>Click on Upload</p> <p></p>"},{"location":"guide/03-03/index.html#19-select-create","title":"1.9. Select Create","text":"<p>At the bottom of the form, make sure to select \"Create\" rather than \"Create and Run\" by clicking on the down arrow first.</p> <p></p>"},{"location":"guide/03-03/index.html#110-review-message","title":"1.10. Review message","text":"<p>A success message appears confirming job has been created. </p> <p></p>"},{"location":"guide/03-03/index.html#111-review-job","title":"1.11. Review Job","text":"<p>The Spark CDE Job is now available in the Jobs UI.</p> <p></p>"},{"location":"guide/03-03/index.html#2-review-edit-the-airflow-dag","title":"2. Review &amp; Edit the Airflow DAG","text":"<p>Let's go over the code.</p>"},{"location":"guide/03-03/index.html#21-open-firstdagpy-file","title":"2.1. Open firstdag.py file","text":"<p>Open the firstdag.py file located in the cde_jobs folder downloaded in the previous exercises.</p> <p></p>"},{"location":"guide/03-03/index.html#22-import-python-modules","title":"2.2. Import Python modules","text":"<p>Between lines 2 and 7 we import the Python modules needed for the DAG. </p> <p>Notice that at line 6 and 7 we import the CDEJobRunOperator and BashOperator. The CDEJobRunOperator was created by Cloudera to support Spark CDE Job. The BashOperator is used to perform shell actions inside an Airflow DAG.</p> <pre><code>from datetime import datetime, timedelta\nfrom dateutil import parser\nimport pendulum\nfrom airflow import DAG\nfrom cloudera.cdp.airflow.operators.cde_operator import CDEJobRunOperator\nfrom airflow.operators.bash import BashOperator\n</code></pre> <p></p>"},{"location":"guide/03-03/index.html#23-declare-dag-and-arguments","title":"2.3. Declare DAG and arguments","text":"<p>Between lines 8 and 22 we declare the DAG and its arguments.</p> <p>The arguments dictionary includes options for scheduling, setting dependencies, and general execution. For a comprehensive list of DAG arguments please consult this page in the documentation.</p> <p>Once the arguments dictionary is complete it is passed as an argument to the DAG object instance.</p> <pre><code>default_args = {\n        'owner': 'YourCDPUsername',\n        'retry_delay': timedelta(seconds=5),\n        'depends_on_past': False,\n        'start_date': pendulum.datetime(2020, 1, 1, tz=\"Europe/Amsterdam\")\n        }\n\nfirstdag = DAG(\n        'airflow-pipeline-demo',\n        default_args=default_args,\n        schedule_interval='@daily',\n        catchup=False,\n        is_paused_upon_creation=False\n        )\n\nspark_step = CDEJobRunOperator(\n        task_id='sql_job_new',\n        dag=firstdag,\n    job_name='sparksql_YourCDPUsername'\n        )\n</code></pre> <p>\u26a0\ufe0f Replace Owner and Job Name</p> <p>Before moving on, make sure to open the DAG python file in your editor and replace the current value of the 'Owner' with your CDP Username in the default arguments dictionary. </p> <p>Also, replace the current value of the 'job_name' with sparksql_YourCDPUsername, set earlier in this exercise, in the spark_step object. </p> <p>No other changes are required at this time.</p> <p></p>"},{"location":"guide/03-03/index.html#24-declare-object","title":"2.4. Declare object","text":"<p>Between lines 24 and 28 an instance of the CDEJobRunOperator obect is declared with the following arguments:</p> <ul> <li>Task ID: This is the name used by the Airflow UI to recognize the node in the DAG.</li> <li>DAG: This has to be the name of the DAG object instance declared at line 16.</li> <li>Job Name: This has to be the name of the Spark CDE Job created in step 1 above.</li> </ul> <pre><code>spark_step = CDEJobRunOperator(\n    task_id='sql_job_new',\n    dag=firstdag,\n    job_name='sparksql_dse_1_250204'\n    )\n</code></pre> <p></p>"},{"location":"guide/03-03/index.html#25-declare-object","title":"2.5. Declare object","text":"<p>Between lines 30 and 34 we declare an instance of the BashOperator object with the following arguments:</p> <ul> <li>Task ID: as above, you can pick an arbitrary string value</li> <li>DAG: This has to be the name of the DAG object instance declared at line 16.</li> <li>Bash Command: the actual shell commands you want to execute.</li> </ul> <p>Notice that this is just a simple example. You can optionally add more complex syntax with Jinja templating, use DAG variables, or even trigger shell scripts. For more please visit the Airflow Bash Operator documentation.</p> <pre><code>shell = BashOperator(\n    task_id='bash',\n    dag=firstdag,\n    bash_command='echo \"Hello Airflow\" '\n    )\n</code></pre> <p></p>"},{"location":"guide/03-03/index.html#26-declare-task-dependencies","title":"2.6. Declare Task Dependencies","text":"<p>Finally, at line 36 we declare Task Dependencies. With this statement you can specify the execution sequence of DAG tasks.</p> <pre><code>spark_step &gt;&gt; shell \n</code></pre> <p>Important</p> <p>Before moving on, make sure to open the DAG python file in your editor and replace the current value of the 'Owner' with your CDP Username in the default arguments dictionary. Also, replace the current value of the 'job_name' with sparksql_YourCDPUsername, set earlier in this exercise, in the spark_step object. No other changes are required at this time.</p> <p></p>"},{"location":"guide/03-03/index.html#3-running-your-first-cde-airflow-dag","title":"3. Running Your First CDE Airflow DAG","text":""},{"location":"guide/03-03/index.html#31-click-on-create-job","title":"3.1. Click on Create Job","text":"<p>Click on Create Job to create a new CDE Job.</p> <p></p>"},{"location":"guide/03-03/index.html#32-fill-in-the-details","title":"3.2. Fill in the details","text":"<ul> <li>Job Type: Airflow</li> <li>Name: FirstDag_YourCDPUsername</li> <li>Dag File: File</li> </ul> <p>Click on Upload</p> <p></p>"},{"location":"guide/03-03/index.html#33-upload-file-to-a-resource","title":"3.3. Upload file to a resource","text":"<p>Upload firstdag.py to the firstdag CDE Resource you created earlier.</p> <p>Ensure that you have replaced the current value of the 'Owner' with your CDP Username in the default arguments dictionary. Please refer to Step 2.3 above in this exercise for details. </p> <p>Click on Upload.</p> <p></p>"},{"location":"guide/03-03/index.html#34-select-create-and-run","title":"3.4. Select 'Create and Run'","text":"<p>Select Create and Run button to trigger the job immediately.</p> <p></p>"},{"location":"guide/03-03/index.html#35-job-run-in-progress","title":"3.5. Job run in progress","text":"<p>Please wait for a min for job to run</p> <p></p>"},{"location":"guide/03-03/index.html#36-new-run-is-initiated","title":"3.6. New run is initiated","text":"<p>A success message appears confirming that a new run with ID has been initiated. </p> <p></p>"},{"location":"guide/03-03/index.html#37-review-the-jobs-in-progress","title":"3.7. Review the Jobs in progress","text":"<p>Navigate to the CDE Jobs Run page and notice that two CDE Jobs are now in progress. One of them is \"sparksql_dse_1_250204\" (Spark CDE Job) and the other is \"FirstDag_dse_1_250204\" (Airflow CDE Job). The former has been triggered by the execution of the latter. Wait a few moments and allow for the DAG to complete.</p> <p></p>"},{"location":"guide/03-03/index.html#38-click-on-the-firstdag-link","title":"3.8. Click on the FirstDag link","text":"<p>Once the Status column shows the Jobs run as succeeded, click on the \"FirstDag_dse_1_250204\" link to access the Job Run page.</p> <p></p>"},{"location":"guide/03-03/index.html#39-select-the-most-recent-run","title":"3.9. Select the most recent Run","text":"<p>This page shows each run along with associated logs, execution statistics, and the Airflow UI.</p> <p>Ensure to select the most recent Run (in the screenshot, the most recent Run ID number is 2)</p> <p></p>"},{"location":"guide/03-03/index.html#310-review-job-run","title":"3.10. Review Job Run","text":"<p>Review the Job Run. Notice the run id in the top section of the page. </p> <p>Click on the Airflow UI tab.</p> <p></p>"},{"location":"guide/03-03/index.html#311-review-airflow-ui-tab","title":"3.11. Review Airflow UI tab","text":"<p>The first landing page lists all tasks along with their status. Notice that the DAG ID, Task ID and Operator columns are populated with the values set in the DAG python files. </p> <p>Next, click on the back arrow on the left side of the screen to navigate to the Airflow UI DAGs view.</p> <p></p>"},{"location":"guide/03-03/index.html#312-explore-dags-view","title":"3.12. Explore DAGs view.","text":"<p>From the DAGs view you can:</p> <ul> <li>Pause/unpause a DAG</li> <li>Filter the list of DAGs to show active, paused, or all DAGs</li> <li>Trigger, refresh, or delete a DAG</li> <li>Navigate quickly to other DAG-specific pages</li> </ul> <p>The DAG name specified in the python file during DAG declaration is airflow-pipeline-demo. </p> <p>Click on it to open the Airflow UI DAG view to drill down with DAG-specific pages.</p> <p></p>"},{"location":"guide/03-03/index.html#313-review-dag","title":"3.13. Review DAG","text":"<p>Here, Airflow provides a number of tabs to increase job observability. Below is a brief explanation of the most important ones.</p>"},{"location":"guide/03-03/index.html#the-tree-view","title":"The Tree View","text":"<p>The Tree View tracks DAG tasks across time. Each column represents a DAG Run and each square is a task instance in that DAG Run. Task instances are color-coded depending on success of failure. DAG Runs with a black border represent scheduled runs while DAG Runs with no border are manually triggered.</p>"},{"location":"guide/03-03/index.html#the-graph-view","title":"The Graph View","text":"<p>The Graph View shows a simpler diagram of DAG tasks and their dependencies for the selected run. You can enable auto-refresh the view to see the status of the tasks update in real time.</p>"},{"location":"guide/03-03/index.html#the-calendar-view","title":"The Calendar View","text":"<p>The Calendar View shows the state of DAG Runs overlaid on a calendar.</p>"},{"location":"guide/03-03/index.html#the-code-view","title":"The Code View","text":"<p>The Code View shows the code that is used to generate the DAG. This is the DAG python file we used earlier.</p> <p></p>"},{"location":"guide/03-03/index.html#4-end-of-the-exercise","title":"4. End of the Exercise","text":""},{"location":"guide/03-04/index.html","title":"03-04 Beyond Airflow for Spark Jobs","text":""},{"location":"guide/03-04/index.html#using-the-cdwrunoperator","title":"Using the CDWRunOperator","text":"<p>Info</p> <p>The CDWRunOperator was contributed by Cloudera in order to orchestrate CDW queries with Airflow.</p>"},{"location":"guide/03-04/index.html#1-setup-cdw","title":"1. Setup CDW","text":"<p>Before we can use it in the DAG we need to connect Airflow to CDW. To complete these steps, you must have access to a CDW virtual warehouse. CDE currently supports CDW operations for ETL workloads in Apache Hive virtual warehouses. To determine the CDW hostname to use for the connection:</p>"},{"location":"guide/03-04/index.html#11-click-on-menu","title":"1.1. Click on Menu","text":"<p>Select the Menu option by clicking on the 9 dots. </p> <p></p>"},{"location":"guide/03-04/index.html#12-select-data-warehouse","title":"1.2. Select Data Warehouse","text":"<p>Navigate to the Cloudera Data Warehouse Overview page by clicking the Data Warehouse tile in the Cloudera management console.</p> <p></p>"},{"location":"guide/03-04/index.html#13-view-the-environment","title":"1.3. View the environment","text":"<p>In the Virtual Warehouses column, find the warehouse you want to connect to. In our case, it would be [vw-hive-{class-ID}].</p> <p></p>"},{"location":"guide/03-04/index.html#14-notice-the-status","title":"1.4. Notice the status","text":"<p>Virtual Warehouse has been configured to auto-suspend every 300 seconds and it will probably go in the stopped state. The stopped status of virtual warehouse will not affect the exercise. </p> <p></p>"},{"location":"guide/03-04/index.html#15-copy-jdbc-url","title":"1.5. Copy JDBC URL","text":"<p>Click the three-dot menu for the selected warehouse, and then click Copy JDBC URL.</p> <p></p>"},{"location":"guide/03-04/index.html#16-note-the-hostname","title":"1.6. Note the hostname","text":"<p>Paste the URL into a text editor, and make note of the hostname.</p> <p>For example, starting with the following URL the hostname is shown below:</p> <pre><code>Original URL: jdbc:hive2://hs2-vw-hive-250204.dw-devops-570-class-250204.fc0b-8n9t.cloudera.site/default;transportMode=http;httpPath=cliservice;socketTimeout=60;ssl=true;retries=3;\n\nHostname: hs2-vw-hive-250204.dw-devops-570-class-250204.fc0b-8n9t.cloudera.site/\n</code></pre> <p></p>"},{"location":"guide/03-04/index.html#2-setup-cde","title":"2. Setup CDE","text":""},{"location":"guide/03-04/index.html#21-click-on-menu","title":"2.1. Click on Menu","text":"<p>Select the Menu option by clicking on the 9 dots.</p> <p></p>"},{"location":"guide/03-04/index.html#22-select-data-engineering","title":"2.2. Select Data Engineering","text":"<p>Navigate to the Cloudera Data Engineering Overview page by clicking the Data Engineering tile in the Cloudera management console.</p> <p></p>"},{"location":"guide/03-04/index.html#23-select-administration","title":"2.3. Select Administration","text":"<p>Navigate to the main menu and select Administration option from the panel. </p> <p></p>"},{"location":"guide/03-04/index.html#24-click-on-cluster-details","title":"2.4. Click on Cluster Details","text":"<p>In the Virtual Clusters column, click Cluster Details for the virtual cluster.</p> <p></p>"},{"location":"guide/03-04/index.html#25-click-airflow-ui","title":"2.5.  Click AIRFLOW UI","text":""},{"location":"guide/03-04/index.html#26-click-the-connection-link","title":"2.6. Click the Connection link","text":"<p>From the Airflow UI, click the Connection link from the Admin menu.</p> <p></p>"},{"location":"guide/03-04/index.html#27-add-a-new-record","title":"2.7. Add a new record","text":"<p>Click the plus sign to add a new record.</p> <p></p>"},{"location":"guide/03-04/index.html#28-fill-in-the-fields","title":"2.8. Fill in the fields","text":"<p>Fill in the following details in the fields</p> <ul> <li>Conn Id: Create a unique connection identifier, such as \"cdw_connection_YourCDPUsername\".</li> <li>For ex: cdw_connection_dse_1_250204</li> <li>Conn Type: Select Hive Client Wrapper.</li> <li>Host: Enter the hostname from the JDBC connection URL. Do not enter the full JDBC URL.</li> <li>Schema: default</li> <li>Login: Enter your workload username</li> <li>Password: Enter your workload password.</li> </ul> <p>Click Save.</p> <p></p>"},{"location":"guide/03-04/index.html#29-review-record","title":"2.9. Review record","text":"<p>Success</p> <p>A success message appears confirming a new row being added. </p> <p></p>"},{"location":"guide/03-04/index.html#3-review-the-dag-python-file","title":"3. Review the DAG Python file","text":"<p>Now you are ready to use the CDWOperator in your Airflow DAG. In the cde_jobs/ directory, a copy of \"firstdag.py\" has been made and named \"cdw_dag.py\" with the following additions. </p>"},{"location":"guide/03-04/index.html#31-import-operator","title":"3.1. Import Operator","text":"<p>At the top, an Operator has been imported along with other import statements.</p> <pre><code>from cloudera.cdp.airflow.operators.cdw_operator import CDWOperator\n</code></pre> <p></p>"},{"location":"guide/03-04/index.html#32-add-object","title":"3.2. Add Object","text":"<p>At the bottom of the file an instance of the CDWOperator object has been added.</p> <pre><code>cdw_query = \"\"\"\nshow databases;\n\"\"\"\n\ndw_step3 = CDWOperator(\n  task_id='dataset-etl-cdw',\n  dag=example_dag,\n  cli_conn_id='cdw_connection_YourCDPUsername',\n  hql=cdw_query,\n  schema='default',\n  use_proxy_user=False,\n  query_isolation=True\n)\n</code></pre> <p>Notice that the SQL syntax run in the CDW Virtual Warehouse is declared as a separate variable and then passed to the Operator instance as an argument.</p> <p></p>"},{"location":"guide/03-04/index.html#33-update-task-dependencies","title":"3.3.  Update task dependencies","text":"<p>Task dependencies have been updated to include \"dw_step3\":</p> <pre><code>spark_step &gt;&gt; shell &gt;&gt; dw_step3\n</code></pre> <p></p>"},{"location":"guide/03-04/index.html#34-update-variables","title":"3.4. Update variables","text":"<p>DAG names are stored in Airflow and must be unique. Therefore, the variable name of the DAG object instance has been changed to \"airflow_cdw_dag\" and the DAG ID to \"dw_dag\" as shown below.</p> <pre><code>airflow_cdw_dag = DAG(\n    'dw_dag',\n    default_args=default_args,\n    schedule_interval='@daily',\n    catchup=False,\n    is_paused_upon_creation=False\n    )\n</code></pre> <p>Notice the new DAG variable needs to be updated in each Operator as well. The completed DAG file is included in the cde_jobs folder for your convenience.</p> <p></p>"},{"location":"guide/03-04/index.html#35-replace-owner","title":"3.5. Replace Owner","text":"<p>Before moving on, make sure to replace the current value of the 'Owner' with your YourCDPUsername in the default arguments dictionary.</p> <p></p>"},{"location":"guide/03-04/index.html#36-replace-spark-job-name","title":"3.6. Replace Spark job name","text":"<p>Before moving on, make sure to replace the current value of the 'job_name' with sparksql_YourCDPUsername in spark_step block.</p> <p></p>"},{"location":"guide/03-04/index.html#37-replace-cli-connection-id","title":"3.7. Replace CLI Connection ID","text":"<p>Before moving on, make sure to replace the current value of the 'cli_conn_id' with the value cdw_connection_YourCDPUsername used while creating the connection. No other changes are required at this time.</p> <p></p>"},{"location":"guide/03-04/index.html#4-create-a-new-airflow-cde-job","title":"4. Create a new Airflow CDE Job","text":""},{"location":"guide/03-04/index.html#41-select-jobs","title":"4.1. Select Jobs","text":"<p>Navigate to the Jobs page by clicking on the Jobs in the main panel.</p> <p></p>"},{"location":"guide/03-04/index.html#42-click-on-create-job","title":"4.2. Click on Create Job","text":"<p>Click on Create Job to create a new CDE Job.</p> <p></p>"},{"location":"guide/03-04/index.html#43-fill-in-the-details","title":"4.3. Fill in the details","text":"<ul> <li>Job Type: Airflow</li> <li>Name: CDWDag_YourCDPUsername</li> <li>Dag File: File</li> </ul> <p>Click on Upload</p> <p></p>"},{"location":"guide/03-04/index.html#44-upload-file-to-a-resource","title":"4.4. Upload file to a resource","text":"<p>Upload cdw_dag.py to the firstdag CDE Resource you created earlier.</p> <p>Ensure that you have replaced the current value of the 'Owner' with your CDP Username in the default arguments dictionary. Please refer to Step 2.3 in the previous exercise for details. </p> <p>Click on Upload.</p> <p></p>"},{"location":"guide/03-04/index.html#45-select-create-and-run","title":"4.5. Select 'Create and Run'","text":"<p>Select Create and Run button to trigger the job immediately.</p> <p></p>"},{"location":"guide/03-04/index.html#46-job-run-in-progress","title":"4.6. Job run in progress","text":"<p>Please wait for a min for job to run</p> <p></p>"},{"location":"guide/03-04/index.html#47-new-run-is-initiated","title":"4.7. New run is initiated","text":"<p>A success message appears confirming that a new run with ID has been initiated. </p> <p></p>"},{"location":"guide/03-04/index.html#48-review-the-jobs-in-progress","title":"4.8. Review the Jobs in progress","text":"<p>Notice that two CDE Jobs are now in progress. One of them is \"sparksql_dse_1_250204\" (Spark CDE Job) and the other is \"CDWDag_dse_1_250204\" (Airflow CDE Job). The former has been triggered by the execution of the latter. Wait a few moments and allow for the DAG to complete.</p> <p></p>"},{"location":"guide/03-04/index.html#49-click-on-the-dag-link","title":"4.9. Click on the Dag link","text":"<p>Once the Status column shows the Jobs run as succeeded, click on the \"CDWDag_dse_1_250204\" link to access the Job Run page.</p> <p></p>"},{"location":"guide/03-04/index.html#410-select-the-most-recent-run","title":"4.10. Select the most recent Run","text":"<p>This page shows each run along with associated logs, execution statistics, and the Airflow UI.</p> <p>Ensure to select the most recent Run (in the screenshot, the most recent Run ID number is 60)</p> <p></p>"},{"location":"guide/03-04/index.html#411-review-job-run","title":"4.11. Review Job Run","text":"<p>Review the Job Run. Notice the run id in the top section of the page. </p> <p>Click on the Airflow UI tab.</p> <p></p>"},{"location":"guide/03-04/index.html#412-review-airflow-ui-tab","title":"4.12. Review Airflow UI tab","text":"<p>The first landing page lists all tasks along with their status. Notice that the DAG ID, Task ID and Operator columns are populated with the values set in the DAG python files. </p> <p>Next, click on the back arrow on the left side of the screen to navigate to the Airflow UI DAGs view.</p> <p></p>"},{"location":"guide/03-04/index.html#413-explore-dags-view","title":"4.13. Explore DAGs view.","text":"<p>From the DAGs view you can:</p> <ul> <li>Pause/unpause a DAG</li> <li>Filter the list of DAGs to show active, paused, or all DAGs</li> <li>Trigger, refresh, or delete a DAG</li> <li>Navigate quickly to other DAG-specific pages</li> </ul> <p>The DAG name specified in the python file during DAG declaration is dw_dag.</p> <p>Click on it to open the Airflow UI DAG view to drill down with DAG-specific pages.</p> <p></p>"},{"location":"guide/03-04/index.html#414-review-dag","title":"4.14. Review DAG","text":"<p>Here, Airflow provides a number of tabs to increase job observability. Open the Tree View and validate that the job has succeeded.</p> <p></p>"},{"location":"guide/03-04/index.html#5-printing-context-variables-with-the-bashoperator","title":"5. Printing Context Variables with the BashOperator","text":"<p>Info</p> <p>When Airflow runs a task, it collects several variables and passes these to the context argument on the execute() method. These variables hold information about the current task.</p> <p>Open the \"bash_dag.py\" file and examine the contents. Notice that at lines 52-56 a new instance of the BashOperator has been declared with the following entries:</p> <pre><code>also_run_this = BashOperator(\n    task_id='also_run_this',\n    dag=bash_airflow_dag,\n    bash_command='echo \"yesterday={{ yesterday_ds }} | today={{ ds }}| tomorrow={{ tomorrow_ds }}\"',\n)\n</code></pre> <p>Above we printed the \"yesterday_ds\", \"ds\" and tomorrow_ds\" dates. There are many more and you can find the full list here.</p> <p>Variables can also be saved and reused by other operators. We will explore this in the section on XComs.</p> <p></p>"},{"location":"guide/03-04/index.html#6-review-the-dag-python-file","title":"6. Review the DAG python file","text":"<p>Now you are ready to use the CDWOperator in your Airflow DAG. In the cde_jobs/ directory, a copy of \"cdw_dag.py\" has been made and named \"py_dag.py\" with the following additions.</p>"},{"location":"guide/03-04/index.html#61-python-operator","title":"6.1. Python Operator","text":"<p>Lines 60-67 in \"py_dag.py\" show how to use the operator to print out all Conext Variables in one run.</p> <pre><code>def _print_context(**context):\n   print(context)\n\nprint_context = PythonOperator(\n    task_id=\"print_context\",\n    python_callable=_print_context,\n    dag=dag,\n)\n</code></pre> <p></p>"},{"location":"guide/03-04/index.html#62-replace-owner","title":"6.2. Replace Owner","text":"<p>Before moving on, make sure to replace the current value of the 'Owner' with your YourCDPUsername in the default arguments dictionary.</p> <p></p>"},{"location":"guide/03-04/index.html#63-replace-spark-job-name","title":"6.3. Replace Spark job name","text":"<p>Before moving on, make sure to replace the current value of the 'job_name' with sparksql_YourCDPUsername in spark_step block.</p> <p></p>"},{"location":"guide/03-04/index.html#64-replace-cli-connection-id","title":"6.4. Replace CLI Connection ID","text":"<p>Before moving on, make sure to replace the current value of the 'cli_conn_id' with the value cdw_connection_YourCDPUsername used while creating the connection. No other changes are required at this time.</p> <p></p>"},{"location":"guide/03-04/index.html#7-using-the-python-operator","title":"7. Using the Python Operator","text":"<p>Info</p> <p>The PythonOperator allows you to run Python code inside the DAG. This is particularly helpful as it allows you to customize your DAG logic in a variety of ways.</p> <p>The PythonOperator requires implementing a callable inside the DAG file. Then, the method is called from the operator.</p>"},{"location":"guide/03-04/index.html#71-select-jobs","title":"7.1. Select Jobs","text":"<p>Navigate to the Jobs page by clicking on the Jobs in the main panel.</p> <p></p>"},{"location":"guide/03-04/index.html#72-click-on-create-job","title":"7.2. Click on Create Job","text":"<p>Click on Create Job to create a new CDE Job.</p> <p></p>"},{"location":"guide/03-04/index.html#73-fill-in-the-details","title":"7.3. Fill in the details","text":"<ul> <li>Job Type: Airflow</li> <li>Name: pydag_YourCDPUsername</li> <li>Dag File: File</li> <li>Upload py_dag.py to the firstdag CDE Resource you created earlier.</li> </ul> <p>Select 'Create and Run' button to trigger the job immediately.</p> <p></p>"},{"location":"guide/03-04/index.html#74-job-run-in-progress","title":"7.4. Job run in progress","text":"<p>Please wait for a min for job to run</p> <p></p>"},{"location":"guide/03-04/index.html#75-new-run-is-initiated","title":"7.5. New run is initiated","text":"<p>A success message appears confirming that a new run with ID has been initiated. </p> <p></p>"},{"location":"guide/03-04/index.html#76-review-the-jobs-in-progress","title":"7.6. Review the Jobs in progress","text":"<p>Notice that two CDE Jobs are now in progress. One of them is \"sparksql_dse_1_250204\" (Spark CDE Job) and the other is \"pydag_dse_1_250204\" (Airflow CDE Job). The former has been triggered by the execution of the latter. Wait a few moments and allow for the DAG to complete.</p> <p></p>"},{"location":"guide/03-04/index.html#77-click-on-the-dag-link","title":"7.7. Click on the Dag link","text":"<p>Once the Status column shows the Jobs run as succeeded, click on the \"pydag_dse_1_250204\" link to access the Job Run page.</p> <p></p>"},{"location":"guide/03-04/index.html#78-select-the-most-recent-run","title":"7.8. Select the most recent Run","text":"<p>This page shows each run along with associated logs, execution statistics, and the Airflow UI.</p> <p>Ensure to select the most recent Run (in the screenshot, the most recent Run ID number is 68)</p> <p></p>"},{"location":"guide/03-04/index.html#79-review-job-run","title":"7.9. Review Job Run","text":"<p>Review the Job Run. Notice the run id in the top section of the page. </p> <p>Click on the Logs tab.</p> <p></p>"},{"location":"guide/03-04/index.html#710-select-dag-task","title":"7.10. Select DAG Task","text":"<p>Ensure to select the correct Airflow task which in this case is \"print_context\":</p> <p></p>"},{"location":"guide/03-04/index.html#711-review-dag-task","title":"7.11. Review DAG Task","text":"<p>Review the print_context task</p> <p>Scroll to the bottom and validate the output.</p> <p></p>"},{"location":"guide/03-04/index.html#8-end-of-the-exercise","title":"8. End of the Exercise","text":""},{"location":"guide/03-05/index.html","title":"03-05 More Airflow DAG Features","text":""},{"location":"guide/03-05/index.html#using-the-simplehttpoperator","title":"Using the SimpleHttpOperator","text":"<p>Info</p> <p>You can use the SimpleHttpOperator to call HTTP requests and get the response text back. The Operator can help when you need to interact with 3rd party systems, APIs, and perform actions based on complex control flow logic.</p> <p>In the following example we will send a request to the Chuck Norris API from the Airflow DAG. Before updating the DAG file, we need to set up a new Airflow Connection and Airflow Variables.</p>"},{"location":"guide/03-05/index.html#1-creating-an-http-airflow-connection","title":"1. Creating an HTTP Airflow Connection","text":""},{"location":"guide/03-05/index.html#11-select-administration","title":"1.1. Select Administration","text":"<p>Navigate to the main menu and select Administration option from the panel. </p> <p></p>"},{"location":"guide/03-05/index.html#12-click-on-cluster-details","title":"1.2. Click on Cluster Details","text":"<p>In the Virtual Clusters column, click Cluster Details for the virtual cluster.</p> <p></p>"},{"location":"guide/03-05/index.html#13-click-airflow-ui","title":"1.3.  Click AIRFLOW UI","text":""},{"location":"guide/03-05/index.html#14-click-the-connection-link","title":"1.4. Click the Connection link","text":"<p>From the Airflow UI, click the Connection link from the Admin menu.</p> <p></p>"},{"location":"guide/03-05/index.html#15-add-a-new-record","title":"1.5. Add a new record","text":"<p>Click the plus sign to add a new record.</p> <p></p>"},{"location":"guide/03-05/index.html#16-fill-in-the-fields","title":"1.6. Fill in the fields","text":"<p>Configure the connection by entering the following values for the following parameters. Leave the remaining entries blank.</p> <ul> <li>Connection ID: chuck_norris_connection_YourCDPUsername</li> <li>Connection Type: HTTP</li> <li>Host: https://matchilling-chuck-norris-jokes-v1.p.rapidapi.com</li> </ul> <p>Click Save.</p> <p></p>"},{"location":"guide/03-05/index.html#17-review-record","title":"1.7. Review record","text":"<p>A success message appears confirming a new row being added. </p> <p></p>"},{"location":"guide/03-05/index.html#2-creating-airflow-variables","title":"2. Creating Airflow Variables","text":"<p>Info</p> <p>Airflow Variables allow you to parameterize your operators. Airflow Variables are used as environment variables for the DAG. Therefore, if you are looking to temporarily store operator results in the DAG and pass values to downstream operators you should use XComs (shown in the next section).</p> <p>In our example, we will use them to pass an API KEY and HOST value to the SimpleHttpOperator below. To set up Airflow Variables, navigate back to the CDE Virtual Cluster Service Details page and open the Airflow UI.</p>"},{"location":"guide/03-05/index.html#21-click-the-connection-link","title":"2.1. Click the Connection link","text":"<p>From the Airflow UI, click on the \"Variables\" tab under the \"Admin\" drop down at the top of the page.</p> <p></p>"},{"location":"guide/03-05/index.html#22-add-a-new-record","title":"2.2. Add a new record","text":"<p>Click the plus sign to add a new record.</p> <p></p>"},{"location":"guide/03-05/index.html#23-create-first-variable","title":"2.3. Create first variable","text":"<p>Create a variable with the following entries:</p> <ul> <li>Key: rapids_api_host_YourCDPUsername</li> <li>Value: matchilling-chuck-norris-jokes-v1.p.rapidapi.com</li> </ul> <p>Click Save</p> <p></p>"},{"location":"guide/03-05/index.html#24-review-new-row","title":"2.4. Review new row","text":"<p>Success</p> <p>A success message appears confirming that a row has been added. </p> <p>Click the plus sign to add a new record.</p> <p></p>"},{"location":"guide/03-05/index.html#25-create-second-variable","title":"2.5. Create second variable","text":"<p>Create a new variable with the following entries:</p> <ul> <li>Key: rapids_api_key_YourCDPUsername</li> <li>Value: f16c49e390msh7e364a479e33b3dp10fff7jsn6bc84b000b75</li> </ul> <p>Click Save</p> <p></p>"},{"location":"guide/03-05/index.html#26-review-second-row","title":"2.6. Review second row","text":"<p>Success</p> <p>A success message appears confirming that the new row has been added.</p> <p></p>"},{"location":"guide/03-05/index.html#3-working-with-the-dag","title":"3. Working with the DAG","text":"<p>Next, open \"http_dag.py\" and familiarize yourself with the code. The code relevant to the new operator is used between lines 71 and 92.</p>"},{"location":"guide/03-05/index.html#31-import-variable","title":"3.1. Import Variable","text":"<p>Notice that at line 11 we are importing the Variable type from the airflow.models module.</p> <p></p>"},{"location":"guide/03-05/index.html#32-create-airflow-variables","title":"3.2. Create Airflow Variables","text":"<p>We are then creating two Airflow Variables at lines 71 and 72.</p> <p></p>"},{"location":"guide/03-05/index.html#33-map-parameter","title":"3.3. Map parameter","text":"<p>The \"http_conn_id\" parameter is mapped to the Connection ID you configured in the prior step.</p> <p></p>"},{"location":"guide/03-05/index.html#34-validate-response","title":"3.4. Validate response","text":"<p>The \"response_check\" parameter allows you to specify a python method to validate responses. This is the \"handle_response\" method declared at line 74.</p> <pre><code>api_host = Variable.get(\"rapids_api_host\")\napi_key = Variable.get(\"rapids_api_key\")\n\ndef handle_response(response):\n    if response.status_code == 200:\n        print(\"Received 200 Ok\")\n        return True\n    else:\n        print(\"Error\")\n        return False\n\nhttp_task = SimpleHttpOperator(\n    task_id=\"chuck_norris_task\",\n    method=\"GET\",\n    http_conn_id=\"chuck_norris_connection\",\n    endpoint=\"/jokes/random\",\n    headers={\"Content-Type\":\"application/json\",\n            \"X-RapidAPI-Key\": api_key,\n            \"X-RapidAPI-Host\": api_host},\n    response_check=lambda response: handle_response(response),\n    dag=http_dag\n)\n</code></pre> <p></p>"},{"location":"guide/03-05/index.html#35-replace-owner","title":"3.5. Replace Owner","text":"<p>Make sure to replace the current value of the 'Owner' with your YourCDPUsername in the default arguments dictionary.</p> <p></p>"},{"location":"guide/03-05/index.html#36-replace-spark-job-name","title":"3.6. Replace Spark job name","text":"<p>Make sure to replace the current value of the 'job_name' with sparksql_YourCDPUsername in spark_step block.</p> <p></p>"},{"location":"guide/03-05/index.html#37-replace-cli-connection-id","title":"3.7. Replace CLI Connection ID","text":"<p>Make sure to replace the current value of the 'cli_conn_id' with the value cdw_connection_YourCDPUsername used while creating the connection.</p> <p></p>"},{"location":"guide/03-05/index.html#38-replace-airflow-variables","title":"3.8. Replace Airflow Variables","text":"<p>Make sure to replace the current value of the 'api_host' and api_key with the value rapids_api_host_YourCDPUsername and rapids_api_key_YourCDPUsername used while creating the variables.</p> <p></p>"},{"location":"guide/03-05/index.html#39-replace-http-cli-connection-id","title":"3.9. Replace HTTP CLI Connection ID","text":"<p>Make sure to replace the current value of the 'http_conn_id' with the value chuck_norris_connection_YourCDPUsername used while creating the connection.</p> <p></p>"},{"location":"guide/03-05/index.html#4-create-a-new-airflow-cde-job","title":"4. Create a new Airflow CDE Job","text":""},{"location":"guide/03-05/index.html#41-select-jobs","title":"4.1. Select Jobs","text":"<p>Navigate to the Jobs page by clicking on the Jobs in the main panel.</p> <p></p>"},{"location":"guide/03-05/index.html#42-click-on-create-job","title":"4.2. Click on Create Job","text":"<p>Click on Create Job to create a new CDE Job.</p> <p></p>"},{"location":"guide/03-05/index.html#43-fill-in-the-details","title":"4.3. Fill in the details","text":"<ul> <li>Job Type: Airflow</li> <li>Name: ChuckNorris_YourCDPUsername</li> <li>Dag File: File</li> <li>Upload http_dag.py to the firstdag CDE Resource you created earlier.</li> </ul> <p>Select 'Create and Run' button to trigger the job immediately.</p> <p></p>"},{"location":"guide/03-05/index.html#44-job-run-in-progress","title":"4.4. Job run in progress","text":"<p>Please wait for a min for job to run</p> <p></p>"},{"location":"guide/03-05/index.html#45-new-run-is-initiated","title":"4.5. New run is initiated","text":"<p>Success</p> <p>A success message appears confirming that a new run with ID has been initiated. </p> <p></p>"},{"location":"guide/03-05/index.html#46-review-the-jobs-in-progress","title":"4.6. Review the Jobs in progress","text":"<p>Notice that two CDE Jobs are now in progress. One of them is \"sparksq_dse_1_250204\" (Spark CDE Job) and the other is \"ChuckNorris_dse_1_250204\" (Airflow CDE Job). The former has been triggered by the execution of the latter. Wait a few moments and allow for the DAG to complete.</p> <p></p>"},{"location":"guide/03-05/index.html#47-click-on-the-dag-link","title":"4.7. Click on the Dag link","text":"<p>Once the Status column shows the Jobs run as succeeded, click on the \"ChuckNorris_dse_1_250204\" link to access the Job Run page.</p> <p></p>"},{"location":"guide/03-05/index.html#48-select-the-most-recent-run","title":"4.8. Select the most recent Run","text":"<p>This page shows each run along with associated logs, execution statistics, and the Airflow UI.</p> <p>Ensure to select the most recent Run (in the screenshot, the most recent Run ID number is 72)</p> <p></p>"},{"location":"guide/03-05/index.html#49-review-job-run","title":"4.9. Review Job Run","text":"<p>Review the Job Run. Notice the run id in the top section of the page. </p> <p>Click on the Logs tab.</p> <p></p>"},{"location":"guide/03-05/index.html#410-select-dag-task","title":"4.10. Select DAG Task","text":"<p>Ensure to select the correct Airflow task which in this case is \"chuck_norris_task\":</p> <p></p>"},{"location":"guide/03-05/index.html#411-review-dag-task","title":"4.11.  Review DAG Task","text":"<p>Review the chuck_norris_task task</p> <p>Scroll to the bottom and validate the output.</p> <p></p>"},{"location":"guide/03-05/index.html#5-using-xcoms","title":"5. Using XComs","text":"<p>Although the request in the prior step was successful the operator did not actually return the response to the DAG. XComs (short for \u201ccross-communications\u201d) are a mechanism that let Tasks talk to each other, as by default Tasks are entirely isolated and may be running on entirely different machines.</p> <p>Note</p> <p>Practically XComs allow your operators to store results into a governed data structure and then reuse the values within the context of different operators. An XCom is identified by a key (essentially its name), as well as the task_id and dag_id it came from. They are only designed for small amounts of data; do not use them to pass around large values, like dataframes.</p> <p>Open \"xcom_dag.py\" and familiarize yourself with the code. Notice the following changes in the code between lines 82 and 103:</p>"},{"location":"guide/03-05/index.html#51-add-an-argument","title":"5.1. Add an argument","text":"<p>At line 92 we added a \"do_xcom_push=True\" argument. This allows the response to be temporarily saved in the DAG.</p> <p></p>"},{"location":"guide/03-05/index.html#52-add-python-method","title":"5.2. Add python method","text":"<p>At line 95 we introduced a new Python method \"print_chuck_norris_quote\" and at line 96 we use the built-in \"xcom_pull\" method to retrieve the temporary value from the SimpleHttpOperator task.</p> <p></p>"},{"location":"guide/03-05/index.html#53-declare-operator","title":"5.3. Declare operator","text":"<p>At line 62, we declared a new Python Operator running the method above.</p> <pre><code>http_task = SimpleHttpOperator(\n    task_id=\"chuck_norris_task\",\n    method=\"GET\",\n    http_conn_id=\"chuck_norris_connection\",\n    endpoint=\"/jokes/random\",\n    headers={\"Content-Type\":\"application/json\",\n            \"X-RapidAPI-Key\": api_key,\n            \"X-RapidAPI-Host\": api_host},\n    response_check=lambda response: handle_response(response),\n    dag=xcom_dag,\n    do_xcom_push=True\n)\n\ndef _print_chuck_norris_quote(**context):\n    return context['ti'].xcom_pull(task_ids='chuck_norris_task')\n\nreturn_quote = PythonOperator(\n    task_id=\"print_quote\",\n    python_callable=_print_chuck_norris_quote,\n    dag=xcom_dag\n)\n</code></pre> <p></p>"},{"location":"guide/03-05/index.html#54-replace-owner","title":"5.4. Replace Owner","text":"<p>Make sure to replace the current value of the 'Owner' with your YourCDPUsername in the default arguments dictionary.</p> <p></p>"},{"location":"guide/03-05/index.html#55-replace-spark-job-name","title":"5.5. Replace Spark job name","text":"<p>Make sure to replace the current value of the 'job_name' with sparksql_YourCDPUsername in spark_step block.</p> <p></p>"},{"location":"guide/03-05/index.html#56-replace-cli-connection-id","title":"5.6. Replace CLI Connection ID","text":"<p>Make sure to replace the current value of the 'cli_conn_id' with the value cdw_connection_YourCDPUsername used while creating the connection.</p> <p></p>"},{"location":"guide/03-05/index.html#57-replace-airflow-variables","title":"5.7. Replace Airflow Variables","text":"<p>Make sure to replace the current value of the 'api_host' and api_key with the value rapids_api_host_YourCDPUsername and rapids_api_key_YourCDPUsername used while creating the variables.</p> <p></p>"},{"location":"guide/03-05/index.html#58-replace-http-cli-connection-id","title":"5.8. Replace HTTP CLI Connection ID","text":"<p>Make sure to replace the current value of the 'http_conn_id' with the value chuck_norris_connection_YourCDPUsername used while creating the connection.</p> <p></p>"},{"location":"guide/03-05/index.html#6-create-a-new-airflow-cde-job","title":"6. Create a new Airflow CDE Job","text":""},{"location":"guide/03-05/index.html#61-select-jobs","title":"6.1. Select Jobs","text":"<p>Navigate to the Jobs page by clicking on the Jobs in the main panel.</p> <p></p>"},{"location":"guide/03-05/index.html#62-click-on-create-job","title":"6.2. Click on Create Job","text":"<p>Click on Create Job to create a new CDE Job.</p> <p></p>"},{"location":"guide/03-05/index.html#63-fill-in-the-details","title":"6.3. Fill in the details","text":"<ul> <li>Job Type: Airflow</li> <li>Name: XcomDag_YourCDPUsername</li> <li>Dag File: File</li> <li>Upload xcom_dag.py to the firstdag CDE Resource you created earlier.</li> </ul> <p>Select 'Create and Run' button to trigger the job immediately.</p> <p></p>"},{"location":"guide/03-05/index.html#64-job-run-in-progress","title":"6.4. Job run in progress","text":"<p>Please wait for a min for job to run</p> <p></p>"},{"location":"guide/03-05/index.html#65-new-run-is-initiated","title":"6.5. New run is initiated","text":"<p>Success</p> <p>A success message appears confirming that a new run with ID has been initiated. </p> <p></p>"},{"location":"guide/03-05/index.html#66-review-the-jobs-in-progress","title":"6.6. Review the Jobs in progress","text":"<p>Notice that two CDE Jobs are now in progress. One of them is \"sparksql_dse_1_250204\" (Spark CDE Job) and the other is \"XcomDags_dse_1_250204\" (Airflow CDE Job). The former has been triggered by the execution of the latter. Wait a few moments and allow for the DAG to complete.</p> <p></p>"},{"location":"guide/03-05/index.html#67-click-on-the-firstdag-link","title":"6.7. Click on the FirstDag link","text":"<p>Once the Status column shows the Jobs run as succeeded, click on the \"XcomDags_dse_1_250204\" link to access the Job Run page.</p> <p></p>"},{"location":"guide/03-05/index.html#68-select-the-most-recent-run","title":"6.8. Select the most recent Run","text":"<p>This page shows each run along with associated logs, execution statistics, and the Airflow UI.</p> <p>Ensure to select the most recent Run (in the screenshot, the most recent Run ID number is 98)</p> <p></p>"},{"location":"guide/03-05/index.html#69-review-job-run","title":"6.9. Review Job Run","text":"<p>Review the Job Run. Notice the run id in the top section of the page. </p> <p>Click on the Logs tab.</p> <p></p>"},{"location":"guide/03-05/index.html#610-select-dag-task","title":"6.10. Select DAG Task","text":"<p>Ensure to select the correct Airflow task which in this case is \"print_quote\":</p> <p></p>"},{"location":"guide/03-05/index.html#611-review-dag-task","title":"6.11. Review DAG Task","text":"<p>Review the print_quote task</p> <p>Scroll all the way down and validate that a Chuck Norris quote has been printed out. Which one did you get?</p> <p></p>"},{"location":"guide/03-05/index.html#7-end-of-the-exercise","title":"7. End of the Exercise","text":""},{"location":"guide/04-01/index.html","title":"04-01_Using_Cloudera_Datawarehouse","text":""},{"location":"guide/04-02/index.html","title":"Cloudera Data Warehouse","text":""},{"location":"guide/04-02/index.html#pre-requisites","title":"Pre-requisites","text":"<ol> <li> <p>Laptop with a supported OS (Windows 7 not supported) or MacBook.     Please disable any VPNs.</p> </li> <li> <p>A modern browser - Google Chrome (IE, Firefox, Safari not     supported).</p> </li> <li> <p>Wi-Fi Internet connection with minimal security firewall on laptop     and network.     and please do not copy/paste strings with trailing characters     while executing the exercise.</p> </li> </ol>"},{"location":"guide/04-02/index.html#preface","title":"Preface","text":"<p>Working for an Aircraft Engine company, the company wants to increase competitive advantage in two keyways:</p> <p>(1) Engineer better, more fault tolerant aircraft engines.</p> <p>(2) Be proactive in predictive maintenance on engines, and faster discovery-to-fix in new engine designs.</p> <p>This will be a three-phase plan:</p> <p>(1) Phase One: Understand how our current engines contribute to airline flight delays and fix for future engines.</p> <p>(2) Phase Two: Implement an ongoing reporting service to support ongoing engineering efforts to continuously improve engines based on delay data.</p> <p>(3) Phase Three: Move to real-time analysis to fix things before they break both in engines already sold, and in new engine designs.</p> <p>To do this, we're going to build a data warehouse &amp; data lakehouse to create reports that engineers can use to improve our engines.</p> <p>We will dive into this scenario to show Cloudera Data Warehouse (CDW) is used to enable the Aircraft company to gain competitive advantage - and at the same time it highlights the performance and automation capabilities that help ensure performance is maintained while controlling costs.</p> <p>The Hands on Labs will take you through how to use the Cloudera Data Warehouse service to quickly explore raw data, create curated versions of the data for simple reporting and dashboarding, and then scale up usage of the curated data by exposing it to more users.</p> <p>ER - Diagram of the data</p> <p>(1) Fact Table: flights (86M rows)</p> <p>(2) Dimension Table: airlines (1.5k rows), airports (3.3k rows) and planes (5k rows)</p> <p></p>"},{"location":"guide/04-02/index.html#high-level-steps","title":"High-Level Steps","text":"<p>Below are the high-level steps for what we will be doing in the exercise.</p> <p>[Step 1 &amp; 2]: General introduction to CDW to get ourselves oriented for the exercise.</p> <pre><code>(a) Get familiar with CDW on CDP and set up our first VW to start working.\n(b) Get the storage information.\n</code></pre> <p>[Step 3]: Set it up.</p> <pre><code>(a) Wrangle our first set of data - sent to us as a series of .csv files exported from \u201csomewhere else\u201d.\n(b) Monitor the VW and watch as it scales up and down, suspends, etc.\n(c) Start digging into the data - looking for \u201cneedle in a haystack\u201d - running a complex query that will find which engines seem to be correlated to airplane delays for any reason.\n</code></pre> <p>[Step 4]: Making it better.</p> <pre><code>(a) Start curating data and building a data lakehouse to improve quality by tweaking data, performance by optimizing schema structures, and ensure reliability and trustworthiness of the data through snapshots, time travel, and rollback.\n(b) Create Hive ACID tables and tweak data for consistency (ex: airline name changes - ensure reporting is consistent with the new name to avoid end user confusion, a new airline joins our customer list, make sure they\u2019re tracked for future data collection, etc..).\n(c) Migrate Tables to Iceberg (We want snapshot and rollback).\n(d) Create new Iceberg tables (we want partitioning).\n</code></pre> <p>[Step 5]: Optimizing for production.</p> <pre><code>(a) Loading more data - change partitioning to maintain performance (NOTE:  Ongoing ELT = CDE?).\n(b) Bad data is loaded - use time travel to detect, and rollback to resolve.\n(c) Introduce materialized views to support scaling to 1000\u2019s of simultaneous users.\n(d) Monitor, report, kill queries that run amok, etc.\n</code></pre> <p>[Step 6]: Cloudera Data Visualization</p> <pre><code>(a) Data Modeling for the lakehouse.\n(b) Data Visualization for insights.\n</code></pre>"},{"location":"guide/04-02/index.html#step-1-getting-started","title":"Step 1: Getting Started","text":"<p>Note</p> <pre><code>Please make sure that you are working on the browser (Chrome)\n![chrome](images/step1a/chrome.png) in **INCOGNITO MODE**\n![incog](images/step1a/incog.png) only for the entirety of the\nworkshop.\n</code></pre>"},{"location":"guide/04-02/index.html#step-1a-youll-need-the-following","title":"Step 1(a): You'll need the following","text":"<p>Please note the following:</p> <p>(a) <code>Login Username</code>: Your Instructor would provide this</p> <p>(b) <code>Login Password</code>: Your Instructor would provide this</p> <p>(c) <code>CDP Workload User</code> (<code>${user_id}</code> or <code>&lt;user_id&gt;</code>): same as Login Username</p> <p>(d) <code>CDP Workload Password</code>: <code>You will</code> set it in <code>Step 1(b)</code> of the exercise - same as login passoword</p> <p>(e) <code>Hive Virtual Warehouse</code>: It has already been provisioned by the DevOps team.</p> <p>(f) <code>Impala Virtual Warehouse</code>: It has already been provisioned by the DevOps team.</p> <p>In the labs when you see:</p> <p><code>${user_id}</code> or <code>&lt;user_id&gt;</code> - this will indicate to use your User (<code>CDP Workload User</code>) as item (c) above.</p>"},{"location":"guide/04-02/index.html#step-1b-define-cdp-workload-password","title":"Step 1(b): Define CDP Workload Password","text":"<p><code>Suggestion:</code> <code>Ideal way to work through this exercise will be to have 2 Incognito mode browsers opened. One to be used in a way that you can follow instructions using Markdown and the other one to be used for the workshop related screens.</code></p> <p>Please use the login URL provided by the Instructor</p> <p>Enter the <code>Login Username</code> and <code>Login Password</code> shared by your instructor. </p> <p>You should be able to get the following home page of CDP Public Cloud.</p> <p> You will need to define your <code>CDP Workload Password</code> that will be used to access non-SSO interfaces. You may read more about it here. Please keep it with you. If you have forgotten it, you will be able to repeat this process and define another one.</p> <ol> <li> <p>Click on your <code>user name (Ex: wuser00@workshop.com</code>) at the lower     left corner.</p> </li> <li> <p>Click on the <code>Profile</code> option.</p> </li> </ol> <p></p> <ol> <li> <p>Click option <code>Set Workload Password</code>.</p> </li> <li> <p>Enter a suitable <code>Password</code> and <code>Confirm Password</code>.</p> </li> <li> <p>Click the button <code>Set Workload Password</code>.</p> </li> </ol> <p></p> <p></p> <p>Check that you got the message - <code>Workload password is currently set</code> or alternatively, look for a message next to <code>Workload Password</code> which says <code>(Workload password is currently set)</code></p> <p></p>"},{"location":"guide/04-02/index.html#step-2-getting-storage-information","title":"Step 2: Getting storage Information","text":"<p>Click on Environment tab and select your environment. </p> <p>Click on Summary tab tab and scroll down. </p> <p>Go to Logs Storage and Audits &amp; Copy the storage info as highlighted below This will be your storage location, you will need this for the upcoming steps.</p> <p>eg. <code>${storage}</code> = cdp-storage-sanket-500-class-240801/datalake-sanket-500-class-240801 </p>"},{"location":"guide/04-03/index.html","title":"Step 3: Cloudera Data Warehouse - Raw Layer (Direct Cloud Object Storage Access)","text":"<p>The objective of this step is to create External tables on top of raw CSV files sitting in cloud storage (In this case it has been stored in AWS S3 by the instructor) and then run few queries to access the data via SQL using HUE.</p>"},{"location":"guide/04-03/index.html#31-open-hue-for-cdw-virtual-warehouse-vw-hive","title":"3.1 Open Hue for CDW Virtual Warehouse - <code>vw-hive</code>","text":"<ul> <li> <p>Click on the  button on the right upper     corner of <code>vw-hive</code> as shown in the screenshot below.     </p> </li> <li> <p>Create new databases. Enter the following query and then make sure     that you enter the user assigned to you. In the screenshot the user     is <code>wuser00</code>.</p> </li> </ul> <pre><code>CREATE DATABASE ${user_id}_airlines_raw;\n\nCREATE DATABASE ${user_id}_airlines;\n</code></pre> <p></p> <ul> <li>There may be many databases, look for the 2 that start with your     <code>&lt;user_id&gt;</code>. Run the following SQL to see the 2 databases that     you created just now.</li> </ul> <pre><code>SHOW DATABASES;\n</code></pre> <p></p>"},{"location":"guide/04-03/index.html#32-run-the-following-ddl-in-hue-for-the-cdw-virtual-warehouse-vw-hive","title":"3.2 Run the following DDL in HUE for the CDW Virtual Warehouse - <code>vw-hive</code>","text":"<p>This will create External Tables on CSV Data Files that have been uploaded previously by your instructor in AWS S3. This provides a fast way to allow SQL layer on top of data in cloud storage.</p> <ul> <li>Copy paste the following into HUE.</li> </ul> <pre><code>drop table if exists ${user_id}_airlines_raw.flights_csv;\nCREATE EXTERNAL TABLE ${user_id}_airlines_raw.flights_csv(month int, dayofmonth int, dayofweek int, deptime int, crsdeptime int, arrtime int, crsarrtime int, uniquecarrier string, flightnum int, tailnum string, actualelapsedtime int, crselapsedtime int, airtime int, arrdelay int, depdelay int, origin string, dest string, distance int, taxiin int, taxiout int, cancelled int, cancellationcode string, diverted string, carrierdelay int, weatherdelay int, nasdelay int, securitydelay int, lateaircraftdelay int, year int)\nROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\n'\nSTORED AS TEXTFILE LOCATION 's3a://${storage}/course-data/meta-cdw-workshop/airlines-raw/airlines-csv/flights' tblproperties(\"skip.header.line.count\"=\"1\");\n\ndrop table if exists ${user_id}_airlines_raw.planes_csv;\nCREATE EXTERNAL TABLE ${user_id}_airlines_raw.planes_csv(tailnum string, owner_type string, manufacturer string, issue_date string, model string, status string, aircraft_type string, engine_type string, year int)\nROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\n'\nSTORED AS TEXTFILE LOCATION 's3a://${storage}/course-data/meta-cdw-workshop/airlines-raw/airlines-csv/planes' tblproperties(\"skip.header.line.count\"=\"1\");\n\ndrop table if exists ${user_id}_airlines_raw.airlines_csv;\nCREATE EXTERNAL TABLE ${user_id}_airlines_raw.airlines_csv(code string, description string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\n'\nSTORED AS TEXTFILE LOCATION 's3a://${storage}/course-data/meta-cdw-workshop/airlines-raw/airlines-csv/airlines' tblproperties(\"skip.header.line.count\"=\"1\");\n\ndrop table if exists ${user_id}_airlines_raw.airports_csv;\nCREATE EXTERNAL TABLE ${user_id}_airlines_raw.airports_csv(iata string, airport string, city string, state DOUBLE, country string, lat DOUBLE, lon DOUBLE)\nROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\n'\nSTORED AS TEXTFILE LOCATION 's3a://${storage}/course-data/meta-cdw-workshop/airlines-raw/airlines-csv/airports' tblproperties(\"skip.header.line.count\"=\"1\");\n</code></pre> <p>Notice the following screenshot corresponding to the above SQL.  Makre sure to replace user_id &amp; storage.</p> <p></p> <ul> <li>Check all the 4 tables were created.</li> </ul> <pre><code>USE ${user_id}_airlines_raw;\n\nSHOW TABLES;\n</code></pre> <p>Make sure that 4 tables (<code>airlines_csv</code>, <code>airports_csv</code>, <code>flights_csv</code>, <code>planes_csv</code>) are created as shown below.</p> <p></p>"},{"location":"guide/04-03/index.html#33-run-the-following-ddl-in-hue-for-the-cdw-virtual-warehouse-vw-impala","title":"3.3 Run the following DDL in HUE for the CDW Virtual Warehouse - <code>vw-impala</code>.","text":"<ul> <li>Go to the page where now you will access HUE of an Impala virtual     warehouse. Click on <code>HUE</code> for <code>vw-impala</code> as shown in the     screenshot below. </li> </ul> <p>Now, copy paste the following in the HUE browser and click on Run as shown below.</p> <pre><code>select count(*) from ${user_id}_airlines_raw.flights_csv;\n</code></pre> <p></p> <p>Notice that while the query is executing, continue to the next step. Once the query returns you will see the following in the Results - <code>the flights_csv table has over 86 million records</code>. </p> <ul> <li>Go back to the CDP Console and observe the Impala Virtual Warehouse     <code>vw-impala</code>.    </li> </ul> <p>Here, you'll notice that the warehouse is now at a state where it is not executing any queries and hence, the node count would be low and as the users will run their queries it will scale up or down depending upon the need of resources or lack of it when queries are not run.</p> <p>Note</p> <pre><code>Since this workshop has many users logged in and the virtual impala warehouse is always ON at this point, the actual behavior might differ from what you see in the screenshot. The idea is to convey that the virtual warehouse scales up and scales down.`\n</code></pre> <ul> <li>Run the following query to start analyzing the data - \"Find the     needle in the haystack\" query.</li> </ul> <pre><code>SELECT model,\n       engine_type\nFROM ${user_id}_airlines_raw.planes_csv\nWHERE planes_csv.tailnum IN\n    (SELECT tailnum\n     FROM\n       (SELECT tailnum,\n               count(*),\n               avg(depdelay) AS avg_delay,\n               max(depdelay),\n               avg(taxiout),\n               avg(cancelled),\n               avg(weatherdelay),\n               max(weatherdelay),\n               avg(nasdelay),\n               max(nasdelay),\n               avg(securitydelay),\n               max(securitydelay),\n               avg(lateaircraftdelay),\n               max(lateaircraftdelay),\n               avg(airtime),\n               avg(actualelapsedtime),\n               avg(distance)\n        FROM ${user_id}_airlines_raw.flights_csv\n        WHERE tailnum IN ('N194JB',\n                          'N906S',\n                          'N575ML',\n                          'N852NW',\n                          'N000AA')\n        GROUP BY tailnum) AS delays);\n</code></pre> <p></p> <ul> <li> <p>Go back to the CDP console to observe the behavior of scaling     up/down of virtual warehouses.     </p> </li> <li> <p>Check in the Hue browser and the query shows the result as follows.     Observe the amount of time taken to run this query.     </p> </li> </ul>"},{"location":"guide/04-04/index.html","title":"Step 4: Data Lakehouse - Hive &amp; Iceberg Table Format","text":"<p>In this step we will take steps to make use of Hive and Iceberg Table formats to provide us with best of both world scenarios in our Data Lakehouse. We will </p> <p>4.1 Create a curated layer from RAW CSV Tables (Created in Step 3). Curated layer will be created in <code>&lt;user_id&gt;_airlines</code> - This will be our 'Data Lakehouse'. Data Lakehouse will be a combination of 2 Table Formats (Hive &amp; Iceberg).</p> <p>4.2 Migrate over time from Hive to Iceberg Table format and hence have the choice to not have to migrate everything at once.</p> <p>4.2.1 Utilize the table Migration feature.</p> <p>4.2.2 Use Create Table as Select (CTAS).</p>"},{"location":"guide/04-04/index.html#41-curated-layer-creation","title":"4.1 Curated layer creation","text":"<ul> <li> <p>Make sure that you are using the HUE of <code>vw-hive</code>. Else, click on     <code>HUE</code> and go to the HUE browser.     </p> </li> <li> <p>Create <code>planes</code> table in <code>Hive</code> table format and stored in <code>parquet</code>     file format.</p> </li> </ul> <pre><code>drop table if exists ${user_id}_airlines.planes;\n\nCREATE EXTERNAL TABLE ${user_id}_airlines.planes (\n  tailnum STRING, owner_type STRING, manufacturer STRING, issue_date STRING,\n  model STRING, status STRING, aircraft_type STRING,  engine_type STRING, year INT\n)\nSTORED AS PARQUET\nTBLPROPERTIES ('external.table.purge'='true');\n</code></pre> <p></p> <ul> <li>Load <code>planes</code> table with data from the Raw layer table <code>planes_csv</code>.</li> </ul> <pre><code>INSERT INTO ${user_id}_airlines.planes\n  SELECT * FROM ${user_id}_airlines_raw.planes_csv;\n</code></pre> <p></p> <ul> <li>Switch to <code>&lt;user_id&gt;_airlines</code> database by clicking the <code>&lt;</code> option     to the left of <code>default</code> database. Click on <code>&lt;user_id&gt;_airlines</code>     database. You should see the <code>planes</code> table.</li> </ul> <p></p> <p></p> <p> -   Run the SQL to see if the <code>planes</code> table was loaded correctly.     Since, <code>parquet</code> uses highly efficient column-wise compression which     occupies much disk space than CSV file and hence makes it faster to     scan data in the <code>parquet</code> file.</p> <pre><code>SELECT * FROM ${user_id}_airlines.planes LIMIT 100;\n</code></pre> <p>Scroll down to see more values for the data.</p> <p></p> <p>Scroll down to see more values. </p> <ul> <li>Execute the following command.</li> </ul> <pre><code>DESCRIBE FORMATTED ${user_id}_airlines.planes;\n</code></pre> <p></p> <p>In the output look for the following.</p> <p>(a) Location: <code>s3a://\u2026\u200b/warehouse/tablespace/external/hive/wuser00_airlines.db/planes</code></p> <p>(b) Table Type: <code>EXTERNAL_TABLE</code></p> <p>(c) SerDe Library: <code>org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe</code></p> <p></p> <ul> <li>Create <code>airlines</code> table in <code>Hive</code> table format and <code>orc</code> file     format. This table should also be fully <code>ACID</code> capable. We will use     <code>Create Table As Select (CTAS)</code>. Since, <code>airlines</code> table can change     we need the ability to <code>Insert/Update/Delete</code> records.</li> </ul> <pre><code>drop table if exists ${user_id}_airlines.airlines_orc;\nCREATE TABLE ${user_id}_airlines.airlines_orc\nSTORED AS ORC\nAS\n  SELECT * FROM ${user_id}_airlines_raw.airlines_csv;\n</code></pre> <p></p> <ul> <li>Run the following query to check data in the <code>airlines_orc</code> table     and it should return only 1 row for code 'UA'.</li> </ul> <pre><code>SELECT * FROM ${user_id}_airlines.airlines_orc WHERE code IN (\"UA\",\"XX\",\"PAW\");\n</code></pre> <p></p> <ul> <li>We shall now add a new record to the <code>airlines_orc</code> table to see     some Hive ACID capabilities.</li> </ul> <pre><code>INSERT INTO ${user_id}_airlines.airlines_orc VALUES(\"PAW\",\"Paradise Air\");\n</code></pre> <p></p> <ul> <li>Now, let's create a new table called <code>airlines_dim_updates</code> and     insert 2 new records for <code>United Airlines</code> with code <code>UA</code> and     <code>Get Out of My Airway!</code> with code <code>XX</code>.</li> </ul> <pre><code>drop table if exists ${user_id}_airlines.airlines_dim_updates;\nCREATE EXTERNAL TABLE ${user_id}_airlines.airlines_dim_updates(code string, description string) tblproperties(\"external.table.purge\"=\"true\");\n\nINSERT INTO ${user_id}_airlines.airlines_dim_updates VALUES(\"UA\",\"Adrenaline Airlines\");\nINSERT INTO ${user_id}_airlines.airlines_dim_updates VALUES(\"XX\",\"Get Out of My Airway!\");\n</code></pre> <ul> <li> <p>At this point the 2 tables contain data for the specific airlines     with code <code>UA, XX &amp; PAW</code> as follows.     </p> </li> <li> <p>Let's update an existing record to change the description of     <code>United Airlines</code> to <code>Adrenaline Airlines</code> to see more of the <code>ACID</code>     capabilities provided by Hive ACID. Run the following SQL.</p> </li> </ul> <pre><code>-- Merge inserted records into Airlines_orc table\nMERGE INTO ${user_id}_airlines.airlines_orc USING (SELECT * FROM ${user_id}_airlines.airlines_dim_updates) AS s\n  ON s.code = airlines_orc.code\n  WHEN MATCHED THEN UPDATE SET description = s.description\n  WHEN NOT MATCHED THEN INSERT VALUES (s.code,s.description);\n\nSELECT * FROM ${user_id}_airlines.airlines_orc WHERE code IN (\"UA\",\"XX\",\"PAW\");\n</code></pre> <p>The final <code>SELECT</code> statement should return the following result - codes <code>XX</code> and <code>PAW</code> were inserted rows, and code <code>UA</code> which had its description value changed from <code>United Air Lines Inc.</code> to <code>Adrenaline Airlines</code>. </p>"},{"location":"guide/04-04/index.html#42-migrate-hive-to-iceberg-table","title":"4.2 Migrate Hive to Iceberg Table","text":"<p>If you already have created a Data Warehouse using the Hive Table Format but would like to take advantage of the features offered in the Iceberg Table Format, you have 2 options. We will see both the options as a part of this step.</p> <p>Note that the <code>planes</code> table that we created earlier has <code>SerDe Library: org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe</code>. Note the \\'ParquetHiveSerDe\\' part. You can check the same by running the command below.</p> <pre><code>DESCRIBE FORMATTED ${user_id}_airlines.planes;\n</code></pre> <p></p>"},{"location":"guide/04-04/index.html#421-option-1-utilize-the-table-migration-feature","title":"4.2.1 (Option 1): Utilize the table Migration feature","text":"<ul> <li>Run the following SQL and note what happens next.</li> </ul> <pre><code>ALTER TABLE ${user_id}_airlines.planes\nSET TBLPROPERTIES ('storage_handler'='org.apache.iceberg.mr.hive.HiveIcebergStorageHandler');\n\nDESCRIBE FORMATTED ${user_id}_airlines.planes;\n</code></pre> <p>The following happened.</p> <p>(a). This migration to Iceberg happened in-place &amp; there was no re-writing of data that occurred as part of this process. It retained the File Format of <code>parquet</code> for the Iceberg table as well. There was a Metadata file that was created, which you can see when you run the <code>DESCRIBE FORMATTED</code>.</p> <p>(b). In the output look for the following fields - look for the following (see image with highlighted fields) key values: <code>Table Type</code>, <code>Location</code> (location of where table data is stored), <code>SerDe Library</code>, and in Table Parameters look for properties <code>MIGRATED_TO_ICEBERG</code>, <code>storage_handler</code>, <code>metadata_location</code>, and <code>table_type</code>.</p> <p><code>Location</code> - Data is stored in cloud storage and in this case AWS S3 in the same location as the Hive Table Format.</p> <p><code>Table Type</code>: Indicates that it is an <code>EXTERNAL TABLE</code>.</p> <p><code>MIGRATED_TO_ICEBERG</code>: Indicates that the table has migrated to <code>ICEBERG</code>.</p> <p><code>table_type</code>: Indicates <code>ICEBERG</code> table format.</p> <p><code>metadata_location</code>: Indicates the location of <code>metadata</code> which is the path to cloud storage.</p> <p><code>storage_handler</code>: <code>org.apache.iceberg.mr.hive.HiveIcebergStorageHandler</code>.</p> <p><code>SerDe Library</code>: <code>org.apache.iceberg.mr.hive.HiveIcebergSerDe</code>.</p> <p></p>"},{"location":"guide/04-04/index.html#422-option-2-use-create-table-as-select-ctas","title":"4.2.2 (Option 2): Use Create Table as Select (CTAS)","text":"<ul> <li>Run the following SQL to create <code>airports</code> table using CTAS. Notice     the syntax to create an Iceberg Table within Hive is     <code>Stored by Iceberg</code>.</li> </ul> <pre><code>drop table if exists ${user_id}_airlines.airports;\nCREATE EXTERNAL TABLE ${user_id}_airlines.airports\nSTORED BY ICEBERG AS\n  SELECT * FROM ${user_id}_airlines_raw.airports_csv;\n\nDESCRIBE FORMATTED ${user_id}_airlines.airports;\n</code></pre> <p>Look for: <code>Table Type</code>, <code>Location</code> (location of where table data is stored), <code>SerDe Library</code>, and in Table Parameters look for properties <code>storage_handler</code>, <code>metadata_location</code>, and <code>table_type</code>.</p> <p></p> <p></p>"},{"location":"guide/04-04/index.html#43-create-iceberg-table-partitioned-parquet-file-format","title":"4.3 Create Iceberg Table (Partitioned, Parquet File Format)","text":"<ul> <li>In this step we will create a partitioned table, in <code>Iceberg</code> Table Format, stored in <code>Parquet</code> File Format. Other than     that we could specify other file formats that are supported for     Iceberg which are: <code>ORC and Avro</code>.</li> </ul> <pre><code>drop table if exists ${user_id}_airlines.flights;\nCREATE EXTERNAL TABLE ${user_id}_airlines.flights (\n month int, dayofmonth int,\n dayofweek int, deptime int, crsdeptime int, arrtime int,\n crsarrtime int, uniquecarrier string, flightnum int, tailnum string,\n actualelapsedtime int, crselapsedtime int, airtime int, arrdelay int,\n depdelay int, origin string, dest string, distance int, taxiin int,\n taxiout int, cancelled int, cancellationcode string, diverted string,\n carrierdelay int, weatherdelay int, nasdelay int, securitydelay int,\n lateaircraftdelay int\n)\nPARTITIONED BY (year int)\nSTORED BY ICEBERG\nSTORED AS PARQUET\ntblproperties ('format-version'='2');\n\nSHOW CREATE TABLE ${user_id}_airlines.flights;\n</code></pre> <p>The <code>SHOW CREATE TABLE</code> command is the unformatted version of <code>DESCRIBE FORMATTED</code> command. Pay attention to the <code>PARTITIONED BY SPEC</code>, where we have partitioned the <code>flights</code> table using the <code>year</code> column.</p> <p></p> <p></p> <ul> <li>We will insert data into this table and it will write data together     within the same partition (i.e. all 2006 data is written to the same     location, all 2005 data is written to the same location, etc.).     <code>This command will take some time to run</code>.</li> </ul> <pre><code>INSERT INTO ${user_id}_airlines.flights\nSELECT * FROM ${user_id}_airlines_raw.flights_csv\nWHERE year &lt;= 2006;\n</code></pre> <p></p> <ul> <li>Run the following SQL and notice that each of the years have a range     of data within a few million flights (each record in the flights     table counts as a flight).</li> </ul> <pre><code>SELECT year, count(*)\nFROM ${user_id}_airlines.flights\nGROUP BY year\nORDER BY year desc;\n</code></pre> <p></p> <ul> <li>Now, make sure that the following 5 tables are created up until     this point as shown in the screenshot below.</li> </ul> <p></p>"},{"location":"guide/04-04/index.html#recap","title":"Recap","text":"<p>Below is the summary of what we have done so far in the form of a screenshot.</p> <p></p> <p>(1). Created a Raw Layer by defining Tables that point to CSV data files in an S3 bucket. We were then immediately able to query and run analytics against that data.</p> <p>(2). Created a Curated Layer to be the basis of our Data Lakehouse.</p> <ul> <li> <p>(2.1). Created the <code>planes</code> table in Hive table format stored in     <code>Parquet</code> to improve performance of querying this from the Raw CSV     data due to how the data is stored. Migrated, <code>in-place</code> - no data     rewrite, the planes table from Hive table format to Iceberg table     format using the Migration utility (Alter Table statement).</p> </li> <li> <p>(2.2). Created the <code>airlines_orc</code> table in Hive table format     stored in <code>ORC</code> to improve performance of querying this from the Raw     CSV data due to how the data is stored. Took advantage of the Hive     <code>ACID</code> capabilities to Insert, Update, Delete, and Merge data into     this table. Here we created a staging table to write new incoming     data to be used to update the <code>airlines_orc</code> table with (Merge     command).\\</p> </li> <li> <p>(2.3). Created the <code>airports</code> table in Iceberg Table Format     using a <code>CTAS</code> statement querying the Raw CSV data to take advantage     of the features of Iceberg.</p> </li> <li> <p>(2.4). Created the flights table in Iceberg Table Format and     partitioned the table by the year column. Inserted data into the     table up to year 2006.</p> </li> </ul> <p>As a final step here, let's run the same analytic query we ran against the Raw layer now in our Data Lakehouse DW, to see what happens with performance. From the cloudera console click on - <code>impala-vw</code>.</p> <p></p> <p></p> <ul> <li>Now run the following query again.</li> </ul> <pre><code>SELECT model,\n       engine_type\nFROM ${user_id}_airlines.planes\nWHERE planes.tailnum IN\n    (SELECT tailnum\n     FROM\n       (SELECT tailnum,\n               count(*),\n               avg(depdelay) AS avg_delay,\n               max(depdelay),\n               avg(taxiout),\n               avg(cancelled),\n               avg(weatherdelay),\n               max(weatherdelay),\n               avg(nasdelay),\n               max(nasdelay),\n               avg(securitydelay),\n               max(securitydelay),\n               avg(lateaircraftdelay),\n               max(lateaircraftdelay),\n               avg(airtime),\n               avg(actualelapsedtime),\n               avg(distance)\n        FROM ${user_id}_airlines.flights\n        WHERE tailnum IN ('N194JB',\n                          'N906S',\n                          'N575ML',\n                          'N852NW',\n                          'N000AA')\n        GROUP BY tailnum) AS delays);\n</code></pre> <p> The Data Lakehouse DW query performs significantly better than the same query running against the CSV data.</p> <p><code>Note: Please note that depending upon how the warehouse is comfigured (Auto suspend being set or unset), the query may take more time as the pods take time to start up</code></p>"},{"location":"guide/04-05/index.html","title":"Step 5: Performance Optimizations &amp; Table maintenance Using Impala VW","text":"<p>In this Step we will look at some of the performance optimization and table maintenance tasks that can be performed to ensure the best possible TCO, while ensuring the best performance.</p>"},{"location":"guide/04-05/index.html#51-iceberg-in-place-partition-evolution-performance-optimization","title":"5.1 Iceberg in-place Partition Evolution [Performance Optimization]","text":"<ul> <li> <p>Open HUE for the CDW <code>Hive</code> Virtual Warehouse - <code>vw-hive</code> </p> </li> <li> <p>One of the key features for Iceberg tables is the ability to evolve     the partition that is being used over time.</p> </li> </ul> <pre><code>ALTER TABLE ${user_id}_airlines.flights\nSET PARTITION spec ( year, month );\n\nSHOW CREATE TABLE ${user_id}_airlines.flights;\n</code></pre> <p></p> <ul> <li> <p>Check for the following where now the partition is by     <code>year, month</code>.     </p> </li> <li> <p>Load new data into the flights table using the NEW partition     definition. <code>This query will take a while to run</code>.</p> </li> </ul> <pre><code>INSERT INTO ${user_id}_airlines.flights\nSELECT * FROM ${user_id}_airlines_raw.flights_csv\nWHERE year = 2007;\n</code></pre> <p></p> <ul> <li> <p>Open HUE for the CDW <code>Impala</code> Virtual Warehouse - <code>impala-vw</code>.     </p> <p></p> </li> <li> <p>Copy/paste the following in the HUE Editor, but <code>DO NOT</code> execute     the query.</p> </li> </ul> <pre><code>SELECT year, month, count(*)\nFROM ${user_id}_airlines.flights\nWHERE year = 2006 AND month = 12\nGROUP BY year, month\nORDER BY year desc, month asc;\n</code></pre> <ul> <li> <p>Run <code>Explain Plans</code> against some typical analytic queries we might     run to see what happens with this new Partition.     </p> <p></p> </li> <li> <p>Copy/paste the following in the HUE Editor, but <code>DO NOT</code> execute     the query.</p> </li> </ul> <pre><code>SELECT year, month, count(*)\nFROM ${user_id}_airlines.flights\nWHERE year = 2007 AND month = 12\nGROUP BY year, month\nORDER BY year desc, month asc;\n</code></pre> <ul> <li>Run <code>Explain Plans</code> against some typical analytic queries we might     run to see what happens with this new Partition.     </li> </ul> <p>In the output notice the amount of data that needs to be scanned for this query, about 11 MB, is significantly less than that of the first, 138 MB. This shows an important capability of Iceberg, Partition Pruning. Meaning that much less data is scanned for this query and only the selected month of data needs to be processed. This should result in much faster query execution times. </p>"},{"location":"guide/04-05/index.html#52-iceberg-snapshots-table-maintenance","title":"5.2 Iceberg Snapshots [Table Maintenance]","text":"<ul> <li>In the previous steps we have loaded data into the <code>flights</code> iceberg     table. We will insert more data into it. Each time we add (update or     delete) data a <code>snapshot</code> is captured. The snapshot is important for     <code>eventual consistency</code> &amp; to allow multiple read/writes concurrently     (from various engines or the same engine).</li> </ul> <pre><code>INSERT INTO ${user_id}_airlines.flights\nSELECT * FROM ${user_id}_airlines_raw.flights_csv\nWHERE year &gt;= 2008;\n</code></pre> <ul> <li>To see snapshots, execute the following SQL.</li> </ul> <pre><code>DESCRIBE HISTORY ${user_id}_airlines.flights;\n</code></pre> <p>In the output there should be 3 Snapshots, described below. Note that we have been reading/writing data from/to the Iceberg table from both Hive &amp; Impala. It is an important aspect of Iceberg Tables that they support <code>multi-function analytics</code> - ie. many engines can work with Iceberg tables (<code>Cloudera Data Warehouse [Hive &amp; Impala]</code>, <code>Cloudera Data Engineering [Spark]</code>, <code>Cloudera Machine Learning [Spark]</code>, <code>Cloudera DataFlow [NiFi]</code>, and <code>DataHub Clusters</code>).</p> <ul> <li> <p>Get the details of the <code>snapshots</code> and store it in a notepad.     </p> <p></p> </li> </ul>"},{"location":"guide/04-05/index.html#53-iceberg-time-travel-table-maintenance","title":"5.3 Iceberg Time Travel [Table Maintenance]","text":"<ul> <li>Copy/paste the following data into the Impala Editor, but     <code>DO NOT</code> execute.</li> </ul> <pre><code>-- SELECT DATA USING TIMESTAMP FOR SNAPSHOT\nSELECT year, count(*)\nFROM ${user_id}_airlines.flights\n  FOR SYSTEM_TIME AS OF '${create_ts}'\nGROUP BY year\nORDER BY year desc;\n\n-- SELECT DATA USING SNAPSHOT ID FOR SNAPSHOT\nSELECT year, count(*)\nFROM ${user_id}_airlines.flights\n  FOR SYSTEM_VERSION AS OF ${snapshot_id}\nGROUP BY year\nORDER BY year desc;\n</code></pre> <ul> <li> <p>After copying you will see 2 parameters as below.     </p> </li> <li> <p>From the notepad just copy the first value of the timestamp. It     could be the date or the timestamp. Paste it in the <code>create_ts</code> box.     In our case the value was <code>2025-02-07 11:06:25.905000000</code>. Then     execute the highlighted query only (1st query).      </p> </li> <li> <p>From the notepad just copy the second value of the snapshot id. In     our case the value was <code>1796099219985023033</code>. Paste it in the     <code>snapshot_id</code> box. Then execute the highlighted query only (2nd     query).      </p> </li> </ul>"},{"location":"guide/04-05/index.html#54-dont-run-fyi-only-iceberg-rollback-table-maintenance","title":"5.4 (Don't Run, FYI ONLY) - Iceberg Rollback [Table Maintenance]","text":"<ul> <li>Sometimes data can be loaded incorrectly, due to many common     issues - missing fields, only part of the data was loaded, bad data,     etc. In situations like this data would need to be removed,     corrected, and reloaded. Iceberg can help with the Rollback command     to remove the \"unwanted\" data. This leverages Snapshot IDs to     perform this action by using a simple ALTER TABLE command as     follows. We will <code>NOT RUN</code> this command in this lab.</li> </ul> <pre><code>-- ALTER TABLE ${user_id}_airlines.flights EXECUTE ROLLBACK(${snapshot_id});\n</code></pre>"},{"location":"guide/04-05/index.html#55-dont-run-fyi-only-iceberg-rollback-table-maintenance","title":"5.5 (Don't Run, FYI ONLY) - Iceberg Rollback [Table Maintenance]","text":"<ul> <li>As time passes it might make sense to expire old Snapshots, instead     of the Snapshot ID you use the Timestamp to expire old Snapshots.     You can do this manually by running a simple ALTER TABLE command as     follows. We will <code>NOT RUN</code> this command in this lab.</li> </ul> <pre><code>-- Expire Snapshots up to the specified timestamp\n-- BE CAREFUL: Once you run this you will not be able to Time Travel for any Snapshots that you Expire ALTER TABLE ${user_id}_airlines.flights\n-- ALTER TABLE ${user_id}_airlines_maint.flights EXECUTE expire_snapshots('${create_ts}');\n</code></pre>"},{"location":"guide/04-05/index.html#56-materialized-views-performance-optimization","title":"5.6 Materialized Views [Performance Optimization]","text":"<ul> <li>This can be used for both Iceberg tables and Hive Tables to improve     performance. Go to the Cloudera console and look for <code>hive-vw</code>.     Click on the <code>Hue</code> button on the right upper corner of <code>hive-vw</code> as     shown in the screenshot below.</li> </ul> <ul> <li>Up until this point we had <code>airlines</code> table which was (Hive + orc).     Now, we shall create the airlines table which is (Iceberg + orc).     Copy/paste the following, make sure to highlight the entire block,     and execute the following.</li> </ul> <pre><code>SET hive.query.results.cache.enabled=false;\n\ndrop table  if exists ${user_id}_airlines.airlines;\nCREATE EXTERNAL TABLE ${user_id}_airlines.airlines (code string, description string) STORED BY ICEBERG STORED AS ORC TBLPROPERTIES ('format-version' = '2');\n\nINSERT INTO ${user_id}_airlines.airlines SELECT * FROM ${user_id}_airlines_raw.airlines_csv;\n\nSELECT airlines.code AS code,  MIN(airlines.description) AS description,\n          flights.month AS month,\n          sum(flights.cancelled) AS cancelled\nFROM ${user_id}_airlines.flights flights , ${user_id}_airlines.airlines airlines\nWHERE flights.uniquecarrier = airlines.code\ngroup by airlines.code, flights.month;\n</code></pre> <p>Note: Hive has built in performance improvements, such as a Query Cache that stores results of queries run so that similar queries don't have to retrieve data, they can just get the results from the cache. In this step we are turning that off using the SET statement, this will ensure when we look at the query plan, we will not retrieve the data from the cache. Note: With this query you are combining an Iceberg Table Format (<code>flight</code> table) with a Hive Table Format (<code>airlines ORC</code> table) in the same query.</p> <ul> <li>Let's look at the Query Plan that was used to execute this query. On     the left side click on <code>Jobs</code>, as shown in the screenshot below.</li> </ul> <p></p> <ul> <li>Then click on <code>Hive Queries</code>. This is where an Admin will go when he     wants to investigate the queries. In our case for this lab, we'd     like to look at the query we just executed to see how it ran and the     steps taken to execute the query. Administrators would also be able     to perform other monitoring and maintenance tasks for what is     running (or has been run). Monitoring and maintenance tasks could     include cancel (kill) queries, see what is running, analyze whether     queries that have been executed are optimized, etc.</li> </ul> <p></p> <ul> <li> <p>Click on the first query as shown below. Make sure that this is the     latest query. You can look at the <code>Start Time</code> field here to know if     it's the latest or not.     </p> </li> <li> <p>This is where you can analyze queries at a deep level. For this lab     let's take a look at the explain details, by clicking on     <code>Visual Explain</code> tab. It might take a while to appear, please click     on refresh.     </p> </li> <li> <p>This plan shows that this query needs to read <code>flights</code> (86M rows)     and <code>airlines</code> (1.5K rows) with hash join, group, and sort. This is     a lot of data processing and if we run this query constantly it     would be good to reduce the time this query takes to execute.     </p> </li> <li> <p>Click on the <code>Editor</code> option on the left side as shown.     </p> </li> <li> <p>Create Materialized View (MV) - Queries will transparently be     rewritten, when possible, to use the MV instead of the base tables.     Copy/paste the following, highlight the entire block, and execute.</p> </li> </ul> <pre><code>DROP MATERIALIZED VIEW IF EXISTS ${user_id}_airlines.traffic_cancel_airlines;\nCREATE MATERIALIZED VIEW ${user_id}_airlines.traffic_cancel_airlines\nas SELECT airlines.code AS code,  MIN(airlines.description) AS description,\n          flights.month AS month,\n          sum(flights.cancelled) AS cancelled,\n          count(flights.diverted) AS diverted\nFROM ${user_id}_airlines.flights flights JOIN ${user_id}_airlines.airlines airlines ON (flights.uniquecarrier = airlines.code)\ngroup by airlines.code, flights.month;\n\n-- show MV\nSHOW MATERIALIZED VIEWS in ${user_id}_airlines;\n</code></pre> <p></p> <ul> <li>Run Dashboard Query again to see usage of the MV - Copy/paste the     following, make sure to highlight the entire block, and execute the     following. This time an <code>order by</code> was added to make this query must     do more work.</li> </ul> <pre><code>SET hive.query.results.cache.enabled=false;\nSELECT airlines.code AS code,  MIN(airlines.description) AS description,\n          flights.month AS month,\n          sum(flights.cancelled) AS cancelled\nFROM ${user_id}_airlines.flights flights , ${user_id}_airlines.airlines airlines\nWHERE flights.uniquecarrier = airlines.code\ngroup by airlines.code, flights.month\norder by airlines.code;\n</code></pre> <p></p> <ul> <li>Let's look at the Query Plan that was used to execute this query. On     the left menu select <code>Jobs</code>. On the Jobs Browser - select the     <code>Queries</code> tab to the right of the <code>Job</code> browser header. Hover over &amp;     click on the Query just executed (should be the first row). Click on     the <code>Visual Explain</code> tab. With query rewrite the materialized view     is used and the new plan just reads the MV and sorts the data vs.     reading <code>flights (86M rows)</code> and <code>airlines (1.5K rows)</code> with hash     join, group and sorts. This results in significant reduction in run     time for this query.</li> </ul> <p></p>"},{"location":"guide/04-06/index.html","title":"Step 6: Cloudera Data Visualization","text":"<p>In this step you will build a Logistics Dashboard using Cloudera Data Visualization. The Dashboard will include details about flight delays and cancellations. But first we will start with Data Modeling.</p>"},{"location":"guide/04-06/index.html#step-6a-data-modeling","title":"Step 6(a): Data Modeling","text":"<ul> <li> <p>If you are not on the CDP home page, then go there and click on the     following CDW icon to go into Cloudera Data Warehouse.     </p> </li> <li> <p>Then click on the Data Visualization option in the left window pane. You\u2019ll see an option Data VIZ next to the data-viz application with the name dataviz. It should open a new window.+</p> </li> </ul> <p></p> <ul> <li> <p>There are 4 areas of CDV - <code>HOME, SQL, VISUALS, DATA</code> - these are     the tabs at the top of the screen in the black bar to the right of     the Cloudera Data Visualization banner. </p> </li> <li> <p>Build a Dataset (aka. Metadata Layer or Data Model) - click on     <code>DATA</code> in the top banner. A Dataset is a Semantic Layer where you     can create a business view on top of the data - data is not copied;     this is just a logical layer.     </p> </li> <li> <p>Create a connection - click on the NEW CONNECTION button on the left     menu. Enter the details as shown in the screenshot and click on     <code>TEST</code>.</p> <p>Connection type: Select <code>CDW Impala</code>.</p> <p>Connection name: <code>&lt;user_id&gt;-airlines-lakehouse</code> (Ex-<code>wuser00-airlines-lakehouse</code>).</p> <p>CDW Warehouse: <code>Make Sure you select the warehouse that is associated with your &lt;user_id&gt;</code>.</p> <p>Hostname or IP address: Gets automatically selected.</p> <p>Port: <code>Gets automatically filled up</code>.</p> <p>Username: <code>Gets automatically filled up</code>.</p> <p>Password: <code>Blank</code></p> </li> </ul> <p> </p> <ul> <li> <p>Click on <code>CONNECT</code>.     </p> </li> <li> <p>You will see your connection in the list of connections on the left     menu. On the right side of the screen you will see Datasets and the     Connection Explorer. Click on <code>NEW DATASET</code>.      </p> </li> <li> <p>Fill the details as follows and click <code>CREATE</code>. <code>airline_logistics</code>     gets created</p> <p>Dataset title - <code>airline_logistics</code>.</p> <p>Dataset Source - Select <code>From Table</code> (however, you could choose to directly enter a SQL statement instead).</p> <p>Select Database - <code>&lt;user_id&gt;_airlines</code> (Make Sure you select the database that is associated with your \\&lt;user_id&gt;).</p> <p>Select Table - <code>flights</code>.</p> </li> </ul> <p></p> <ul> <li> <p>Edit the Dataset - click on <code>airline_logistics</code> on the right of the     screen. This will open the details page, showing you information     about the Dataset, such as connection details, and options that are     set. Click on <code>Fields</code> option in the left window pane.      </p> </li> <li> <p>Click on <code>Data Model</code> - for our Dataset we need to join additional     data to the flights table including the <code>planes</code>, <code>airlines</code>, and     <code>airports</code> tables.     </p> </li> <li> <p>Click on <code>EDIT DATA MODEL</code>.     </p> </li> <li> <p>Click on the <code>+</code> icon next to the <code>flights</code> table option.     </p> </li> <li> <p>Select the appropriate <code>Database Name</code> based on your user id (Ex:     <code>wuser00_airlines</code>) and table name <code>planes</code>.     </p> </li> <li> <p>Then click on the  <code>join</code> icon     and see that there are 2 join options <code>tailnum</code> &amp; <code>year</code>. </p> <p>Click on <code>EDIT JOIN</code> and then remove the <code>year</code> join by clicking the little <code>-</code> (minus) icon to the right next to the <code>year</code> column. </p> <p>Click on <code>APPLY</code>.</p> <p> </p> </li> <li> <p>Now we will create a join between another table. </p> <p>Click on <code>+</code> icon next to <code>flights</code> as shown below.</p> <p>Select the appropriate <code>Database Name</code> based on your &lt;user_id&gt; (Ex: <code>wuser00_airlines</code>) and table name <code>airlines</code>.  </p> </li> <li> <p>Make sure you select the column <code>uniquecarrier</code> from <code>flights</code> and     column <code>code</code> from the <code>airlines</code> table. Click <code>APPLY</code>.      </p> </li> <li> <p>Click on <code>+</code> icon next to <code>flights</code> as shown below. Select the     appropriate <code>Database Name</code> based on your &lt;user_id&gt; (Ex:     <code>wuser00_airlines</code>) and table name <code>airports</code>.     </p> </li> <li> <p>Make sure you select the column <code>origin</code> from <code>flights</code> and column     <code>iata</code> from the <code>airports</code> table. Click <code>APPLY</code>.     </p> </li> <li> <p>Click on <code>+</code> icon next to <code>flights</code> as shown below. Select the     appropriate <code>Database Name</code> based on your &lt;user_id&gt; (Ex:     <code>wuser00_airlines</code>) and table name <code>airports</code>.     </p> </li> <li> <p>Make sure you select the column <code>dest</code> from <code>flights</code> and column     <code>iata</code> from the <code>airports</code> table. Click <code>APPLY</code>. Then click on     <code>SAVE</code>.      </p> </li> <li> <p>Verify that you have the joins which are as following. </p> <p>You can do so by clicking the  <code>join</code> icon.</p> <p><code>flights.tailnum</code>\u2009---\u2009<code>planes.tailnum</code></p> <p><code>flights.uniquecarrier</code>\u2009---\u2009<code>airlines.code</code></p> <p><code>flights.origin</code>\u2009---\u2009<code>airports.iata</code></p> <p><code>flights.dest</code>\u2009---\u2009<code>airports_1.iata</code></p> </li> <li> <p>Click on <code>SHOW DATA</code>. And then click on <code>SAVE</code>.  </p> </li> <li> <p>Click on the <code>Fields</code> column on the left window pane. Then click on     <code>EDIT FIELDS</code>. Make sure that you click on the highlighted area to     change <code>#</code> (measures icon) next to each column to <code>Dim</code> (dimension     icon). The columns are as follows.</p> <p>a.  <code>flights</code> table: Columns (<code>month</code>, <code>dayofmonth</code>, <code>dayofweek</code>,     <code>deptime</code>, <code>crsdeptime</code>, <code>arrtime</code>, <code>crsarrtime</code>, <code>flightnum</code> &amp;     <code>year</code>)</p> <p>b.  <code>planes</code> table: <code>All columns</code></p> <p>c.  <code>airports</code> table: <code>All columns</code></p> <p>d.  <code>airports_1</code> table: <code>All columns</code></p> </li> </ul> <p> </p> <ul> <li> <p>Click on <code>TITLE CASE</code>. And notice that the column names changes to     be <code>Camel case</code>. Click on the <code>pencil</code> icon next to the <code>Depdelay</code>     icon.      </p> </li> <li> <p>Change the <code>Default Aggregation</code> to <code>Average</code>. Click on the     <code>Display Format</code> and then change <code>Category</code> to be <code>Integer</code>. Check     mark the box next to the <code>Use 1000 separator</code>. Click on <code>APPLY</code>.      </p> </li> <li> <p>Click on the <code>down arrow</code> shown against the <code>Origin</code> column and then     click on <code>Clone</code>. A column <code>Copy of Origin</code> is created. Click on the     <code>pencil</code> icon next to it.      </p> </li> <li> <p>Change the <code>Display Name</code> to <code>Route</code>. Then click on <code>Expression</code> and     enter the following in the <code>Expression</code> editor. Click on <code>APPLY</code>.</p> </li> </ul> <pre><code>concat([Origin], '-', [Dest])\n</code></pre> <p> </p> <ul> <li>Click on <code>SAVE</code>. We have completed the step of data modeling and now     we will create data visualization.     </li> </ul>"},{"location":"guide/04-06/index.html#step-6b-creating-dashboard","title":"Step 6(b): Creating Dashboard","text":"<ul> <li>Now we will create a dashboard page based on the data model that we     just created. Click on <code>NEW DASHBOARD</code>.</li> </ul> <ul> <li> <p>You will see the following. </p> </li> <li> <p>A quick overview of the screen that you are seeing is as follows.</p> <p>On the right side of the screen there will be a VISUALS menu.</p> <p>At the top of this menu, there is a series of Visual Types to choose from.</p> <p>There will be 30+ various visuals to choose from.</p> <p>Below the Visual Types you will see what are called Shelves. The Shelves that are present depend on the Visual Type that is selected. Shelves with a <code>*</code> are required, all other Shelves are optional. </p> <p>On the far right of the page there is a DATA menu, which identifies the Connection &amp; Dataset used for this visual.  Underneath that is the Fields from theDataset broken down by Dimensions and Measures. With each of these Categories you can see that it is subdivided by each Table in the Dataset.  </p> </li> <li> <p>Let's build the 1st visual - <code>Top 25 Routes by Avg Departure Delay</code>.     CDV will add a Table visual displaying a sample of the data from the     Dataset as the default visualization when you create a new Dashboard     or new Visuals on the Dashboard (see New Dashboard screen above).     The next step is to modify (Edit) the default visualization to suit     your needs.</p> </li> <li> <p>Pick the Visual Type - Select the <code>Stacked Bar</code> chart visual on the     top right as shown below. Make sure <code>Build</code> is selected for it to     appear on the right side.     </p> </li> <li> <p>Find <code>Route</code> under <code>Dimensions \u2192 flights</code>. Drag to <code>X-Axis</code>.     Similarly, find <code>DepDelay</code> under <code>Measures \u2192 flights</code>. Drag to     <code>Y-Axis</code>. By default the aggregation selected is average and hence     you would see <code>avg(Depdelay)</code>.     </p> </li> <li> <p>Click on the arrow next to <code>avg(Depdelay)</code>. Enter <code>50</code> against the     text box labeled <code>Top K</code>. Click on <code>REFRESH VISUAL</code>.      </p> </li> <li> <p>Click <code>enter title</code> and enter the title based on your user id as -     <code>&lt;user_id&gt;- Routes with Highest Avg. Departure Delays</code>. Then click     on <code>SAVE</code>. </p> </li> </ul>"},{"location":"guide/04-06/index.html#_1","title":"04-06 Data Visualization","text":""},{"location":"guide/05-01/index.html","title":"05-01_Using_ClouderaAI","text":""},{"location":"guide/05-02/index.html","title":"MlOps in Cloudera AI - Banking Usecase","text":""},{"location":"guide/05-02/index.html#1-mlops-machine-learning-operations-for-a-banking-use-case","title":"1. MLOps (Machine Learning Operations) for a banking use case.","text":"<p>The goal is to demonstrate how to implement MLOps practices using MLFlow and Cloudera Machine Learning (CML). The exercise provides step-by-step instructions to build, deploy, and manage a machine learning model in a banking context, emphasizing reproducibility, scalability, and operational efficiency.</p>"},{"location":"guide/05-02/index.html#2-key-objectives","title":"2. Key Objectives","text":"<p>Key Goals of the Exercise:</p> <p>Data Generation and Exploration:</p> <ul> <li>Generate and explore data at scale using PySpark.</li> </ul> <p>Model Development:</p> <ul> <li>Develop an XGBoost classifier to predict fraudulent credit card transactions.</li> </ul> <p>Experiment Tracking:</p> <ul> <li>Utilize MLflow Tracking to monitor experiments from interactive notebook sessions.</li> </ul> <p>Model Registry:</p> <ul> <li>Employ MLflow Registry to package model experiments in a uniform format, facilitating collaboration, deployments, and reproducibility.</li> </ul> <p>Model Deployment:</p> <ul> <li>Deploy models as REST endpoints in a high-availability manner, with automated lineage building and metric tracking for MLOps purposes.</li> </ul> <p>Pipeline Orchestration:</p> <ul> <li>Use CML Jobs to orchestrate an end-to-end automated pipeline, including monitoring for model drift and automatically initiating model retraining and redeployment as needed.</li> </ul>"},{"location":"guide/05-02/index.html#3-what-you-will-learn","title":"3. What You Will Learn","text":"<p>By completing this exercise, you will gain hands-on experience with:</p> <ul> <li>Building and deploying machine learning models in a real-world banking context.</li> <li>Using MLFlow for experiment tracking, model versioning, and deployment.</li> <li>Leveraging CML for end-to-end MLOps workflows.</li> <li>Implementing best practices for reproducibility, collaboration, and scalability in machine learning projects.</li> </ul>"},{"location":"guide/05-02/index.html#4-on-cloudera-console-select-cloudera-ai","title":"4. On Cloudera Console select Cloudera AI","text":""},{"location":"guide/05-02/index.html#5-click-on-workbench-name","title":"5. Click on workbench name.","text":""},{"location":"guide/05-02/index.html#6-click-on-new-project","title":"6. Click on New Project","text":""},{"location":"guide/05-02/index.html#7-enter-the-following-parameters-in-the-form","title":"7. Enter the following parameters in the form:","text":"<pre><code>Project Name: MLOps HOL &lt;username&gt;\nProject Visibility: Private or Public\nInitial Setup: Git -&gt; https://github.com/cloudera-edu/CML_MLops_Banking_MLFlow.git\nRuntimes:\n  1. Remove all default runtimes.\n  2. Select Advanced Options\n  3. Select: Workbench Editor / Python 3.9 Kernel / Standard Edition / 2024.02 Version\n</code></pre>"},{"location":"guide/05-02/index.html#8-runtimes","title":"8. Runtimes:","text":"<pre><code>  1. Remove all default runtimes.\n  2. Select Advanced Options\n  3. Select: Workbench Editor / Python 3.9 Kernel / Standard Edition / 2024.02 Version\n</code></pre> <p>Click create project </p>"},{"location":"guide/05-02/index.html#9-go-to-project-settings-data-conections-copy-the-connection-name-to-your-notepad","title":"9. Go to Project settings -&gt; Data Conections -&gt; Copy the Connection Name to your notepad","text":"<p>You need this connection name to modify the various scripts in the upcoming step. </p>"},{"location":"guide/05-02/index.html#10-create-a-cml-session-and-install-requirements","title":"10. Create a CML Session and Install Requirements","text":"<p>Go to sessions tab and click on new session. </p>"},{"location":"guide/05-02/index.html#11-launch-a-cml-session-with","title":"11. Launch a CML Session with:","text":"<pre><code>Session Name: Mlops session-userx\nEditor: Workbench\nKernel: Python 3.9\nEdition: Standard\nVersion: 2024.02\nEnable Spark: Version 3.2 or 3.3\nResource Profile: 2 vCPU / 4 GiB Mem / 0 GPU\n</code></pre>"},{"location":"guide/05-02/index.html#12-in-the-prompt-on-the-right-side-enter-the-following-command","title":"12. In the prompt on the right side enter the following command:","text":"<pre><code>!pip3 install -r requirements.txt\n</code></pre> <p>Hit Enter. </p>"},{"location":"guide/05-02/index.html#13-once-all-packages-have-been-installed-proceed-to-next-step","title":"13. Once all packages have been installed, proceed to next step.","text":""},{"location":"guide/05-02/index.html#14-open-00_datagenpy-in-your-cml-session-and-update-the-connection-name-on-line-no-160","title":"14. Open 00_datagen.py in your CML Session and update the Connection Name on Line no 160.","text":"<p>Save the file and press the play button in order to run the whole script. Code output is available on the right side of your screen. </p>"},{"location":"guide/05-02/index.html#141-code-highlights","title":"14.1 Code Highlights","text":"<p>Line 50: the cml.data_v1 library is imported. This library allows you to take advantage of CML Data Connections in order to launch a Spark Session and connect to the Data Lake. The DataConnection is used at lines 103 - 109 within the \"createSparkConnection\" module.</p> <p>Lines 63 - 94: the \"dataGen\" module is used to create synthetic data for the classification use case. Observe the data attributes that are being created, and their respective types and value ranges.</p> <p>Lines 132 -145: the PySark API for Apache Iceberg is used to create or append data to an Iceberg table format table from a PySpark dataframe.</p>"},{"location":"guide/05-02/index.html#142-summary","title":"14.2 Summary","text":"<p>In this lab you used CML Data Connections to preconfigure boiler plate code for data access to CDP and 3rd party data sources including Postgres, SQLServer, and even Snowflake. You can customize Data Connections to standardize and simplify Data Access configurations, as well as restrict access from 3rd party systems.</p> <p>The PySpark Iceberg API allows you to create Iceberg tables from Spark DataFrames. It is similar and as simple as the standard PySpark API for interacting with Hive tables.</p>"},{"location":"guide/05-02/index.html#15-open-01_train_xgboostpy-in-your-cml-session-and-update-the-connection_name-variables-at-lines-55-save-it","title":"15. Open 01_train_xgboost.py in your CML Session and update the CONNECTION_NAME variables at lines 55 &amp; Save it.","text":"<p>Next, press the play button in order to run the whole script. You will be able to observe code output on the right side of your screen. </p>"},{"location":"guide/05-02/index.html#151-click-on-project-tab","title":"15.1 Click on project tab.","text":""},{"location":"guide/05-02/index.html#152-click-on-experiment-tab","title":"15.2 Click on Experiment tab.","text":"<p>Validate experiment creation. Open the Experiment Run and familiarize yourself with Experiment Run metadata. At the bottom of the page, open the Artifacts folder and notice that model dependencies have been tracked. </p>"},{"location":"guide/05-02/index.html#153-code-highlights","title":"15.3 Code Highlights","text":"<ul> <li>Line 41: the MLFlow package is imported. MLFlow is installed in CML Projects by default. An internal Plugin tranaslates MLFlow methods to CML API methods. There is no need to install or configure MLFlow in order to use its Tracking capabilities.</li> <li>Lines 71 - 98: XGBoost training code is defined within the context of an MLFlow Experiment Run. XGBoost code is otherwise unchanged. The \"mlflow.log_param()\" method is used to log model metrics. The \"mlflow.log_model()\" method is used to track model artifacts in the \"artifacts\" folder.</li> <li>Line 120: the MLFlow Client is used to interact with Experiment Run metadata. You can use the Client to search through runs, compare results, and much more.</li> </ul>"},{"location":"guide/05-02/index.html#154-summary","title":"15.4 Summary","text":"<p>In this lab used MLFlow to track experiment runs in the Experiments UI, access experiment rund ata programmatically via the MLFlow Client, and register Runs to the MLFlow Registry. When an Exoeriment Run is tracked, MLFlow automatically stores model artifacts and dependencies in the backend.</p> <p>The Registry is a separate component from the Workspace and acts as a staging environment for optionally moving models and associated dependencies from one Workspace to another, for example in a DEV to QA to PRD pattern.</p> <p>MLFlow in CML does not require any installation or configurations on the part of the CML Admins or Users. It is preinstalled by default in every CML Workspace. CML includes a special Plugin that translates MLFlow API calls to CML API v2 routines. You will learn more about CML API v2 in the next section.</p>"},{"location":"guide/05-02/index.html#16-click-on-sessions-tab-and-select-your-running-session","title":"16. Click on Sessions tab and select your running session.","text":""},{"location":"guide/05-02/index.html#17-return-to-02_api_deploymentpy-and-press-the-play-button-in-order-to-run-the-whole-script-you-will-be-able-to-observe-code-output-on-the-right-side-of-your-screen","title":"17. Return to 02_api_deployment.py and press the play button in order to run the whole script. You will be able to observe code output on the right side of your screen.","text":"<p>Once successful run Click on project tab </p>"},{"location":"guide/05-02/index.html#171-go-to-model-deployment-tab","title":"17.1 Go to Model Deployment tab","text":"<p>You will see your model is building and deploying &amp; finally Deployed </p>"},{"location":"guide/05-02/index.html#172-code-highlights","title":"17.2 Code Highlights","text":"<ul> <li>Line 46: the \"ModelDeployment\" class is imported from the \"mlops\" util. This util has been placed in the \"/home/cdsw\" folder.</li> <li>Line 49: the CML API client is instantiated. The API provides you with over 100 Python methods to execute actions such as creating projects, launching jobs, and a lot more. In this example, the API is used to \"list_projects()\".</li> <li>Line 62: the API Client is passed as an argument to the ModelDeployment instance. The mlops.py util includes a few methods that extend and override API methods. Typically, CML Machine Learning Engineers create Python Interfaces to build custom MLOps pipelines as required by their use case.</li> <li>Line 56-60: the latest MLFlow Experiment Run is used to register the Model in the CML MLFlow Registry.</li> <li>Lines 68,74, 78, 81: the registered Model is used to create a new CML Model Deployment. The Model is first created, then built, and finally deployed.</li> </ul>"},{"location":"guide/05-02/index.html#173-summary","title":"17.3 Summary","text":"<p>In this lab you used CML APIv2 allows you to programmatically execute actions within CML Workspaces. You can use the API with plain curl CLI statements, or the Python Wrapper which is a lib that is preinstalled in every Cloudera CML Runtime. The API can be used both from 3rd party systems that are external to the CML Workspace, and within the CML Workspace.</p> <p>CML Data Scientists leverage the API to build MLOPs Pipelines. In this lab you used a simple Python Interface to override API methods in order to build a standardized MLOps pipeline to register an Experiment Run in the MLFlow Registry, and then deploy an API Endpoint for model serving.</p>"},{"location":"guide/05-02/index.html#18-click-on-sessions-tab-and-select-your-running-session","title":"18. Click on Sessions tab and select your running session.","text":""},{"location":"guide/05-02/index.html#19-mlops-pipeline","title":"19. MLOps Pipeline","text":"<p>This section explains the most important aspects of 03_newbatch.py, 04_train_xgboost.py, and 05_api_redeployment.py.</p> <p>In the above script only update the CONNECTION_NAME and don't run script.</p> <p>Follow below steps to update the CONNECTION_NAME</p>"},{"location":"guide/05-02/index.html#191-open-03_newbatchpy","title":"19.1 Open 03_newbatch.py","text":"<p>Go to line 159, update the connection name, and save this file. </p>"},{"location":"guide/05-02/index.html#192-open-04_train_xgboostpy-update-the-connection_name-variables-at-line-no-55-save-it","title":"19.2 Open 04_train_xgboost.py &amp; update the CONNECTION_NAME variables at line no 55 &amp; Save it.","text":""},{"location":"guide/05-02/index.html#193-open-05_api_redeploymentpy-update-the-connection_name-variables-at-line-no-210-save-it","title":"19.3 Open 05_api_redeployment.py &amp; update the CONNECTION_NAME variables at line no 210 &amp; Save it.","text":"<p>Click on project tab. </p>"},{"location":"guide/05-02/index.html#194-create-a-cml-job-for-each-do-not-run-the-jobs-yet","title":"19.4 Create a CML Job for each. Do not run the jobs yet.","text":"<p>Go to the Jobs tab and click on New Job.</p> <p>Please do not run the below job just create three jobs as per the below steps. </p>"},{"location":"guide/05-02/index.html#195-create-job-new-batch-with-the-following-configurations","title":"19.5 Create Job \"New Batch\" with the following configurations:","text":"<pre><code>Name: New Batch Userx\nScript: 03_newbatch.py\nEditor: Workbench\nKernel: Python 3.9\nSpark Add On: Spark 3.2 or 3.3\nEdition: Standard\nVersion: 2024.02\nSchedule: Manual\nResource Profile: 2 vCPU / 4 Gib / 0 GPU\n</code></pre> <p>Click Create </p>"},{"location":"guide/05-02/index.html#196-click-on-new-job","title":"19.6 Click on New Job","text":""},{"location":"guide/05-02/index.html#197-create-job-new-batch-with-the-following-configurations","title":"19.7 Create Job \"New Batch\" with the following configurations:","text":"<pre><code>Name: Retrain XGBoost Userx\nScript: 04_train_xgboost.py\nEditor: Workbench\nKernel: Python 3.9\nSpark Add On: Spark 3.2 or 3.3\nEdition: Standard\nVersion: 2024.02\nSchedule: Dependent on New Batch Userx\nResource Profile: 2 vCPU / 4 Gib / 0 GPU\n</code></pre> <p>Click Create </p>"},{"location":"guide/05-02/index.html#198-click-on-new-job","title":"19.8 Click on New Job","text":"<p>You should see the dependency </p>"},{"location":"guide/05-02/index.html#199-create-job-new-batch-with-the-following-configurations","title":"19.9 Create Job \"New Batch\" with the following configurations:","text":"<pre><code>Name: API Redeployment Userx\nScript: 05_api_redeployment.py\nEditor: Workbench\nKernel: Python 3.9\nSpark Add On: Spark 3.2 or 3.3\nEdition: Standard\nVersion: 2024.02\nSchedule: Dependent on Retrain XGBoost Userx\nResource Profile: 2 vCPU / 4 Gib / 0 GPU\n</code></pre> <p>Click Create. </p>"},{"location":"guide/05-02/index.html#1910-once-you-created-all-three-jobs-manually-trigger-the-new-batch-job-monitor-execution-in-the-job-history-tab-and-observe-that-once-it-is-complete-the-next-job-in-the-mlops-pipeline-retrain-xgboost-is-triggered-and-finally-the-last-job-api-redeployment-is-executed","title":"19.10 Once you created all three jobs, manually trigger the New Batch job. Monitor execution in the Job History tab, and observe that once it is complete the next job in the MLOps pipeline, Retrain XGBoost, is triggered, and finally the last job, API Redeployment, is executed.","text":""},{"location":"guide/05-02/index.html#1911-once-all-there-job-run-go-to-model-deployment-tab-you-will-see-your-model-is-building-deploying-and-finally-deployed","title":"19.11 Once all there job run, go to model deployment tab. you will see your model is building, deploying and finally deployed.","text":"<p>Once finally deployed, go to the next step. </p>"},{"location":"guide/05-02/index.html#1912-code-highlights","title":"19.12 Code Highlights","text":"<ul> <li>03_newbatch.py is mostly identical to 00_datagen.py.</li> <li>04_train_xgboost.py is nearly identical to \"01_train_xgboost.py\". However, at lines 66-68 Iceberg Snapshot metadata is stored as variables. This metadata is used at lines 70-74 in order to perform an Incremental Read i.e. only loading data from the Iceberg table within a start and end time boundary. The metadata is then saved as MLFlow Tags during Experiment Run execution.</li> <li>05_api_redeployment.py includes both methods from the mlops util and code to execute the MLOps pipeline. This is also nearly identical to the code in \"02_api_deployment.py\".</li> </ul>"},{"location":"guide/05-02/index.html#1913-summary","title":"19.13 Summary","text":"<p>In this lab you used CML Jobs in tandem with CML APIv2, Apache Iceberg, and MLFlow in order to orchestrate a more advanced MLOps pipeline. With just three scripts and a few lines of code, you've implemented a standardized CI/CD Process that adhers to MLOps Best Practices including data and model reproducibility, auditability, explainability. You did this leveraging built-in components and without any custom installations.</p> <ul> <li>In the first job, a new data batch is appended to the Iceberg Credit Card Transaction table.</li> <li>In the second job, you used Iceberg Time Travel in order to read data within specified time boundaries - in other words to access the latest batch of data only - and then retrain the same XGBoost Classifier with this data.</li> <li>Finally, in the last job, you redeployed a new API Endpoint version with the latest model version.</li> </ul>"},{"location":"guide/05-02/index.html#20-model-monitoring","title":"20. Model Monitoring","text":"<p>This section explains the most important aspects of 06_model_simulation.py and 07_model_monitoring.py.</p>"},{"location":"guide/05-02/index.html#201-go-to-the-sessions-tab-select-your-running-session","title":"20.1 Go to the sessions tab &amp; select your running session","text":"<p>If your session expires, then create a new session. </p>"},{"location":"guide/05-02/index.html#202-open-06_model_simulationpy-and-update-connection_name-variable-at-line-no-62","title":"20.2 Open 06_model_simulation.py  and update CONNECTION_NAME variable at line no 62.","text":"<p>Click save. </p> <p>Execute 06_model_simulation.py and immediately navigate out of the Session to the CML Model Deployment. Open the Monitoring tab and watch the monitoring dashboard get updated in real time as requests are received by the model endpoint. </p>"},{"location":"guide/05-02/index.html#203-click-on-1h-as-shown-in-the-image","title":"20.3 Click on 1h as shown in the image.","text":""},{"location":"guide/05-02/index.html#204","title":"20.4","text":""},{"location":"guide/05-02/index.html#205-open-07_model_monitoringpy-in-your-cml-session","title":"20.5 Open 07_model_monitoring.py in your CML Session","text":"<p>Do not edit anything for this step.</p> <p>Run 07_model_monitoring.py. Explore the model monitoring diagrams on the right side of the page. How is your model performing? </p>"},{"location":"guide/05-02/index.html#206-code-highlights","title":"20.6 Code Highlights","text":"<p>06_model_simulation.py creates more synthetic data and leverages the CDSW SDK to interact with the deployed endpoints. The data is submitted to the model along with ground truth in order to simulate a wave of requests to the endpoint.</p> <ul> <li>Line 41: the cdsw sdk is imported. This does not need to be installed.</li> <li>Lines 90-129: a method to submit request batches to the specified Model Deployment endpoint.</li> <li>Lines 133-153: a method to submit ground truth to the specified Model Deployment endpoint.</li> </ul> <p>07_model_monitoring.py shows you how you can monitor model performance programmatically. CML features a Postgres Database called \"Model Metrics Store\" that automatically stores metadata for each request by deployed model. In this script, the cdsw SDK is used again in order to read model metadata and access the Model Metrics Store.</p> <ul> <li>Lines 64-67: the ApiUtility is instantiated to obtain model metadata. The util's source code is located in the src folder. Similarly to the \"mlops\" util you leveraged in 02_api_deployment.py, this util allows you to build custom methods in order to obtain model metadata as needed by your use case.</li> <li>Line 76: cdsw.read_metrics() is used to read the tracked model requests from the Model Metrics Store.</li> <li>Seaborn and Pandas are used throughout the rest of the script in order to create the model performance plots.</li> </ul>"},{"location":"guide/05-02/index.html#207-summary","title":"20.7 Summary","text":"<p>In this lab you used the cdsw SDK to access predictions requests and ground truth backed up in the CML Model Metrics Store in order to monitor model performance (Accuracy).</p> <p>You can leverage the same tools in unison with scheduled CML Jobs to create continuous monitoring systems and, similarly to the previous labs, programmatically deploy new model versions when performance criteria are not met.</p>"},{"location":"guide/05-02/index.html#21-end-of-exercise","title":"21. END of Exercise.","text":""},{"location":"guide/06-01/index.html","title":"06-01_End _to_End_Stock_Data_Analysis","text":""},{"location":"guide/06-02/index.html","title":"End to end alphaa vantage stocks","text":""},{"location":"guide/06-02/index.html#1-end-to-end-alphaa-vantage-stocks","title":"1. End to end alphaa vantage stocks","text":"<p>In this exercise, we will work get stock data from Alpha Vantage, that offers free stock APIs in JSON and CSV formats for real-time and historical stock market data.</p> <ul> <li>Data ingestion and streaming\u2009\u2014\u2009provided by Cloudera Data Flow (CDF) and Cloudera Data Engineering (CDE).</li> <li>Global data access and persistence\u2014\u200bprovided by Cloudera Data Warehouse (CDW).</li> <li>Data visualization with CDP Data Visualization.</li> </ul>"},{"location":"guide/06-02/index.html#2-high-level-steps","title":"2. High-Level Steps","text":"<p>Below are the high-level steps for what we will be doing in the workshop.</p> <p>(1) Get Alpha Vantage key to be used in Cloudera Data Flow (CDF) to collect stock data (IBM, AMZN, MSFT, GOOGL).</p> <p>(2) Create CDF workflow and run it to ingest data into S3.</p> <p>(3) Create Iceberg Table using Cloudera Data Warehouse (CDW/Hue).</p> <p>(4) Create CDE job and run it to ingest data into iceberg table.</p> <p>(5) Use Cloudera Data Viz to create a simple dashboard on Iceberg table.</p> <p>(6) Run the CDE job with updated ticker (NVDA).</p> <p>(7) Use/Test Iceberg time travel features. </p>"},{"location":"guide/06-02/index.html#3-pre-requisites","title":"3. Pre-requisites","text":"<ol> <li>Laptop with a supported OS (Windows 7 not supported) or MacBook.</li> <li>A modern browser - Google Chrome (IE, Firefox, Safari not supported).</li> <li>Wi-Fi Internet connection.</li> <li>Git installed (optional).</li> </ol>"},{"location":"guide/06-02/index.html#4-download-the-exercise-files","title":"4. Download the exercise files","text":"<p>Click here to download</p>"},{"location":"guide/06-02/index.html#41-use-any-unzip-utility-to-download-extract-the-content-of-the-e2e-cdp-alphavantagezip-file","title":"4.1 Use any unzip utility to download extract the content of the e2e-cdp-alphavantage.zip file.","text":"<p>In the extracted content just be sure that the downloaded files has a file Stocks_Intraday_Alpha_Template.json which should be around ~65 KB in size. You will need this file in later step. </p>"},{"location":"guide/06-02/index.html#5-get-alpha-vantage-key","title":"5. Get Alpha Vantage Key","text":"<p>Go to website</p> <pre><code>https://www.alphavantage.co/\n</code></pre> <p>Choose link -&gt; GET FREE API KEY . </p>"},{"location":"guide/06-02/index.html#51-fill-the-details","title":"5.1 Fill the details:","text":"<ol> <li>Choose <code>Student</code> for the question - <code>Which of the following best describes you?</code>.</li> <li>Enter your own organisation name for the question - <code>Organization (e.g. company, university, etc.):</code></li> <li>Enter your email address for the question - <code>Email:</code> (Note: Please enter personal email id and not the workshop email id)</li> <li>Click on <code>GET FREE API KEY</code>. </li> </ol>"},{"location":"guide/06-02/index.html#52-you-should-see-a-message-like-welcome-to-alpha-vantage-your-dedicated-access-key-is-yxxxxxxxxxxxxxxe","title":"5.2 You should see a message like - Welcome to Alpha Vantage! Your dedicated access key is: YXXXXXXXXXXXXXXE.","text":"<p>Please record this API key at a safe place for future data access. </p>"},{"location":"guide/06-02/index.html#6-define-workload-password","title":"6. Define Workload Password","text":"<p>You will need to define your workload password that will be used to acess non-SSO interfaces. You may read more about it: Non-SSO interfaces.</p> <pre><code>https://docs.cloudera.com/management-console/cloud/user-management/topics/mc-access-paths-to-cdp.html\n</code></pre> <p>Click on your user name (Ex: user1) at the lower left corner.</p> <p>Click on the Profile option. </p>"},{"location":"guide/06-02/index.html#61-click-option-set-workload-password","title":"6.1 Click option Set Workload Password.","text":"<p>Enter a Password and Confirm Password. (Use the credentials provided by your instructor to log in to Cloudera on cloud console.)</p> <p>Click button Set Workload Password. </p>"},{"location":"guide/06-02/index.html#62","title":"6.2","text":""},{"location":"guide/06-02/index.html#63-check-that-you-got-the-message-workload-password-is-currently-set-or-alternatively-look-for-a-message-next-to-workload-password-which-says-workload-password-is-currently-set","title":"6.3 Check that you got the message - Workload password is currently set or alternatively, look for a message next to Workload Password which says (Workload password is currently set)","text":""},{"location":"guide/06-02/index.html#7-create-the-flow-to-ingest-stock-data-via-api-to-object-storage","title":"7. Create the flow to ingest stock data via API to Object Storage","text":"<p>Click on Home option on top left corner to go to the landing page. </p>"},{"location":"guide/06-02/index.html#8-click-on-dataflow-icon-as-shown-in-the-image-below","title":"8. Click on DataFlow icon as shown in the image below.","text":""},{"location":"guide/06-02/index.html#9-create-a-new-cdf-catalog","title":"9. Create a new CDF Catalog","text":"<pre><code>1.  On the left menu click on the option -&gt; Catalog.\n\n2.  On the top right corner click the button -&gt; Import Flow Definition.\n</code></pre>"},{"location":"guide/06-02/index.html#10-fill-up-those-parameters","title":"10. Fill up those parameters :","text":"<p>Flow Name</p> <p>(user)-stock-data</p> <p>Depending upon your user name it should be something like - user1-stock-data</p> <p>Nifi Flow Configuration</p> <p>*Upload the file Stocks_Intraday_Alpha_Template.json</p> <p>(Note: You had downloaded this file in Step 4.1 or Step 4.2 depending on what you chose initially.).*</p> <p>Click Import </p>"},{"location":"guide/06-02/index.html#11-the-new-catalog-has-been-added-type-in-the-name-so-that-you-can-only-see-the-one-that-you-had-created-and-not-the-others-for-example-user1-stock-data","title":"11. The new catalog has been added. Type in the name so that you can only see the one that you had created and not the others. For example - user1-stock-data","text":"<p>Now let's deploy it. </p>"},{"location":"guide/06-02/index.html#12-deploy-dataflow","title":"12. Deploy DataFlow","text":"<p>Click on the small arrow towards right of the catalog you just created. </p> <p>Click on Deploy button. </p>"},{"location":"guide/06-02/index.html#13-you-will-need-to-select-your-environment","title":"13. You will need to select your environment","text":"<p>Click on Continue \u2192 </p>"},{"location":"guide/06-02/index.html#14-give-a-name-to-this-dataflow","title":"14. Give a name to this dataflow.","text":"<p>(user)_stock_dataflow</p> <p>Depending on your user name it should be something like - user1_stock_dataflow.</p> <p>Make sure that the right Target Environment is selected. Click Next. </p>"},{"location":"guide/06-02/index.html#15-click-next","title":"15. Click Next.","text":""},{"location":"guide/06-02/index.html#16-update-parameter-as-below","title":"16. Update parameter as below","text":"<ol> <li>CDP_Password</li> </ol> <p>Fill up your CDP workload password here</p> <p>2.CDP_User</p> <p>your user</p> <p>Depending on your user name it should be something like - admin1.</p> <p>(Make sure you are not adding any white spaces after CDP_User and Password)</p> <p>3.S3 Path</p> <p>stocks</p> <p>4.api_alpha_key</p> <p>your Alpha Vantage key</p> <ol> <li>stock_list</li> </ol> <p>IBM</p> <p>GOOGL</p> <p>AMZN</p> <p>MSFT</p> <p>Click Next \u2192. </p>"},{"location":"guide/06-02/index.html#17-nifi-node-sizing-extra-small","title":"17. Nifi Node Sizing : Extra Small","text":"<p>Enable Auto scaling and let the min nodes be 1 and max nodes be 3. </p>"},{"location":"guide/06-02/index.html#18-you-can-define-kpis-in-regards-what-has-been-specified-in-your-dataflow-but-we-will-skip-this-for-now-click-next","title":"18. You can define KPI's in regards what has been specified in your dataflow, but we will skip this for now. Click Next\u2192","text":""},{"location":"guide/06-02/index.html#19-click-deploy-to-launch-the-deployment","title":"19. Click Deploy to launch the deployment.","text":""},{"location":"guide/06-02/index.html#20-the-deployment-will-get-initiated-check-the-deployment-on-the-run-and-look-for-the-status-good-health","title":"20. The deployment will get initiated. Check the deployment on the run and look for the status Good Health.","text":""},{"location":"guide/06-02/index.html#21","title":"21.","text":""},{"location":"guide/06-02/index.html#22-dataflow-is-up-and-running-and-you-can-confirm-the-same-by-looking-at-the-green-tick-and-message-good-health-against-the-dataflow-name-its-will-take-7-minutes-before-you-see-the-green-tick-notice-the-event-history-and-there-are-approximately-8-steps-that-happen-after-the-flow-deployment-you-might-want-to-observe-those","title":"22. Dataflow is up and running and you can confirm the same by looking at the green tick and message Good Health against the dataflow name. It\u2019s will take ~7 minutes before you see the green tick. Notice the Event History and there are approximately 8 steps that happen after the flow deployment. You might want to observe those.","text":""},{"location":"guide/06-02/index.html#23","title":"23.","text":""},{"location":"guide/06-02/index.html#24-after-the-successful-deployment-we-will-start-receiving-stock-information-into-our-bucket-if-you-want-you-can-check-in-your-bucket-under-the-path-s3auserusernamestocksnew","title":"24. After the successful deployment we will start receiving stock information into our bucket. If you want you can check in your bucket under the path s3a:///user/(username)/stocks/new.","text":""},{"location":"guide/06-02/index.html#25-you-can-verify-the-s3-buck-bucket-by-running-below-command","title":"25. you can verify the s3 buck bucket by running below command <p>Make sure to replace the storage location and username from the below command.</p> <p>Copy below command to your notepad and make necessary changes</p> <pre><code>hdfs dfs -ls s3a://cdp-storage-sanket-500-class-240801/user/admin_1_240801/stocks/new\n\n\nhdfs dfs -ls s3a://&lt;storage location&gt;/user/&lt;username&gt;/stocks/new\n</code></pre> <p>Follow the next step to get the storage location</p>","text":""},{"location":"guide/06-02/index.html#26-get-storage-location","title":"26. Get storage location <p>Go to the Management console &gt; Select environment </p>","text":""},{"location":"guide/06-02/index.html#27-click-on-summary-tab-scroll-down","title":"27. Click on Summary tab &amp; scroll down","text":""},{"location":"guide/06-02/index.html#28-copy-storage-location-and-add-it-to-below-command","title":"28. Copy storage location and add it to below command <p>Make sure replace the username as well</p> <pre><code>hdfs dfs -ls s3a://cdp-storage-sanket-500-class-240801/user/admin_1_240801/stocks/new\n</code></pre> <p>.</p> <pre><code>hdfs dfs -ls s3a://&lt;storage location&gt;/user/&lt;username&gt;/stocks/new\n</code></pre> <p></p>","text":""},{"location":"guide/06-02/index.html#29-login-to-datahub-cluster","title":"29. Login to Datahub cluster <p>Click on Datahub cluster &gt; select your Datahub  training-gateway &gt; select Nodes tab &gt; Copy the public IP of the node. </p>","text":""},{"location":"guide/06-02/index.html#30","title":"30.","text":""},{"location":"guide/06-02/index.html#31-login-to-nodes","title":"31. Login to Nodes <p>Run below \"ssh\" command to login to this node</p> <p>User your username and PublicIP which you copied in your previous step &amp; Enter your Password</p> <pre><code>ssh username@PublicIP\n</code></pre> <p>.</p> <pre><code>eg. ssh admin_1_240801@18.142.188.180\n</code></pre> <p></p>","text":""},{"location":"guide/06-02/index.html#32-verify-files-in-s3-bucket","title":"32. Verify files in s3 bucket <p>Run the hdfs command which you have modified in the previous step</p> <pre><code>hdfs dfs -ls s3a://&lt;storage location&gt;/user/&lt;username&gt;/stocks/new\n</code></pre> <p>After running above command you can see we are receiving stock information into our bucket. </p>","text":""},{"location":"guide/06-02/index.html#33-view-nifi-dataflow","title":"33. View Nifi DataFlow <p>Click on blue arrow on the right of your deployed dataflow user1_stock_dataflow. </p>","text":""},{"location":"guide/06-02/index.html#34-select-actions-view-in-nifi-on-top-right-corner","title":"34. Select Actions \u2192 View in NiFi on top right corner.","text":""},{"location":"guide/06-02/index.html#35-double-click-on-the-process-group","title":"35. Double click on the Process group.","text":""},{"location":"guide/06-02/index.html#36-you-can-see-the-nifi-data-flow-that-has-been-deployed-from-the-json-file-you-can-click-each-of-the-processor-groups-to-go-inside-and-see-the-flow-details-make-sure-that-there-are-no-errors-in-the-flow-if-you-see-any-please-let-the-instructor-know","title":"36. You can see the Nifi data flow that has been deployed from the json file. You can click each of the processor groups to go inside and see the flow details. Make sure that there are no errors in the flow. If you see any please let the instructor know","text":""},{"location":"guide/06-02/index.html#37-at-this-stage-you-can-suspend-this-dataflow-go-back-to-actions-manage-deploymentdeployment-manager-actions-stop-flow-we-will-add-a-new-stock-later-and-restart-it","title":"37. At this stage you can suspend this dataflow, go back to Actions &gt; Manage deployment&gt;Deployment Manager -&gt; Actions -&gt; Stop flow. We will add a new stock later and restart it.","text":""},{"location":"guide/06-02/index.html#38-stop-flow","title":"38. Stop flow","text":""},{"location":"guide/06-02/index.html#39","title":"39.","text":""},{"location":"guide/06-02/index.html#40-confirm-that-the-status-is-stopped","title":"40. Confirm that the status is Stopped.","text":""},{"location":"guide/06-02/index.html#41-create-iceberg-table","title":"41. Create Iceberg Table <p>Now we are going to create the Iceberg table. Click on Home option on top left corner to go to the landing page. </p>","text":""},{"location":"guide/06-02/index.html#42-from-the-cdp-portal-or-cdp-menu-choose-data-warehouse","title":"42. From the CDP Portal or CDP Menu choose Data Warehouse.","text":""},{"location":"guide/06-02/index.html#43-from-the-cdw-overview-window-click-the-hue-button-on-the-right-corner-as-shown-under-the-virtual-warehouses-to-the-right","title":"43. From the CDW Overview window, click the \"HUE\" button on the right corner as shown under the Virtual Warehouses to the right.","text":""},{"location":"guide/06-02/index.html#44-now-youre-accessing-to-the-sql-editor-called-hue-hadoop-user-experience","title":"44. Now you\u2019re accessing to the sql editor called - \"HUE\" (Hadoop User Experience). <p>Make sure that the Impala engine is selected to interact with the database. </p>","text":""},{"location":"guide/06-02/index.html#45-create-database-using-your-login-for-example-wuser00-replace-user_id-by-your-username-for-database-creation-in-the-command-below","title":"45. Create database using your login For example: wuser00. Replace user_id by your username for database creation in the command below. <pre><code>CREATE DATABASE ${user_id}_stocks;\n</code></pre> <p>See the result to notice a message Database has been created. </p>","text":""},{"location":"guide/06-02/index.html#46-after-creating-the-database-create-an-iceberg-table-replace-user_id-by-your-username-for-iceberg-table-creation-in-the-hue-browser-similar-to-previous-step","title":"46. After creating the database create an Iceberg table. Replace user_id by your username for iceberg table creation in the HUE browser similar to previous step. <pre><code>CREATE TABLE IF NOT EXISTS ${user_id}_stocks.stock_intraday_1min (\n  interv STRING,\n  output_size STRING,\n  time_zone STRING,\n  open DECIMAL(8,4),\n  high DECIMAL(8,4),\n  low DECIMAL(8,4),\n  close DECIMAL(8,4),\n  volume BIGINT)\nPARTITIONED BY (\n  ticker STRING,\n  last_refreshed string,\n  refreshed_at string)\nSTORED AS iceberg;\n</code></pre> <p>See the result to notice a message Table has been created.</p> <p>Let\u2019s now create our engineering process. </p>","text":""},{"location":"guide/06-02/index.html#47-process-and-ingest-iceberg-using-cde","title":"47.  Process and Ingest Iceberg using CDE <p>Now we will use Cloudera Data Engineering to check the files in the object storage that were populated as a part of the above DataFlow run and then compare if it's new data, and insert them into the Iceberg table.</p>","text":""},{"location":"guide/06-02/index.html#48-click-on-home-option-on-top-left-corner-to-go-to-the-landing-page","title":"48. Click on Home option on top left corner to go to the landing page.","text":""},{"location":"guide/06-02/index.html#49-from-the-cdp-portal-or-cdp-menu-choose-data-engineering","title":"49. From the CDP Portal or CDP Menu choose Data Engineering.","text":""},{"location":"guide/06-02/index.html#50-lets-create-a-job-click-on-jobs","title":"50. Let\u2019s create a job. Click on Jobs <p>Then click Create Job button in the right side of the screen.</p> <p>Note: This page may differ a little bit depending on the fact that some user may have created a job prior to you or not. </p>","text":""},{"location":"guide/06-02/index.html#51-fill-the-following-values-carefully","title":"51. Fill the following values carefully. <p>Job Type*</p> <p>Choose Spark 3.2.3</p> <p>Name*</p> <p>Replace (user) with your username. For example: user1-StockIceberg.</p> <p>(user)-StockIceberg</p> <p>Make sure Application File that is selected is resource. Select the option Upload</p> <p>Select stockdata-job -&gt; stockdatabase_2.12-1.0.jar</p> <p>Resource Name: Userx-stock </p>","text":""},{"location":"guide/06-02/index.html#52","title":"52. <p>Main Class</p> <pre><code>com.cloudera.cde.stocks.StockProcessIceberg\n</code></pre> <p>.</p> <p>.Make sure the below arguments are filled so that (user) is replaced with the actual username. For example wuser00_stocks and instead of (user) at the end it is wuser00. Make sure to check the next screenshot to comply.</p> <p><code>Arguments</code></p>  <p>(user)_stocks</p> <p>s3a://meta-workshop/ (Storage account see the step 28</p> <p>stocks</p> <p>(user) </p>","text":""},{"location":"guide/06-02/index.html#53-click-the-create-and-run-button-at-the-bottom-there-is-no-screenshot-for-the-same-note-it-might-take-3-minutes-so-its-okay-to-wait-until-its-done","title":"53. Click the Create and Run button at the bottom. (There is no screenshot for the same). Note: It might take ~3 minutes. So, it\u2019s okay to wait until it\u2019s done. <p>This application will:</p> <ul> <li>Check new files in the new directory;</li> <li>Create a temp table in Spark/cache this table and identify duplicated rows (in case that NiFi loaded the same data again);</li> <li>MERGE INTO the final table, INSERT new data or UPDATE if exists;</li> <li>Archive files in the bucket; </li> </ul>","text":""},{"location":"guide/06-02/index.html#54-after-execution-the-processed-files-will-be-in-your-bucket-but-under-the-name-which-has-the-format-processeddate","title":"54. After execution, the processed files will be in your bucket but under the name which has the format - processed\"+date/.","text":""},{"location":"guide/06-02/index.html#55-checking-logs-of-cde-job-run","title":"55. Checking Logs of CDE Job Run <p>Click on the Job Name </p>","text":""},{"location":"guide/06-02/index.html#56-click-on-the-run-id","title":"56. Click on the Run Id.","text":""},{"location":"guide/06-02/index.html#57-click-the-logs-and-go-through-the-stdout-to-understand-better","title":"57. Click the Logs and go through the stdout to understand better. <p>Under Logs tab check for the following. In most of the cases Processing temp dirs indicates that job would run successfully and is in it's last stages. </p>","text":""},{"location":"guide/06-02/index.html#58-optional-you-can-aslo-verify-that-using-below-hdfs-command","title":"58. (optional) You can aslo verify that using below hdfs command <pre><code>hdfs dfs -ls s3a://cdp-storage-sanket-500-class-240801/user/admin_1_240801/stocks/\n</code></pre>","text":""},{"location":"guide/06-02/index.html#59-create-dashboard-using-cdp-dataviz","title":"59. Create Dashboard using CDP DataViz <p>Note: Before moving ahead with this section make sure that the CDE job ran successfully. Go to Job Runs option in the left pane and look for the job that you ran now. It should have a green tick box next to it's name.</p> <p>We will now create a simple dashboard using Cloudera Data Viz. </p>","text":""},{"location":"guide/06-02/index.html#60-click-on-home-option-on-top-left-corner-to-go-to-the-landing-page","title":"60. Click on Home option on top left corner to go to the landing page.","text":""},{"location":"guide/06-02/index.html#61-from-the-cdp-portal-or-cdp-menu-choose-data-warehouse","title":"61. From the CDP Portal or CDP Menu choose Data Warehouse.","text":""},{"location":"guide/06-02/index.html#62-then-click-on-the-data-visualization-option-in-the-left-window-pane-youll-see-an-option-data-viz-next-to-the-data-viz-application-with-the-name-dataviz-it-should-open-a-new-window","title":"62. Then click on the Data Visualization option in the left window pane. You\u2019ll see an option Data VIZ next to the data-viz application with the name dataviz. It should open a new window.+","text":""},{"location":"guide/06-02/index.html#63-you-will-access-to-the-following-window","title":"63. You will access to the following window. <p>Click close </p>","text":""},{"location":"guide/06-02/index.html#64-choose-data-on-the-upper-menu-bar-next-to-the-options-of-home-sql-visuals","title":"64. Choose DATA on the upper menu bar next to the options of HOME, SQL, VISUALS. <p>Then click on New Connection </p>","text":""},{"location":"guide/06-02/index.html#65-fill-the-below-deatils","title":"65. Fill the below deatils. <ol> <li> <p>Connection type: CDW Impala</p> </li> <li> <p>Connection name: Userx-DataViz</p> </li> <li> <p>CDW Warehouse: Select from the drop-down</p> </li> </ol> <p>Hostname or IP address: Gets automatically selected.</p> <p>Port: <code>Gets automatically filled up</code>.</p> <p>Username: <code>Gets automatically filled up.</code></p> <p>Password: <code>Blank</code></p> <ol> <li>Click on Test.</li> </ol> <p>Once the connection is verified.</p> <ol> <li>Click Connect. </li> </ol>","text":""},{"location":"guide/06-02/index.html#66-you-will-see-your-connection-in-the-all-connections-list","title":"66. You will see your connection in the All connections list <p>Select your connection(userx-dataviz) then click on NEW DATASET </p>","text":""},{"location":"guide/06-02/index.html#67-replace-user-with-your-username-wherever-it-is-applicable","title":"67. Replace (user) with your username wherever it is applicable. <p>Dataset title</p> <p>(user)_dataset</p> <p>Dataset Source</p> <p>From Table</p> <p>Select Database</p> <p>(user)_stocks</p> <p>Select Table</p> <p>stock_intraday_1min</p> <p>Click CREATE. </p>","text":""},{"location":"guide/06-02/index.html#68-select-new-dashboard-icon-next-to-the-table-that-you-created-just-now","title":"68. Select \"New Dashboard\" -&gt;  icon next to the Table that you created just now.","text":""},{"location":"guide/06-02/index.html#69-youll-land-in-the-following-page","title":"69. You\u2019ll land in the following page.","text":""},{"location":"guide/06-02/index.html#70-lets-drag-from-data-section-on-the-right-under-dashboard-designer-the-following-attributemetric-and-the-refresh-the-visual","title":"70. Let\u2019s drag from DATA section on the right under Dashboard Designer the following attribute/metric. And the REFRESH THE VISUAL <p>Dimensions -&gt; ticker</p> <p>Move it to Visuals -&gt; Dimensions</p> <p>Measures -&gt; #volume</p> <p>Move it to Visuals -&gt; Measures </p>","text":""},{"location":"guide/06-02/index.html#71-then-on-visuals-choose-packed-bubbles","title":"71. Then on VISUALS choose Packed Bubbles. <p>Click on Refresh Visual.</p> <p>Your visual could be slighltly different from the image here. </p>","text":""},{"location":"guide/06-02/index.html#72-make-it-public-by-changing-the-option-from-private-to-public-save-it-by-clicking-the-save-button-on-the-top","title":"72. Make it PUBLIC by changing the option from PRIVATE to PUBLIC. Save it by clicking the SAVE button on the top. <p>You have succeeded to create a simple dashboard. Now, let\u2019s query our data and explore the time-travel and snapshot capabilties of Iceberg. </p>","text":""},{"location":"guide/06-02/index.html#73-query-iceberg-tables-in-hue-and-cloudera-data-visualization","title":"73. Query Iceberg Tables in Hue and Cloudera Data Visualization <p>Our example will load the intraday stock daily since the free API does not give real-time data, but we can change the Cloudera Dataflow Parameter to add one more ticker and we've scheduled to run hourly the CDE process. After this we will be able to see the new ticker information in the dashboard and also perform time travel using Iceberg!</p>","text":""},{"location":"guide/06-02/index.html#74-logging-into-hue","title":"74. Logging into Hue <p>From the CDW Overview window, click the \"HUE\" button on the right corner as shown under the Virtual Warehouses to the right. Make sure that the correct Virtual Warehouse is selected. </p>","text":""},{"location":"guide/06-02/index.html#75-now-youre-accessing-to-the-sql-editor-called-hue-make-sure-that-you-can-see-impala","title":"75. Now you\u2019re accessing to the sql editor called \"HUE\". Make sure that you can see Impala.","text":""},{"location":"guide/06-02/index.html#76-iceberg-snapshots","title":"76. Iceberg snapshots <p>Let's see the Iceberg table history. Replace user_id with your username. For example: wuser00.</p> <pre><code>DESCRIBE HISTORY ${user_id}_stocks.stock_intraday_1min;\n</code></pre> <p></p>","text":""},{"location":"guide/06-02/index.html#77-copy-the-snapshot-id-to-your-notepad","title":"77. Copy the snapshot id to your notepad","text":""},{"location":"guide/06-02/index.html#78-copy-and-paste-the-snapshot_id-and-use-it-on-the-following-impala-queries-replace-user_id-with-your-username-for-example-wuser00","title":"78. Copy and paste the snapshot_id and use it on the following impala queries. Replace user_id with your username. For example: wuser00. <pre><code>SELECT ticker, count(*)\nFROM ${user_id}_stocks.stock_intraday_1min\nFOR SYSTEM_VERSION AS OF ${snapshot_id}\nGROUP BY ticker;\n</code></pre>","text":""},{"location":"guide/06-02/index.html#79-add-a-new-stock-nvda","title":"79. Add a New stock (NVDA) <p>We shall load new data and this time we will include additional stock ticker - NVDA. Go to CDF, and find the data flow that you had created earlier. It should be in stopped state if you had stopped it towards the end of</p> <p>Step 5: Create the flow to ingest stock data via API to Object Storage section of the workshop. </p>","text":""},{"location":"guide/06-02/index.html#80-go-to-cloudera-data-flow-option-and-look-for-the-flow-that-you-had-created-earlier-based-on-your-user-name-ex-wuser00_stock_dataflow","title":"80. Go to Cloudera Data Flow option and look for the flow that you had created earlier based on your user name. Ex - wuser00_stock_dataflow.","text":""},{"location":"guide/06-02/index.html#81-click-on-the-arrow-towards-the-right-side-of-the-flow-and-then-click-on-actions-manage-deployment","title":"81. Click on the arrow towards the right side of the flow and then click on Actions \u2192 Manage Deployment.","text":""},{"location":"guide/06-02/index.html#82-click-on-parameter","title":"82. Click on parameter","text":""},{"location":"guide/06-02/index.html#83-click-on-the-parameters-tab-and-then-scroll-down-to-the-text-box-where-you-had-entered-stock-tickers-stock_list","title":"83. Click on the Parameters tab and then scroll down to the text box where you had entered stock tickers (stock_list).","text":""},{"location":"guide/06-02/index.html#84-add-the-stock-nvda-and-then-click-on-apply-changes","title":"84. Add the stock NVDA. And then click on Apply Changes.","text":""},{"location":"guide/06-02/index.html#85","title":"85.","text":""},{"location":"guide/06-02/index.html#86-now-start-the-flow-again-by-clicking-actions-and-then-start-flow","title":"86. Now, start the flow again by clicking Actions and then Start flow.","text":""},{"location":"guide/06-02/index.html#87","title":"87.","text":""},{"location":"guide/06-02/index.html#88","title":"88.","text":""},{"location":"guide/06-02/index.html#89-the-s3-bucket-gets-updated-with-new-data-and-this-time-it-includes-the-new-ticker-nvda-as-well-we-will-see-it-you-can-see-the-same-in-s3-bucket-as-shown-here","title":"89. The S3 bucket gets updated with new data and this time it includes the new ticker NVDA as well. We will see it. You can see the same in S3 bucket as shown here.","text":""},{"location":"guide/06-02/index.html#90","title":"90.","text":""},{"location":"guide/06-02/index.html#91-you-can-verify-the-same-by-running-hdfs-command-similar-to-previous-step","title":"91. You can verify the same by running hdfs command similar to previous step <p>Ask your instructor to verify or you can verify the same.</p> <p>Modify the below commands as per your username and storage.</p> <pre><code>hdfs dfs -ls s3a://cdp-storage-sanket-500-class-240801/user/admin_1_240801/stocks/\n</code></pre> <p></p>","text":""},{"location":"guide/06-02/index.html#92","title":"92.","text":""},{"location":"guide/06-02/index.html#93-now-go-to-cloudera-data-engineering-from-the-home-page-and-jobs-choose-the-cde-job-that-you-had-created-earlier-with-your-username","title":"93. Now go to Cloudera Data Engineering from the home page and Jobs. Choose the CDE Job that you had created earlier with your username.","text":""},{"location":"guide/06-02/index.html#94","title":"94.","text":""},{"location":"guide/06-02/index.html#95-click-the-3-dots-next-to-your-job-that-you-had-created-earloer-and-then-click-on-run-now","title":"95. Click the 3 dots next to your job that you had created earloer and then click on Run Now.","text":""},{"location":"guide/06-02/index.html#96-click-on-job-runs-in-the-left-to-see-the-status-of-the-job-that-was-initiated-now-it-should-succeed","title":"96. Click on Job Runs in the left to see the status of the job that was initiated now. It should succeed.","text":""},{"location":"guide/06-02/index.html#97-as-cdf-has-ingested-a-new-stock-value-and-then-cde-has-merged-those-value-it-has-created-new-iceberg-snapshots","title":"97. As CDF has ingested a new stock value and then CDE has merged those value it has created new Iceberg snapshots.","text":""},{"location":"guide/06-02/index.html#98-check-new-snapshot-history","title":"98. Check new snapshot history <p>Now let check again the snapshot history by going to Hue.</p> <pre><code>DESCRIBE HISTORY ${user_id}_stocks.stock_intraday_1min;\n</code></pre> <p>Copy the snapshot id. </p>","text":""},{"location":"guide/06-02/index.html#99-paste-the-new-snapshot_id-and-use-it-on-the-following-impala-query","title":"99. Paste the new snapshot_id and use it on the following impala query. <pre><code>SELECT ticker, count(*)\nFROM ${user_id}_stocks.stock_intraday_1min\nFOR SYSTEM_VERSION AS OF ${new_snapshot_id}\nGROUP BY ticker;\n</code></pre> <p>Now, we can see that this snapshot retrieves the count value for stock NVDA that has been added in the CDF stock_list parameter. </p>","text":""},{"location":"guide/06-02/index.html#100-show-data-files","title":"100. Show Data Files <p>Replace user_id with your username. For example: wuser00.</p> <p>.</p> <pre><code>show files in ${user_id}_stocks.stock_intraday_1min;\n</code></pre> <p></p>","text":""},{"location":"guide/06-02/index.html#101-check-the-iceberg-table-replace-user_id-with-your-username-for-example-wuser00","title":"101. Check the Iceberg table. Replace user_id with your username. For example: wuser00. <pre><code>describe formatted ${user_id}_stocks.stock_intraday_1min;\n</code></pre>","text":""},{"location":"guide/06-02/index.html#102-note-please-make-sure-that-the-data-flow-that-was-created-by-you-is-suspended-else-it-will-be-running-continously-to-suspend-the-flow-do-the-following","title":"102. Note: \ud83d\udd34 Please make sure that the data flow that was created by you is Suspended else it will be running continously. To suspend the flow, do the following.","text":""},{"location":"guide/06-02/index.html#103","title":"103.","text":""},{"location":"guide/06-02/index.html#104","title":"104.","text":""},{"location":"guide/06-02/index.html#105","title":"105.","text":""},{"location":"guide/06-02/index.html#106-end-of-exercise","title":"106. End of exercise.","text":""},{"location":"guide/07-01/index.html","title":"07-01 Course Summary","text":""},{"location":"guide/07-01/index.html#1-introductions","title":"1. Introductions","text":""},{"location":"guide/07-01/index.html#2-prepare-students-for-career-success","title":"2. Prepare Students for Career Success","text":""},{"location":"guide/07-01/index.html#3-after-class","title":"3. After Class","text":""},{"location":"guide/07-01/index.html#4-learning-path-for-platform-administrators","title":"4. Learning Path for Platform Administrators","text":""},{"location":"guide/07-01/index.html#5-thank-you","title":"5. Thank You","text":""},{"location":"tests/index.html","title":"Test Suite for Predicting with Cloudera Machine Learning","text":""},{"location":"tests/cml/index.html","title":"Environment Test Suite","text":""}]}