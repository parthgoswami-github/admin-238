{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Just Enough Kubernetes","text":"<p>Version 1.0.0</p> <p>Welcome to Just Enough Kubernetes. Learning the architecture for orchestrating microservices. Follow along with your instructor to complete each section.</p>"},{"location":"index.html#about-this-training","title":"About This Training","text":"<p>Kubernetes is an open-source platform that automates the management of containerized applications. It's also known as \"K8s\". Kubernetes has fundamentally transformed the software development and operations of the modern enterprise. It is the de facto standard for container orchestration. While it addresses a critical need for deploying and managing containers, it is by necessity a complex system. </p> <p></p> <p>This course provides an explanation of architecture and the basic skills required for understanding Kubernetes. The course is focused on explaining how it works by defining the architecture for clusters, the control plane, and resources. The course requires students to install a small-scale Kubernetes environment and run a basic deployment of containers.</p>"},{"location":"index.html#who-should-take-this-course","title":"Who Should Take This Course?","text":"<p>This course is an entry point for learning Kubernetes. This course is intended for a broad audience, to include managers, project managers, developers, and operators. In short, anyone who wants to have a working knowledge and baseline skill of Kubernetes.</p>"},{"location":"index.html#course-details","title":"Course Details","text":"<ul> <li> <p>Get Aboard</p> <ul> <li>Installing Kubernetes minikube</li> <li>Downloading the Github repo</li> <li>The kubectl command</li> </ul> </li> <li> <p>Mastering Kubernetes</p> <ul> <li>Design principles</li> <li>Learning paths</li> <li>References</li> </ul> </li> <li> <p>Cluster Architecture</p> <ul> <li>Hosts configurations</li> <li>Open Container Initiative</li> <li>ContainerD</li> <li>ContainerD CLI</li> <li>Exercises</li> </ul> </li> <li> <p>Control Plane Architecture</p> <ul> <li>API Server</li> <li>Controller Manager</li> <li>Scheduler</li> <li>Persistent storage with etcd</li> <li>Cluster DNS</li> <li>Kube-proxy</li> <li>Exercises</li> </ul> </li> <li> <p>Resource Architecture</p> <ul> <li>Resources</li> <li>Namespace</li> <li>Pods</li> <li>Services</li> <li>Deployments</li> <li>Network</li> <li>Access Control</li> <li>Storage</li> <li>Exercises</li> </ul> </li> <li> <p>Manifest Files</p> <ul> <li>YAML file basics </li> <li>Manifest files</li> <li>Imperative vs Declarative</li> <li>System for Imperative</li> <li>System for Declarative</li> <li>Exercises</li> </ul> </li> <li> <p>Deployment</p> <ul> <li>Run Pods</li> <li>Expose Services</li> <li>Deploy deployments</li> <li>Labels and Selectors</li> <li>Overview networking</li> <li>Exercises</li> </ul> </li> </ul>"},{"location":"guide/index.html","title":"Table of Contents","text":"<p>01-01 Course Introduction 02-01 Get Aboard 03-01 Design Principles for Kubernetes 04-01 Access and Interact 05-01 Microservices 06-01 Architecture for Kubernetes Resources 07-01 Kubernetes Manifest Files 08-01 Architecture for Kubernetes Cluster 09-01 Architecture for Kubernetes Control Plane 10-01 Next Steps Appendix-A Cloudera Support </p>"},{"location":"guide/01-01/index.html","title":"01-01 Course Introduction","text":""},{"location":"guide/01-01/index.html#1-introduction","title":"1. Introduction","text":""},{"location":"guide/01-01/index.html#2-welcome-to-cloudera","title":"2. Welcome to Cloudera","text":""},{"location":"guide/01-01/index.html#3-course-objectives","title":"3. Course Objectives","text":""},{"location":"guide/01-01/index.html#4-table-of-contents","title":"4. Table of Contents","text":""},{"location":"guide/01-01/index.html#5-summary-and-exercise-assignments","title":"5. Summary and Exercise Assignments","text":""},{"location":"guide/01-02/index.html","title":"01-02 Getting Aboard","text":""},{"location":"guide/01-02/index.html#1-accessing-cloudera-management-console","title":"1. Accessing Cloudera Management Console","text":""},{"location":"guide/01-02/index.html#11-login-to-edu-keycloak","title":"1.1. Login to edu-keycloak","text":"<p>Get the edu-keycloak URL from the instructor, and log in using the provided username and a password.</p> <p></p>"},{"location":"guide/01-02/index.html#12-click-maybe-later","title":"1.2. Click 'Maybe Later'","text":"<p>You might be prompted to try out the new UI; however, the exercises were created based on the old UI. To ensure a consistent hands-on experience, please select the 'Maybe Later' button.</p> <p>You can also switch between the old and new UI using the toggle button in the top right corner.</p> <p></p>"},{"location":"guide/01-02/index.html#13-review-console","title":"1.3. Review Console","text":"<p>You will be presented with all the Cloudera Data Services tiles in the Cloudera Management Console homepage.</p> <p></p>"},{"location":"guide/01-02/index.html#2-access-the-cloudera-data-services-documentation","title":"2. Access the Cloudera Data Services Documentation","text":""},{"location":"guide/01-02/index.html#21-cloudera-data-services-docs","title":"2.1. Cloudera Data Services Docs","text":"<p>The Cloudera Data Services Documentation is located at https://docs.cloudera.com/?tab=cdp-public-cloud</p> <p></p>"},{"location":"guide/01-02/index.html#3-access-the-cloudera-blog-main-page","title":"3. Access the Cloudera Blog Main Page","text":"<p>In this part of the exercise, you will access the Cloudera Blog</p>"},{"location":"guide/01-02/index.html#31-the-cloudera-blog-main-page","title":"3.1. The Cloudera Blog Main Page","text":"<p>The Cloudera Blog is located at https://blog.cloudera.com. Here, you will find relevant and timely articles, videos, a blog search function, and the Cloudera Twitter feed.</p> <p></p>"},{"location":"guide/01-02/index.html#32-the-category-and-blog-filter-function","title":"3.2. The Category and Blog Filter Function","text":"<p>First, select a category from Business, Technical, or Culture and then use the Cloudera Blog filter function to narrow the scope of the articles you see. As you'll see in the next two slides, the content varies depending on which category you select.</p> <p></p>"},{"location":"guide/01-02/index.html#33-category-and-filter-functions","title":"3.3. Category and Filter Functions","text":"<p>Note the differences in articles retrieved by selecting the Technical category and Cloudera Data Services in the Filter By section that you saw in the previous slide.</p> <p></p>"},{"location":"guide/01-02/index.html#34-blog-category-and-filter-results","title":"3.4. Blog Category and Filter Results","text":"<p>This slide shows that selecting the Partners category and filtering articles by different Data Services.</p> <p></p>"},{"location":"guide/01-02/index.html#4-the-cloudera-community","title":"4. The Cloudera Community","text":""},{"location":"guide/01-02/index.html#41-the-cloudera-community","title":"4.1. The Cloudera Community","text":"<p>You can begin your Cloudera Community experience by scrolling the homepage. </p> <p></p>"},{"location":"guide/01-02/index.html#42-asking-questions-and-searching-for-answers","title":"4.2. Asking Questions and Searching for Answers","text":"<p>You can ask a question of the community or search for an answer that might already exist. Try the Advanced Search if you need to narrow your search.</p> <p></p>"},{"location":"guide/01-02/index.html#43-get-started","title":"4.3. Get Started","text":"<p>Click the Get Started link on the Cloudera Community main page to view some general information or to register as a new user.</p> <p></p>"},{"location":"guide/01-02/index.html#44-the-cloudera-support-portal-main-page","title":"4.4. The Cloudera Support Portal Main Page","text":"<p>From this Cloudera Support Portal main page, you may access documentation, downloads, training opportunities, and create and manage your support cases (Login required).</p> <p></p>"},{"location":"guide/01-02/index.html#5-end-of-exercise","title":"5. End of Exercise","text":""},{"location":"guide/02-01/index.html","title":"02-01 Get Aboard","text":""},{"location":"guide/02-01/index.html#1-introduction","title":"1. Introduction","text":""},{"location":"guide/02-01/index.html#2-access-to-courseware","title":"2. Access to Courseware","text":""},{"location":"guide/02-01/index.html#3-classroom-environment","title":"3. Classroom Environment","text":""},{"location":"guide/02-01/index.html#4-cloudera-educational-services","title":"4. Cloudera Educational Services","text":""},{"location":"guide/02-01/index.html#5-summary-and-exercise-assignments","title":"5. Summary and Exercise Assignments","text":""},{"location":"guide/02-02/index.html","title":"02-02 Accessing Environment","text":"<p>Get aboard the classroom environment for Cloudera DataFlow. This exercise will walk you through how to acquire the login URL and required credentials.</p>"},{"location":"guide/02-02/index.html#1-student-credentials","title":"1. Student Credentials","text":""},{"location":"guide/02-02/index.html#11-review-login-details","title":"1.1. Review login details","text":"<p>Review the student email for the log in credentials and the URL to the login page.</p> <p>Make a note of the workload username and password as you will need it from time to time throughout the exercises. </p> <p></p>"},{"location":"guide/02-02/index.html#12-lab-environment","title":"1.2. Lab Environment","text":"<p>This course provides a lab environment which is a Cloudera Public Cloud in which the following environments are pre-created:</p> <p>(1) Environment should be enabled as part of the CDF Data Service - [devops-570-class-{class-id}]  </p> <p>(2) Streams Messaging Data Hub Cluster should be created and running - [edu-ds-messaging-{class-id}]</p> <p>(3) Stream analytics Data Hub cluster should be created and running - [edu-ds-analytics-{class-id}]</p>"},{"location":"guide/02-02/index.html#2-download-zip-files","title":"2. Download Zip files","text":"<p>This module will need you to have access to a log data file. </p> <p>Click here to download the Syslog to Kafka json file.</p>"},{"location":"guide/02-02/index.html#3-accessing-cloudera-data-flow","title":"3. Accessing Cloudera Data Flow","text":""},{"location":"guide/02-02/index.html#31-login-to-edu-keycloak","title":"3.1. Login to edu-keycloak","text":"<p>Get the edu-keycloak URL from the instructor, and log in using the provided username and a password. You will be able to copy and paste in both the user ID and the password.</p> <p></p>"},{"location":"guide/02-02/index.html#32-review-home-page","title":"3.2. Review Home Page","text":"<p>You should be able to get the following home page of Cloudera Public Cloud.</p> <p>Select the Data Flow tile from the Cloudera Public Cloud Enterprise Data web interface.</p> <p></p>"},{"location":"guide/02-02/index.html#4-tour-the-home-page","title":"4. Tour the Home Page","text":""},{"location":"guide/02-02/index.html#41-view-the-navigation-panel","title":"4.1. View the Navigation Panel","text":"<p>The Main navigation panel enables access to the following Cloudera DataFlow functions:</p> <ul> <li>Deployments</li> <li>Catalog</li> <li>ReadyFlow Gallery</li> <li>Flow Design</li> <li>Projects</li> <li>Resources</li> <li>Functions</li> <li>Environments</li> </ul> <p></p>"},{"location":"guide/02-02/index.html#42-review-overview-page","title":"4.2. Review Overview page","text":"<p>The Overview page lists access to various functions, including:</p> <ul> <li>Build Flows</li> <li>Organize &amp; Control Access</li> <li>Import existing flows</li> <li>Deploy a ReadyFlow</li> </ul> <p></p>"},{"location":"guide/02-02/index.html#5-end-of-exercise","title":"5. End of Exercise","text":""},{"location":"guide/02-03/index.html","title":"02-03 Bookmarking Cloudera Resources","text":"<p>The purpose of this exercise is to bookmark Cloudera resources available to a Cloudera Data Platform Administrator. This exercise is best run on your own browser. You will create a folder and bookmark a number of sites for future reference.</p>"},{"location":"guide/02-03/index.html#1-open-your-browser","title":"1. Open Your Browser","text":""},{"location":"guide/02-03/index.html#11-open-create-folder","title":"1.1. Open Create Folder","text":"<p>On your local browser's bookmark bar create a folder.</p> <p></p>"},{"location":"guide/02-03/index.html#12-create-folder","title":"1.2. Create Folder","text":"<p>Create a Cloudera folder in the bookmarks.</p> <pre><code>Cloudera\n</code></pre> <p></p>"},{"location":"guide/02-03/index.html#2-tour-cloudera-resources","title":"2. Tour Cloudera Resources","text":""},{"location":"guide/02-03/index.html#21-bookmark-cloudera-resources","title":"2.1. Bookmark Cloudera Resources","text":"<p>This page is an excellent front end to all of Cloudera web sites of value to the CDP Administrator. Add bookmark:</p> <pre><code>https://www.cloudera.com/users.html\n</code></pre> <p></p>"},{"location":"guide/02-03/index.html#22-tour-demo-videos","title":"2.2. Tour Demo Videos","text":"<p>Click Videos. There are a number of excellent demonstrations of Cloudera products. These two to five minute overviews lead a user through the fundamentals of Cloudera products. Scan the list and select one to follow.</p> <p></p>"},{"location":"guide/02-03/index.html#3-tour-cloudera-documentation","title":"3. Tour Cloudera Documentation","text":""},{"location":"guide/02-03/index.html#31-bookmark-cloudera-docs","title":"3.1. Bookmark Cloudera Docs","text":"<p>Cloudera documentation is constantly improving and is a valuable resource. Add bookmark:</p> <pre><code>https://docs.cloudera.com/cdp-private-cloud/latest/index.html\n</code></pre> <p></p>"},{"location":"guide/02-03/index.html#32-review-landing-page","title":"3.2. Review Landing Page","text":"<p>Cloudera documentation is divided into CDP Public Cloud and CDP Private Cloud. This is to account for the Data Services in Private Cloud following a delay in rollout behind CDP Public Cloud. Document is presented with a primary menu in black to the left side. There are two submenus. One on each side of the page.</p> <p></p>"},{"location":"guide/02-03/index.html#33-cdp-private-cloud-base","title":"3.3. CDP Private Cloud Base","text":"<p>Click Private Cloud Base.</p> <p>CDP Private Cloud Base is the CDP Runtime built on hosts or virtual machines. It is the \"base\" for building up Embedded Container Services. Having prior experience and knowledge of CDP Private Cloud Base is a prerequisite for the ADMIN-335 Running Cloudera Private Cloud.</p> <p></p>"},{"location":"guide/02-03/index.html#34-open-installation-instructions","title":"3.4. Open Installation Instructions","text":"<p>Select Getting Started &gt; Installation &gt; Installing CDP Private Cloud Data Services.</p> <p></p>"},{"location":"guide/02-03/index.html#35-review-cdp-private-cloud-data-services","title":"3.5. Review CDP Private Cloud Data Services","text":"<p>Note at the top of the page the version of Data Services. This is version 1.5.0 (latest). Clicking the up arrowhead will present a menu of previous versions.</p> <p>The left hand table of contents presents the primary subjects. A recommended reading to start is the Overview.</p> <p></p>"},{"location":"guide/02-03/index.html#36-open-installing-on-the-embedded-container-service","title":"3.6. Open Installing on the Embedded Container Service","text":"<p>Click Installing on the Embedded Container Service.</p> <p></p>"},{"location":"guide/02-03/index.html#37-scan-through-the-requirements-and-the-installation-pages","title":"3.7. Scan through the Requirements and the Installation Pages","text":"<p>Take a few minutes and scan through pages for requirements and installation. Many of these topics will be discussed in lectures and supported with exercises in this course. Being familiar with documentation is always important to a CDP administrator.</p> <p></p>"},{"location":"guide/02-03/index.html#4-tour-cloudera-blog","title":"4. Tour Cloudera Blog","text":""},{"location":"guide/02-03/index.html#41-open-cloudera-blog","title":"4.1. Open Cloudera Blog","text":"<p>Add bookmark:</p> <pre><code>blog.cloudera.com\n</code></pre> <p>Click Technical.</p> <p></p>"},{"location":"guide/02-03/index.html#42-search-for-data-services","title":"4.2. Search for Data Services","text":"<p>Click checkbox CDP Private Cloud. Search \"data services\". A number of technical articles will be listed. These are curated articles written by technical leadings in Cloudera. Scroll through and select one to read.</p> <p></p>"},{"location":"guide/02-03/index.html#5-tour-cloudera-community","title":"5. Tour Cloudera Community","text":"<p>Cloudera community is where the industry learns, shares, and collaborates on the use of Cloudera products.</p> <p>Click Take a Tour of the Community</p>"},{"location":"guide/02-03/index.html#51-open-cloudera-community","title":"5.1. Open Cloudera Community","text":"<p>Add bookmark:</p> <pre><code>https://community.cloudera.com\n</code></pre> <p></p>"},{"location":"guide/02-03/index.html#52-review-the-tour","title":"5.2. Review the Tour","text":"<p>Step through the size screens for touring Cloudera Community.</p> <p>!!! important    All CDP Administrators should have a single sign on account with Cloudera, in particular to make full use of the community. If you do not have a login please take a moment and subscribe now.</p> <p></p>"},{"location":"guide/02-03/index.html#53-hunting-resolutions","title":"5.3. Hunting Resolutions","text":"<p>Search \"Pods are crashing in namespace\". Select \"Pods are Crashing in Namespace shared-services\"</p> <p></p>"},{"location":"guide/02-03/index.html#6-tour-cloudera-educational-services","title":"6. Tour Cloudera Educational Services","text":""},{"location":"guide/02-03/index.html#61-add-bookmark-for-education","title":"6.1. Add Bookmark for Education","text":"<p>Cloudera Education offers educational paths for every role in the Cloudera Data Lifecycle. This is intended to guide a students career progression in Big Data and in Big Compute. Add bookmark:</p> <pre><code>https://www.cloudera.com/about/training.html\n</code></pre> <p></p>"},{"location":"guide/02-03/index.html#7-tour-cloudera-tutorials","title":"7. Tour Cloudera Tutorials","text":""},{"location":"guide/02-03/index.html#71-add-bookmark","title":"7.1. Add Bookmark","text":"<p>The Cloudera tutorials are interactive short videos to provide an overview of all of Cloudera's products. Add bookmark:</p> <pre><code>https://www.cloudera.com/tutorials.html\n</code></pre> <p></p>"},{"location":"guide/02-03/index.html#8-tour-cloudera-on-youtube","title":"8. Tour Cloudera on YouTube","text":""},{"location":"guide/02-03/index.html#81-subscribe-to-cloudera-on-youtube","title":"8.1. Subscribe to Cloudera on YouTube","text":"<p>Search \"Discover Cloudera Data Platform\". This provides a great overview of CDP.</p> <p>Cloudera recommends for all CDP Administrators to subscribe to this youtube channel. Add bookmark:</p> <pre><code>https://www.youtube.com/@ClouderaInc\n</code></pre> <p></p>"},{"location":"guide/02-03/index.html#9-review-bookmarks","title":"9. Review Bookmarks","text":""},{"location":"guide/02-03/index.html#91-review-the-cloudera-bookmarks","title":"9.1. Review the Cloudera Bookmarks","text":""},{"location":"guide/02-03/index.html#10-end-of-exercise","title":"10. End of Exercise","text":""},{"location":"guide/02-04/index.html","title":"02-04 Verify permissions in Apache Ranger","text":"<p>Note</p> <p>THESE STEPS HAVE ALREADY BEEN DONE FOR YOU. This section will walk you through how Permissions/Policies are managed in Ranger. </p>"},{"location":"guide/02-04/index.html#_1","title":"02-04 Verify permissions in Apache Ranger","text":"<p>Caution</p> <p>PLEASE DO NOT EXECUTE THE STEPS IN THIS SECTION OR CHANGE ANYTHING.</p> <p>Important</p> <p>Since we could not grant the required privileges to the workload users to implement the steps in the environment, this exercise has been written as a read-only guide for your future reference. Please go through all the steps, screenshots, policies, permissions in detail. You won't be able to view the policies in your environment. You may request your instructor to provide a short demo.  </p>"},{"location":"guide/02-04/index.html#1-accessing-apache-ranger","title":"1. Accessing Apache Ranger","text":""},{"location":"guide/02-04/index.html#11-click-on-environments","title":"1.1. Click on Environments","text":"<p>Click on the <code>Environments</code> tab on the left pane in Cloudera Management Console Main panel. </p> <p></p>"},{"location":"guide/02-04/index.html#12-navigate-to-management-console","title":"1.2. Navigate to Management Console","text":"<p>Select the environment that is shared by the instructor (Ex: <code>devops-570-class-250204</code>).</p> <p>The environment assigned to you would be named similar except the class ID. </p> <p></p>"},{"location":"guide/02-04/index.html#13-access-ranger-ui","title":"1.3. Access Ranger UI","text":"<p>Click on the Ranger quick link to access the Ranger UI.</p> <p>Also, make a note of the 3 Data Hubs pre-created for the exercises. We will need them in later steps.</p> <p>(1) Environment should be enabled as part of the CDF Data Service - [devops-570-class-250204]</p> <p>(2) Streams Messaging Data Hub Cluster should be created and running - [edu-ds-messaging-250204]</p> <p>(3) Stream analytics Data Hub cluster should be created and running - [edu-ds-analytics-250204]</p> <p>(4) A gateway node to access the services - [training-gateway-250204]</p> <p>In your case, the class ID would be different:</p> <ul> <li>[devops-570-class-{class-id}]   </li> <li>[edu-ds-messaging-{class-id}]</li> <li>[edu-ds-analytics-{class-id}]</li> <li>[training-gateway-{class-id}]</li> </ul> <p></p>"},{"location":"guide/02-04/index.html#14-review-ranger-ui","title":"1.4. Review Ranger UI","text":"<p>Notice the two data hubs are populated under KAFKA and KAFKA-CONNECT. </p> <p></p>"},{"location":"guide/02-04/index.html#2-kafka-permissions","title":"2. Kafka Permissions","text":""},{"location":"guide/02-04/index.html#21-select-the-kafka-repository","title":"2.1. Select the Kafka repository","text":"<p>In Ranger UI, select the Kafka repository that\u2019s associated with the stream messaging datahub.</p> <p>In this case, it will be [edu-ds-messaging 250204_kafka_716a]</p> <p></p>"},{"location":"guide/02-04/index.html#22-verify-user","title":"2.2. Verify user","text":"<p>Verify if the user who will be performing the exercise is present in both <code>all-consumergroup</code> and <code>all-topic</code>.</p> <p>A variable {USER} with all the required permissions in Apache Ranger has been added. This variable {USER} is used to represent each student in this environment in a Ranger Policy. </p> <p>The below image reflects the variable {USER} being part of <code>all-consumergroup</code>.</p> <p></p>"},{"location":"guide/02-04/index.html#23-verify-user","title":"2.3. Verify user","text":"<p>The below image reflects the variable {USER} being part of <code>all-topic</code>.</p> <p></p>"},{"location":"guide/02-04/index.html#3-schema-registry-permissions","title":"3. Schema Registry Permissions","text":"<p>Schema Registry Permissions</p>"},{"location":"guide/02-04/index.html#31-navigate-to-ranger-home-page","title":"3.1. Navigate to Ranger Home Page","text":"<p>Click on the Ranger icon in the top left corner to navigate back to the Ranger home page. </p> <p></p>"},{"location":"guide/02-04/index.html#32-select-schema-registry","title":"3.2. Select SCHEMA-REGISTRY","text":"<p>In Ranger, select the <code>SCHEMA-REGISTRY</code> repository that\u2019s associated with the stream messaging datahub.</p> <p>In this case, it will be [edu_ds_messaging_250204_schemaregistry]</p> <p></p>"},{"location":"guide/02-04/index.html#33-verify-user","title":"3.3. Verify user","text":"<p>Verify if the user who will be performing the exercise is present in the Policy: all - <code>schema-group, schema-metadata, schema-branch, schema-version</code>.</p> <p>A variable {USER} with all the required permissions in Apache Ranger has been added. This variable {USER} is used to represent each student in this environment in a Ranger Policy.</p> <p></p>"},{"location":"guide/02-04/index.html#34-click-on-edit","title":"3.4. Click on Edit","text":"<p>Click on the Edit button under the Actions column to edit the policy. </p> <p></p>"},{"location":"guide/02-04/index.html#35-review-policy-details","title":"3.5. Review Policy Details","text":"<p>Review the policy details. </p> <p></p>"},{"location":"guide/02-04/index.html#36-review-allow-conditions","title":"3.6. Review Allow Conditions","text":"<p>Verify if the user who will be performing the exercise is present in the Selected Users column. </p> <p>Review the policy permissions. </p> <p>Exit the Ranger UI by closing the tab. </p> <p></p>"},{"location":"guide/02-04/index.html#4-end-of-the-exercise","title":"4. End of the Exercise","text":""},{"location":"guide/02-05/index.html","title":"02-05 Create a Flow using Flow Designer","text":"<p>Creating a data flow for CDF-PC is the same process as creating any data flow within Nifi with 3 very important steps.</p> <p>(a) The data flow that would be used for CDF-PC must be self-contained within a process group.</p> <p>(b) Data flows for CDF-PC must use parameters for any property on a processor that is modifiable, e.g. user names, Kafka topics, etc.</p> <p>(c) All queues need to have meaningful names (instead of Success, Fail, and Retry). These names will be used to define Key Performance Indicators in CDF-PC.</p> <p>Let's build a data flow using flow designer.</p>"},{"location":"guide/02-05/index.html#1-create-canvas","title":"1. Create Canvas","text":""},{"location":"guide/02-05/index.html#we-will-start-by-creating-the-canvas-to-design-the-flow","title":"We will start by creating the canvas to design the flow","text":""},{"location":"guide/02-05/index.html#11-select-dataflow","title":"1.1. Select DataFlow","text":"<p>Access the <code>DataFlow</code> data service from the Management Console.</p> <p></p>"},{"location":"guide/02-05/index.html#12-click-on-flow-design","title":"1.2. Click on Flow Design","text":"<p>From the main navigation panel, select the Flow Design option. </p> <p></p>"},{"location":"guide/02-05/index.html#13-click-on-create-draft","title":"1.3. Click on Create Draft","text":"<p>Click on <code>Create Draft.</code> This will be the main process group for the flow that you\u2019ll create.</p> <p></p>"},{"location":"guide/02-05/index.html#14-name-the-draft","title":"1.4. Name the Draft","text":"<ul> <li> <p>Select the appropriate environment as part of the Target Workshop name (Ex: <code>devops-570-class-250204</code>).</p> </li> <li> <p>Let the Target Project be Unassigned.</p> </li> <li> <p>Give your flow a name with your username as prefix (Ex: <code>dse_2_250204_datadump_flow</code>).</p> </li> <li> <p>Click on Create button.</p> </li> </ul> <p></p>"},{"location":"guide/02-05/index.html#15-review-canvas","title":"1.5. Review Canvas","text":"<p>On successful creation of the Draft, you should now be redirected to the canvas on which you can design your flow.</p> <p></p>"},{"location":"guide/02-05/index.html#2-adding-new-parameters","title":"2. Adding new parameters","text":"<p>Configure Parameters: Parameters are reused within the flow multiple times and will also be configurable at the time of deployment. </p> <p>There are 2 options available: <code>Add Parameter</code>, which is used for specifying non-sensitive values and <code>Add Sensitive Parameter</code>, which is used for specifying sensitive parameters like password.</p>"},{"location":"guide/02-05/index.html#21-click-on-the-flow-options","title":"2.1. Click on the Flow Options","text":"<p>Click on the <code>Flow Options</code> on the top right corner of your canvas. and then select <code>Parameters</code>.</p> <p></p>"},{"location":"guide/02-05/index.html#22-click-on-add-parameter","title":"2.2. Click on Add Parameter","text":""},{"location":"guide/02-05/index.html#23-create-first-parameter","title":"2.3. Create first parameter","text":"<p>Create a variable with the following entries:</p> <ul> <li>Name: <code>S3 Directory</code>.</li> <li>Value: <code>LabData</code>.</li> </ul> <p>Click Save</p> <p></p>"},{"location":"guide/02-05/index.html#24-review-first-parameter","title":"2.4. Review first parameter","text":""},{"location":"guide/02-05/index.html#25-click-on-add-parameter","title":"2.5. Click on Add Parameter","text":""},{"location":"guide/02-05/index.html#26-create-second-parameter","title":"2.6. Create second parameter","text":"<p>Create a variable with the following entries:</p> <ul> <li>Name: <code>CDP Workload User</code>.</li> <li>Value: <code>The username assigned to you Ex: dse_2_250204</code>.</li> </ul> <p>Click Save</p> <p></p>"},{"location":"guide/02-05/index.html#27-review-second-parameter","title":"2.7. Review second parameter","text":""},{"location":"guide/02-05/index.html#28-click-on-add-sensitive-parameter","title":"2.8. Click on Add Sensitive Parameter","text":""},{"location":"guide/02-05/index.html#29-create-third-parameter","title":"2.9. Create third parameter","text":"<p>Create a variable with the following entries:</p> <ul> <li>Name: <code>CDP Workload User Password</code>.</li> <li>Value: <code>Workload User password set by you earlier in exercise 02-03 Define Workload Password</code>.</li> </ul> <p>Click Save</p> <p></p>"},{"location":"guide/02-05/index.html#210-review-third-parameter","title":"2.10. Review third parameter","text":""},{"location":"guide/02-05/index.html#211-click-apply-changes","title":"2.11. Click Apply Changes","text":""},{"location":"guide/02-05/index.html#212-click-ok","title":"2.12. Click OK","text":""},{"location":"guide/02-05/index.html#213-review-all-parameters","title":"2.13. Review all Parameters","text":"<p>All the parameters are applied and listed under the All Parameters tab.</p> <p>Now that we have created these parameters, we can easily search and reuse them within our dataflow. This is useful for <code>CDP Workload User</code> and <code>CDP Workload User Password</code>.</p> <p></p>"},{"location":"guide/02-05/index.html#3-create-the-flow","title":"3. Create the flow","text":"<p>Let\u2019s go back to the canvas to start designing our flow. This flow will contain 2 Processors:</p> <ul> <li><code>GenerateFlowFile</code>: Generates random data.</li> <li><code>PutCDPObjectStore</code>: Loads data into HDFS(S3).</li> </ul>"},{"location":"guide/02-05/index.html#31-navigate-to-flow-designer","title":"3.1. Navigate to Flow Designer","text":""},{"location":"guide/02-05/index.html#32-pull-the-processor","title":"3.2. Pull the Processor","text":"<p>Click on Processor and drag it onto the canvas</p> <p></p>"},{"location":"guide/02-05/index.html#33-add-generateflowfile-processor","title":"3.3. Add GenerateFlowFile processor","text":"<p>Type <code>GenerateFlowFile</code> in the text box, and once the processor appears click on Add.</p> <p></p>"},{"location":"guide/02-05/index.html#34-review-processor","title":"3.4. Review processor","text":"<p>The <code>GenerateFlowFile</code> Processor will now be on your canvas. </p> <p>Reposition the processor appropriately if required by dragging it over the canvas.</p> <p></p>"},{"location":"guide/02-05/index.html#35-configure-generateflowfile-processor","title":"3.5. Configure GenerateFlowFile processor","text":"<p>Right click on the processor and select Configuration. </p> <p></p>"},{"location":"guide/02-05/index.html#36-fill-in-the-values","title":"3.6. Fill in the values","text":"<p>Fill in the values in the right window pane to configure the processor in the following way.</p> <p><code>Processor Name</code>: <code>DataGenerator</code></p> <p><code>Scheduling Strategy</code>: <code>Timer Driven</code></p> <p></p>"},{"location":"guide/02-05/index.html#37-fill-in-the-values","title":"3.7. Fill in the values","text":"<p><code>Run Duration</code>: <code>0ms</code></p> <p><code>Run Schedule</code>: <code>1 min</code></p> <p><code>Execution</code>: <code>All Nodes</code></p> <p><code>Properties</code>: <code>Custom Text</code></p> <p></p>"},{"location":"guide/02-05/index.html#38-fill-in-the-values","title":"3.8. Fill in the values","text":"<pre><code>&lt;26&gt;1 2021-09-21T21:32:43.967Z host1.example.com application4 3064 ID42 [exampleSDID@873 iut=\"4\" eventSource=\"application\" eventId=\"58\"] application4 has\nstopped unexpectedly\n</code></pre> <p>The above represents a syslog out in RFC5424 format. Subsequent portions of these exercises under this module will leverage this same syslog format.</p> <p>Click Apply</p> <p></p>"},{"location":"guide/02-05/index.html#39-review-datagenerator-processor","title":"3.9. Review DataGenerator Processor","text":"<p>Review DataGenerator Processor on the canvas. </p> <p></p>"},{"location":"guide/02-05/index.html#310-pull-second-processor","title":"3.10. Pull second Processor","text":"<p>Pull a new <code>Processor</code> onto the canvas </p> <p></p>"},{"location":"guide/02-05/index.html#311-add-putcdpobjectstore-processor","title":"3.11. Add PutCDPObjectStore processor","text":"<p>Type <code>PutCDPObjectStore</code> in the text box, and once the processor appears click on <code>Add</code>.</p> <p></p>"},{"location":"guide/02-05/index.html#312-review-processor","title":"3.12. Review processor","text":"<p>The <code>PutCDPObjectStore</code> processor will now be on your canvas. </p> <p>Reposition the processor appropriately if required by dragging it over the canvas.</p> <p></p>"},{"location":"guide/02-05/index.html#313-configure-putcdpobjectstore-processor","title":"3.13. Configure PutCDPObjectStore processor","text":"<p>Right click on the processor and select Configuration. </p> <p>Fill in the values in the right window pane to configure the processor in the following way.</p> <p><code>Processor Name</code> : <code>Move2S3</code></p> <p><code>Scheduling Strategy</code> : <code>Timer Driven</code></p> <p><code>Run Duration</code> : <code>0ms</code></p> <p><code>Run Schedule</code> : <code>0 sec</code></p> <p><code>Execution</code> : <code>All Nodes</code></p> <p></p>"},{"location":"guide/02-05/index.html#314-fill-in-the-values","title":"3.14. Fill in the values","text":"<p>Click on No Value set for in front of Directory under Property.</p> <p></p>"},{"location":"guide/02-05/index.html#315-fill-directory","title":"3.15. Fill Directory","text":"<p><code>Directory</code> : #{S3 Directory}</p> <p></p>"},{"location":"guide/02-05/index.html#316-fill-properties","title":"3.16. Fill properties","text":"<p>Similarly, fill the below properties</p> <p><code>CDP Username</code> : #{CDP Workload User}</p> <p><code>CDP Password</code> : #{CDP Workload User Password}</p> <p><code>Relationships</code>: Check the <code>Terminate</code> box under <code>success</code>.</p> <p>Click on **Apply. **</p> <p></p>"},{"location":"guide/02-05/index.html#317-create-connection-between-processors","title":"3.17. Create connection between processors","text":"<p>Review <code>Move2S3</code> processor on the canvas. </p> <p>Connect the two processors by dragging the arrow from <code>DataGenerator</code> processor to the <code>Move2S3</code> processor. </p> <p></p>"},{"location":"guide/02-05/index.html#318-drag-the-arrow","title":"3.18. Drag the arrow","text":""},{"location":"guide/02-05/index.html#319-connect-arrow","title":"3.19. Connect arrow","text":""},{"location":"guide/02-05/index.html#320-select-success","title":"3.20. Select Success","text":"<p>Select on <code>success</code> relationship checkbox. </p> <p>Click Add.</p> <p></p>"},{"location":"guide/02-05/index.html#321-review-flow","title":"3.21. Review Flow","text":"<p>Your flow should look something like this. </p> <p></p>"},{"location":"guide/02-05/index.html#322-add-a-queue","title":"3.22. Add a queue","text":"<p>The <code>Move2S3</code> processor does not know what to do in case of a failure. Let\u2019s add a retry queue to it. This can be done by dragging the arrow on the <code>Move2S3</code> processor outwards then back to itself, as shown below.</p> <p></p>"},{"location":"guide/02-05/index.html#323-select-failure","title":"3.23. Select Failure","text":"<p>Select on <code>failure</code> relationship checkbox.</p> <p>Click Add.</p> <p></p>"},{"location":"guide/02-05/index.html#324-review-flow","title":"3.24. Review flow","text":""},{"location":"guide/02-05/index.html#4-renaming-the-queues","title":"4. Renaming the queues","text":"<p>Info</p> <p>Naming the queue: Providing unique names to all queues is very important as they are used to define Key Performance Indicators (KPI) upon which CDF-PC will auto scale. To name a queue, double-click the queue and give it a unique name. A best practice here is to start the existing queue name (i.e. success, failure, retry, etc\u2026) and add the source and destination processor information.</p>"},{"location":"guide/02-05/index.html#41-rename-success-queue","title":"4.1. Rename Success Queue","text":"<p>Click on Success Queue. In the Connection Name field, rename the queue as <code>success_Move2S3.\ufeff</code></p> <p>Click **Apply. **</p> <p></p>"},{"location":"guide/02-05/index.html#42-rename-failure-queue","title":"4.2. Rename Failure Queue","text":"<p>Click on failure Queue. In the Connection Name field, rename the queue as <code>failure_Move2S3.</code></p> <p>Click **Apply. **</p> <p></p>"},{"location":"guide/02-05/index.html#43-review-flow","title":"4.3. Review flow","text":""},{"location":"guide/02-05/index.html#5-end-of-the-exercise","title":"5. End of the Exercise","text":""},{"location":"guide/02-06/index.html","title":"02-06 Testing the flow","text":"<p>Testing the Data Flow: To test the flow we need to first start the test session.</p>"},{"location":"guide/02-06/index.html#1-testing-the-data-flow","title":"1. Testing the Data Flow","text":""},{"location":"guide/02-06/index.html#11-click-on-flow-options","title":"1.1. Click on Flow Options","text":"<p>Click on <code>Flow Options</code> on the top right corner and then click <code>Start</code> under <code>Test Session</code> section.</p> <p></p>"},{"location":"guide/02-06/index.html#12-click-start-test-session","title":"1.2. Click Start Test Session","text":"<p>In the next window, click <code>Start Test Session</code>.</p> <p></p>"},{"location":"guide/02-06/index.html#13-wait-a-couple-of-mins","title":"1.3. Wait a couple of mins","text":""},{"location":"guide/02-06/index.html#14-initializing-test-session","title":"1.4. Initializing Test Session","text":"<p>The activation should take about a couple of minutes. While this happens, you will see this at the top right corner of your screen.</p> <p></p>"},{"location":"guide/02-06/index.html#15-active-test-session","title":"1.5. Active Test Session","text":"<p>Once the Test Session is ready you will see the Active Test Session in green button. </p> <p></p>"},{"location":"guide/02-06/index.html#2-setup-s3-dir-access","title":"2. Setup S3 dir access","text":"<p>Setup the command line access for the S3 LadData directory. </p>"},{"location":"guide/02-06/index.html#21-click-on-menu","title":"2.1. Click on Menu","text":"<p>Select the Menu option by clicking on the 9 dots. </p> <p></p>"},{"location":"guide/02-06/index.html#22-select-management-console","title":"2.2. Select Management Console","text":"<p>Navigate to the Management Console page by clicking the Management Console tile.</p> <p></p>"},{"location":"guide/02-06/index.html#23-select-environment","title":"2.3. Select Environment","text":"<p>Click on the environment. </p> <p></p>"},{"location":"guide/02-06/index.html#24-select-gateway","title":"2.4. Select Gateway","text":"<p>Click on training gateway</p> <p></p>"},{"location":"guide/02-06/index.html#25-review-page","title":"2.5. Review page","text":"<p>Review the details for training gateway. </p> <p>Scroll down the page. </p> <p></p>"},{"location":"guide/02-06/index.html#26-click-on-nodes","title":"2.6. Click on Nodes","text":"<p>Click on the Nodes option in the left hand side pane. </p> <p></p>"},{"location":"guide/02-06/index.html#27-copy-public-ip","title":"2.7. Copy Public IP","text":"<p>Click on the Copy to Clipboard button next to Public IP. </p> <p></p>"},{"location":"guide/02-06/index.html#28-access-the-gateway","title":"2.8. Access the gateway","text":"<p>Open a terminal on your device and run the ssh command to access the gateway. </p> <pre><code>ssh username@&lt;training-gateway-public-up&gt;\n</code></pre> <p></p>"},{"location":"guide/02-06/index.html#29-enter-password","title":"2.9. Enter password","text":"<p>Enter password for your username. </p> <p></p>"},{"location":"guide/02-06/index.html#210-login-successful","title":"2.10. Login Successful","text":"<p>After the password, you should be able to successfully access the gateway. </p> <p></p>"},{"location":"guide/02-06/index.html#3-fetching-location","title":"3. Fetching location","text":""},{"location":"guide/02-06/index.html#31-navigate-to-training-gateway","title":"3.1. Navigate to training gateway","text":"<p>Navigate back to training gateway page. </p> <p>Click on Environment under Environment Details. </p> <p></p>"},{"location":"guide/02-06/index.html#32-click-on-summary","title":"3.2. Click on Summary","text":"<p>Scroll down on the Summary Tab</p> <p></p>"},{"location":"guide/02-06/index.html#33-copy-the-location","title":"3.3. Copy the location","text":"<p>Copy the location until datalake</p> <p>For example: s3a://cdp-storage-devops-570-class-250204</p> <p></p>"},{"location":"guide/02-06/index.html#34-list-files","title":"3.4. List files","text":"<p>Run the following command to list the files under your user. You should not see LabData directory just yet.</p> <pre><code>hdfs dfs -ls s3a://cdp-storage-devops-570-class-250204/user/dse_2_250204/\n</code></pre> <pre><code>Syntax: hdfs dfs -ls &lt;storage-location&gt;/user/&lt;username&gt;/\n</code></pre> <p></p>"},{"location":"guide/02-06/index.html#4-run-the-flow","title":"4. Run the flow","text":""},{"location":"guide/02-06/index.html#41-click-on-menu","title":"4.1. Click on Menu","text":"<p>Select the Menu option by clicking on the 9 dots. </p> <p></p>"},{"location":"guide/02-06/index.html#42-select-dataflow","title":"4.2. Select DataFlow","text":"<p>Navigate to the Cloudera DataFlow page by clicking the DataFlow tile in the Cloudera management console.</p> <p></p>"},{"location":"guide/02-06/index.html#43-select-flow-design","title":"4.3. Select Flow Design","text":"<p>Select the Flow Design option. </p> <p></p>"},{"location":"guide/02-06/index.html#44-select-draft","title":"4.4. Select Draft","text":"<p>Click on the Draft Name</p> <p></p>"},{"location":"guide/02-06/index.html#45-run-the-flow","title":"4.5. Run the flow","text":"<p>Run the flow by right clicking the <code>empty part</code> of the canvas and selecting <code>Start</code>.</p> <p></p>"},{"location":"guide/02-06/index.html#46-review-the-processor-state","title":"4.6. Review the processor state","text":"<p>Both the processors should now be in the <code>Start</code> state. This can be confirmed by looking at the green play button against each processor.</p> <p></p>"},{"location":"guide/02-06/index.html#5-verify-contents-under-labdata","title":"5. Verify contents under /LabData","text":""},{"location":"guide/02-06/index.html#51-list-the-files","title":"5.1. List the files","text":"<p>Run the same command as executed earlier to list the files under your user. You should now see LabData directory</p> <pre><code>hdfs dfs -ls s3a://cdp-storage-devops-570-class-250204/user/dse_2_250204/\n</code></pre> <pre><code>Syntax: hdfs dfs -ls &lt;storage-location&gt;/user/&lt;username&gt;/\n</code></pre> <pre><code>hdfs dfs -ls s3a://cdp-storage-devops-570-class-250204/user/dse_2_250204/LabData\n</code></pre> <pre><code>Syntax: hdfs dfs -ls &lt;storage-location&gt;/user/&lt;username&gt;/LabData/\n</code></pre> <p></p>"},{"location":"guide/02-06/index.html#6-end-of-the-exercise","title":"6. End of the Exercise","text":""},{"location":"guide/02-07/index.html","title":"02-07 Moving the flow to the flow catalog","text":"<p>After the flow has been created and tested, we can now Publish the flow to the Flow Catalog.</p>"},{"location":"guide/02-07/index.html#1-publish-the-flow","title":"1. Publish the Flow","text":""},{"location":"guide/02-07/index.html#11-stop-test-session","title":"1.1. Stop Test Session","text":"<p>Stop the current test session by clicking on the green tab on top right corner indicating <code>Active Test Session</code>. </p> <p></p>"},{"location":"guide/02-07/index.html#12-click-on-end","title":"1.2. Click on End.","text":""},{"location":"guide/02-07/index.html#13-review-session-status","title":"1.3. Review Session Status","text":"<p>Keep a check on the Test Session status as it turns red. </p> <p></p>"},{"location":"guide/02-07/index.html#14-publish-flow","title":"1.4. Publish Flow","text":"<p>Once the session stops click on <code>Flow Options</code> on the top right corner of your screen and click on <code>Publish</code>.</p> <p></p>"},{"location":"guide/02-07/index.html#15-name-the-flow","title":"1.5. Name the flow","text":"<p>Give your flow a unique name and click on <code>Publish</code>.</p> <p><code>Flow Name</code>: <code>{user_id}_datadump_flow</code> (Ex: <code>dse_2_250204_datadump_flow</code>).</p> <p></p>"},{"location":"guide/02-07/index.html#16-review-success-message","title":"1.6. Review Success Message","text":"<p>The flow will now be visible on the Flow <code>Catalog</code> and is ready to be deployed.</p> <p></p>"},{"location":"guide/02-07/index.html#2-deploying-the-flow","title":"2. Deploying the flow","text":""},{"location":"guide/02-07/index.html#21-click-on-menu","title":"2.1. Click on Menu","text":"<p>Select the Catalog option. </p> <p></p>"},{"location":"guide/02-07/index.html#22-search-for-the-flow-catalog","title":"2.2. Search for the Flow Catalog","text":"<p>Search for the <code>Flow Catalog</code> by typing the name of the flow that you just now published. </p> <p>Click on the flow. </p> <p></p>"},{"location":"guide/02-07/index.html#23-deploy-flow","title":"2.3. Deploy Flow","text":"<p>You should see the option to <code>Deploy</code>. Verify 'Version 1' and then click on <code>Deploy</code>. </p> <p></p>"},{"location":"guide/02-07/index.html#24-click-continue","title":"2.4. Click Continue","text":"<p>The target workspace should be pre-selected for you. </p> <p>Click Continue.</p> <p></p>"},{"location":"guide/02-07/index.html#25-name-the-deployment","title":"2.5. Name the Deployment","text":"<p>Give a unique name to the deployment.</p> <p>Deployment Name: <code>{user_id}_flow_prod</code></p> <p>(Ex: <code>dse_2_250204_flow_prod</code>).</p> <p>Click Next</p> <p></p>"},{"location":"guide/02-07/index.html#26-set-nifi-configuration","title":"2.6. Set Nifi Configuration","text":"<p>In this step we let everything be the default and click Next.</p> <p></p>"},{"location":"guide/02-07/index.html#27-set-the-parameters","title":"2.7. Set the Parameters","text":"<p>Set the parameters are following:</p> <ul> <li>CDP Workload User: <code>The username assigned to you</code>. </li> <li>Ex: <code>dse_2_250204</code>.</li> <li>CDP Workload User Password: <code>Workload User password set by you earlier in exercise 02-03 'Define Workload Password'</code>.</li> <li>S3 Directory: <code>LabData</code></li> </ul> <p></p>"},{"location":"guide/02-07/index.html#28-set-the-cluster-size","title":"2.8. Set the cluster size","text":"<p>Select the <code>Extra Small</code> size. In this step you can configure how your flow will auto scale, but keep it disabled for this lab. Select the standard storage selection. </p> <p>Click Next.</p> <p></p>"},{"location":"guide/02-07/index.html#29-add-key-performance-indicators","title":"2.9. Add Key Performance indicators","text":"<p>Set up KPIs to track specific performance metrics of a deployed flow.</p> <p>Click on Add New KPI.</p> <p></p>"},{"location":"guide/02-07/index.html#210-fill-in-the-details","title":"2.10. Fill in the details","text":"<p>In the <code>Add New KPI</code> window, fill in the details as below.</p> <ul> <li>KPI Scope: <code>Connection</code></li> <li>Connection Name: <code>failure_Move2S3</code></li> <li>Metric to Track: <code>Percent Full</code></li> <li>Check box against <code>Trigger alert when metric is greater than</code>: <code>50Percent</code>.</li> </ul> <p><code>Alert will be triggered when metric is outside the boundary(s) for</code>: <code>2Minutes</code>.</p> <p>Click Add.</p> <p></p>"},{"location":"guide/02-07/index.html#211-review-kpi","title":"2.11. Review KPI","text":"<p>Click Next. </p> <p></p>"},{"location":"guide/02-07/index.html#212-click-deploy","title":"2.12. Click Deploy","text":""},{"location":"guide/02-07/index.html#213-review-deployment-initiated-message","title":"2.13. Review Deployment Initiated message","text":"<p>The Deployment Initiated message will be displayed. Wait until the flow deployment is completed, which might take a few minutes. </p> <p></p>"},{"location":"guide/02-07/index.html#214-review-deployment","title":"2.14. Review Deployment","text":"<p>When deployed, the flow will show up on the Data flow dashboard, as below.</p> <p></p>"},{"location":"guide/02-07/index.html#215-list-the-files","title":"2.15. List the files","text":"<p>The data gets populated in the S3 bucket. </p> <p>Run the same command as executed earlier to list the files under your user. </p> <pre><code>hdfs dfs -ls s3a://cdp-storage-devops-570-class-250204/user/dse_2_250204/LabData\n</code></pre> <pre><code>Syntax: hdfs dfs -ls &lt;storage-location&gt;/user/&lt;username&gt;/LabData/\n</code></pre> <p></p>"},{"location":"guide/02-07/index.html#216-click-on-flow","title":"2.16. Click on Flow","text":"<p>Click in the empty area in the left hand side to take a look at the flow. </p> <p></p>"},{"location":"guide/02-07/index.html#217-review-flow","title":"2.17. Review Flow","text":"<p>After a while you will see the flow something like below for the flow you just deployed.</p> <p></p>"},{"location":"guide/02-07/index.html#3-verifying-flow-deployment","title":"3. Verifying flow deployment","text":""},{"location":"guide/02-07/index.html#31-select-manage-deployment","title":"3.1. Select Manage Deployment","text":"<p>Click on the flow in the Dashboard and select <code>Manage Deployment</code> under Actions button. </p> <p></p>"},{"location":"guide/02-07/index.html#32-review-deployment","title":"3.2. Review Deployment","text":""},{"location":"guide/02-07/index.html#33-review-kpi-and-alerts","title":"3.3. Review KPI and Alerts","text":"<p>Click on the <code>KPI and Alerts</code> tab under <code>Deployment Settings</code> to get the list of KPIs that have been set. You also have an option to modify or add more KPIs to your flow here.</p> <p></p>"},{"location":"guide/02-07/index.html#34-review-sizing-and-scaling","title":"3.4. Review Sizing and Scaling","text":"<p>Click on the <code>Sizing and Scaling</code> tab to get detailed information.</p> <p></p>"},{"location":"guide/02-07/index.html#35-review-parameters","title":"3.5. Review Parameters","text":"<p>The parameters that we earlier created can be managed from the Parameters tab. Click on <code>Parameters</code>.</p> <p></p>"},{"location":"guide/02-07/index.html#36-review-nifi-configurations","title":"3.6. Review NiFi Configurations","text":"<p>If you have set any configuration w.r.t to Nifi they will show up on the <code>NiFi Configuration</code> tab.</p> <p></p>"},{"location":"guide/02-07/index.html#37-view-in-nifi","title":"3.7. View in NiFi","text":"<p>Click on <code>Actions</code> and then click on <code>View in NiFi</code>. This will open the flow in the Nifi UI.</p> <p></p>"},{"location":"guide/02-07/index.html#38-review-flow-in-nifi-ui","title":"3.8. Review flow in Nifi UI","text":""},{"location":"guide/02-07/index.html#4-suspend-flow","title":"4. Suspend flow","text":"<p>We will now suspend this flow. </p>"},{"location":"guide/02-07/index.html#41-click-on-suspend-deployment","title":"4.1. Click on Suspend Deployment","text":"<p>Navigate back to the Manage Deployment page. Click on <code>Actions</code> and then <code>Suspend Deployment</code>.</p> <p></p>"},{"location":"guide/02-07/index.html#42-click-on-suspend-flow","title":"4.2. Click on Suspend Flow","text":"<p>Click on the verification <code>Suspend Flow</code>.</p> <p></p>"},{"location":"guide/02-07/index.html#43-observe-the-status","title":"4.3. Observe the status","text":"<p>Observe the change in the status of the flow.</p> <p></p>"},{"location":"guide/02-07/index.html#44-review-deployment-state","title":"4.4. Review Deployment State","text":"<p>After a few minutes, the status of the deployment has been changed to Suspended. </p> <p></p>"},{"location":"guide/02-07/index.html#5-end-of-the-exercise","title":"5. End of the Exercise","text":""},{"location":"guide/02-08/index.html","title":"02-08 Migrating Existing Data Flows to CDF-PC","text":"<p>Info</p> <p>The purpose of this exercise is to demonstrate how existing NiFi flows externally developed (e,g. on local laptops of developers, or pushed from a code repo) can be migrated to the Data Flow. This workshop will leverage an existing NiFi flow template that has been designed with the best practices for CDF-PC flow deployment.</p> <p>The existing NiFi Flow will perform the following actions. - Generate random syslogs in 5424 Format.</p> <ul> <li> <p>Convert the incoming data to a JSON using record writers.</p> </li> <li> <p>Apply a SQL filter to the JSON records.</p> </li> <li> <p>Send the transformed syslog messages to Kafka.</p> </li> </ul> <p>Note</p> <p>A parameter context has already been defined in the flow and the queues have been uniquely named.</p> <p>For this we will be leveraging the DataHubs which have already been created - <code>edu-ds-messaging-250204</code>, <code>edu-ds-analytics-250204.</code></p> <p>Note that the above names might be different depending upon your environment.</p>"},{"location":"guide/02-08/index.html#1-create-a-kafka-topic","title":"1. Create a Kafka Topic","text":""},{"location":"guide/02-08/index.html#11-click-on-menu","title":"1.1. Click on Menu","text":"<p>Select the Menu option by clicking on the 9 dots. </p> <p></p>"},{"location":"guide/02-08/index.html#12-select-management-console","title":"1.2. Select Management Console","text":"<p>Navigate to the Management Console page by clicking the Management Console tile.</p> <p></p>"},{"location":"guide/02-08/index.html#13-select-environment","title":"1.3. Select Environment","text":"<p>Click on the environment. </p> <p></p>"},{"location":"guide/02-08/index.html#14-select-messaging-data-hub","title":"1.4. Select Messaging Data Hub","text":"<p>Click on the Data Hub for Stream Messaging. For ex: edu-ds-messaging-250204</p> <p></p>"},{"location":"guide/02-08/index.html#15-note-hostname-of-master","title":"1.5. Note Hostname of master","text":"<p>Note the hostname of the master server in the Kafka Datahub. This will be used while created a new deployment later in this exercise. </p> <pre><code>edu-ds-messaging-250204-master0.devops-5.fc0b-8n9t.a6.cloudera.site\n</code></pre> <p></p>"},{"location":"guide/02-08/index.html#16-login-to-streams-messaging-manager","title":"1.6. Login to Streams Messaging Manager","text":"<p>Login to <code>Streams Messaging Manager</code> by clicking the appropriate hyperlink in the Streams Messaging Datahub</p> <p></p>"},{"location":"guide/02-08/index.html#17-click-on-topics","title":"1.7. Click on Topics","text":"<p>Click on <code>Topics</code> in the left tab.</p> <p></p>"},{"location":"guide/02-08/index.html#18-click-on-add-new","title":"1.8. Click on Add New.","text":""},{"location":"guide/02-08/index.html#19-create-a-topic","title":"1.9. Create a Topic","text":"<p>Create a Topic with the following parameters. </p> <ul> <li>Name: <code>&lt;username&gt;_syslog</code>. </li> <li>Ex: <code>wuser00_syslog</code>.</li> <li>Partitions: <code>1</code></li> <li>Availability: <code>MODERATE</code></li> <li>Cleanup Policy: <code>delete</code></li> </ul> <p>Note</p> <p>The Flow will not work if you set the Cleanup Policy to anything other than <code>Delete</code>. This is because we are not specifying keys when writing to Kafka</p> <p>Click Save.</p> <p></p>"},{"location":"guide/02-08/index.html#110-review-message","title":"1.10. Review Message","text":"<p>A pop-up message will appear confirming topic has been added.</p> <p></p>"},{"location":"guide/02-08/index.html#111-search-for-the-topic","title":"1.11. Search for the topic","text":"<p>You can search for the topic that you created now and look for it as shown here.</p> <p></p>"},{"location":"guide/02-08/index.html#2-obtain-the-kafka-broker-list","title":"2. Obtain the Kafka Broker List","text":"<p>We will require the broker list to configure our processors to connect to our Kafka brokers which allows consumers to connect and fetch messages by partition, topic or offset. This information can be found in the Datahub cluster associated to the Streams Messaging Manager. Later in the lab, we will need to have at hand the list of kafka brokers - already configured in this environment- so to be able to our dataflow to publish to our Kafka topics.</p>"},{"location":"guide/02-08/index.html#21-select-brokers","title":"2.1. Select Brokers","text":"<p>Select <code>Brokers</code> from the left tab.</p> <p></p>"},{"location":"guide/02-08/index.html#22-save-broker-list","title":"2.2. Save Broker List","text":"<p>Save the name of the broker list in a notepad.</p> <p>Example: </p> <ul> <li>edu-ds-messaging-250204-corebroker2.devops-5.fc0b-8n9t.a6.cloudera.site:9093</li> <li>edu-ds-messaging-250204-corebroker0.devops-5.fc0b-8n9t.a6.cloudera.site:9093</li> <li>edu-ds-messaging-250204-corebroker1.devops-5.fc0b-8n9t.a6.cloudera.site:9093</li> </ul> <p></p>"},{"location":"guide/02-08/index.html#3-create-a-schema-in-schema-registry","title":"3. Create a Schema in Schema Registry","text":"<p>You need to now work on <code>Schema Registry</code>. </p>"},{"location":"guide/02-08/index.html#31-login-to-schema-registry","title":"3.1. Login to Schema Registry","text":"<p>Navigate back to Data Hub for Stream Messaging. Login to <code>Schema Registry</code> by clicking the appropriate hyperlink in the Streams Messaging Datahub</p> <p></p>"},{"location":"guide/02-08/index.html#32-create-a-new-schema","title":"3.2. Create a new schema","text":"<p>Click on the <code>+</code> button on the top right to create a new schema.</p> <p></p>"},{"location":"guide/02-08/index.html#33-fill-in-the-details","title":"3.3. Fill in the details","text":"<p>Create a new schema with the following information.</p> <ul> <li>Name: <code>&lt;username&gt;_syslog</code></li> <li>Ex: <code>dse_2_250204_syslog</code></li> <li>Description: <code>syslog schema for dataflow workshop</code></li> <li>Type: <code>Avro schema provider</code></li> <li>Schema Group: <code>Kafka</code></li> <li>Compatibility: <code>Backward</code></li> <li>Evolve: <code>True</code></li> <li>Schema Text: Copy and paste the below schema text below into the <code>Schema Text</code> field.</li> </ul> <pre><code>{\n \"name\": \"syslog\",\n \"type\": \"record\",\n \"namespace\": \"com.cloudera\",\n \"fields\": [\n  {\n   \"name\": \"priority\",\n   \"type\": \"int\"\n  },\n  {\n   \"name\": \"severity\",\n   \"type\": \"int\"\n  },\n  {\n   \"name\": \"facility\",\n   \"type\": \"int\"\n  },\n  {\n   \"name\": \"version\",\n   \"type\": \"int\"\n  },\n  {\n   \"name\": \"timestamp\",\n   \"type\": \"long\"\n  },\n  {\n   \"name\": \"hostname\",\n   \"type\": \"string\"\n  },\n  {\n   \"name\": \"body\",\n   \"type\": \"string\"\n  },\n  {\n   \"name\": \"appName\",\n   \"type\": \"string\"\n  },\n  {\n   \"name\": \"procid\",\n   \"type\": \"string\"\n  },\n  {\n   \"name\": \"messageid\",\n   \"type\": \"string\"\n  },\n  {\n   \"name\": \"structuredData\",\n   \"type\": {\n    \"name\": \"structuredData\",\n    \"type\": \"record\",\n    \"fields\": [\n     {\n      \"name\": \"SDID\",\n      \"type\": {\n       \"name\": \"SDID\",\n       \"type\": \"record\",\n       \"fields\": [\n        {\n         \"name\": \"eventId\",\n         \"type\": \"string\"\n        },\n        {\n         \"name\": \"eventSource\",\n         \"type\": \"string\"\n        },\n        {\n         \"name\": \"iut\",\n         \"type\": \"string\"\n        }\n       ]\n      }\n     }\n    ]\n   }\n  }\n ]\n}\n</code></pre> <p>Note:</p> <p>The name of the Kafka Topic (Ex: <code>dse_2_250204_syslog</code>) you previously created and the Schema Name must be the same.</p> <p>Click Save.</p> <p></p>"},{"location":"guide/02-08/index.html#34-review-message","title":"3.4. Review Message","text":"<p>A pop-up message will appear confirming Schema has been added. </p> <p></p>"},{"location":"guide/02-08/index.html#4-operationalizing-externally-developed-data-flows-with-cdf-pc","title":"4. Operationalizing Externally Developed Data Flows with CDF-PC","text":"<p>Import the Flow into the CDF-PC Catalog</p>"},{"location":"guide/02-08/index.html#41-click-on-menu","title":"4.1. Click on Menu","text":"<p>Select the Menu option by clicking on the 9 dots. </p> <p></p>"},{"location":"guide/02-08/index.html#42-select-dataflow","title":"4.2. Select DataFlow","text":"<p>Navigate to the Cloudera DataFlow page by clicking the DataFlow tile in the Cloudera management console.</p> <p></p>"},{"location":"guide/02-08/index.html#43-select-catalog","title":"4.3. Select Catalog","text":"<p>Select the Catalog option. </p> <p></p>"},{"location":"guide/02-08/index.html#44-select-import-flow-definition","title":"4.4. Select Import Flow Definition","text":"<p>Select <code>Import Flow Definition</code> on the Top Right.</p> <p></p>"},{"location":"guide/02-08/index.html#45-fill-in-the-details","title":"4.5. Fill in the details","text":"<p>Add the following information.</p> <ul> <li><code>Flow Name</code>: _syslog_to_kafka. (Ex: <code>wuser00_syslog_to_kafka</code>) <li><code>Flow Description</code>: <code>Reads Syslog in RFC 5424 format, applies a SQL filter, transforms the data into JSON records, and publishes to Kafka.</code></li> <li><code>NiFi Flow Configuration</code>: syslog-to-kafka.json (From the resources downloaded earlier).</li> <li><code>Version Comments</code>: Initial Version.</li> <p></p>"},{"location":"guide/02-08/index.html#46-upload-file","title":"4.6. Upload File","text":"<p>Upload the syslog-to-kafka.json script downloaded earlier in step 2 of exercise 02-02 Accessing Environment. </p> <p>Click Import.</p> <p></p>"},{"location":"guide/02-08/index.html#47-review-message","title":"4.7. Review Message","text":"<p>A pop-up message will appear confirming flow definition has been imported successfully. </p> <p></p>"},{"location":"guide/02-08/index.html#5-deploy-the-flow-in-cdf-pc","title":"5. Deploy the Flow in CDF-PC","text":""},{"location":"guide/02-08/index.html#51-search-for-the-flow","title":"5.1. Search for the flow","text":"<p>Search for the flow in the Flow Catalog by typing the flow name that you created in the previous step.</p> <p></p>"},{"location":"guide/02-08/index.html#52-deploy-the-flow","title":"5.2. Deploy the Flow","text":"<p>Click on the Flow, you should see the following. You should see a <code>Deploy</code> Option appear shortly. Then click on <code>Deploy</code>.</p> <p></p>"},{"location":"guide/02-08/index.html#53-select-target-environment","title":"5.3. Select Target Environment","text":"<p>Select the CDP <code>Target Environment</code> (Ex: <code>emeaworkshop-environ</code>) where this flow will be deployed. </p> <p>Click Continue.</p> <p></p>"},{"location":"guide/02-08/index.html#54-name-the-deployment","title":"5.4. Name the Deployment","text":"<p>Give the deployment a unique name (Ex: <code>{user_id}_syslog_to_kafka</code>). </p> <p>In case the character limit exceeds, you may shorten the name as shown in the next step. </p> <p></p>"},{"location":"guide/02-08/index.html#55-name-the-deployment","title":"5.5. Name the Deployment","text":"<p>Removed extra '_' from the name so as to meet the character limit for the deployment name.</p> <p>Click Next.</p> <p></p>"},{"location":"guide/02-08/index.html#56-click-next","title":"5.6. Click Next","text":"<p>In the NiFi Configuration screen, click Next to take the default parameters.</p> <p></p>"},{"location":"guide/02-08/index.html#57-add-the-flow-parameters","title":"5.7. Add the Flow Parameters","text":"<p>Add the Flow Parameters as below.</p> <p>Note that you might have to navigate to multiple screens to fill it. Then click <code>Next</code>.</p> <ul> <li><code>CDP Workload User</code>: The workload username for the current user. (Ex: <code>dse_2_250204</code>)</li> <li><code>CDP Workload Password</code>: The workload password for the current user (This password was set by you earlier).</li> <li><code>Filter Rule</code>: <code>SELECT * FROM FLOWFILE</code>.</li> </ul> <p></p>"},{"location":"guide/02-08/index.html#58-add-the-flow-parameters","title":"5.8. Add the Flow Parameters","text":"<ul> <li><code>Kafka Broker Endpoint</code>: The list of Kafka Brokers previously noted, which is comma separated as shown below.</li> <li>Example: edu-ds-messaging-250204-corebroker2.devops-5.fc0b-8n9t.a6.cloudera.site:9093,edu-ds-messaging-250204-corebroker0.devops-5.fc0b-8n9t.a6.cloudera.site:9093,edu-ds-messaging-250204-corebroker1.devops-5.fc0b-8n9t.a6.cloudera.site:9093</li> <li><code>Kafka Destination Topic</code>: username_syslog (Ex: <code>dse_2_250204_syslog</code>)</li> <li><code>Kafka Producer ID</code>: nifi_dfx_p1</li> <li><code>Schema Name</code>: username-syslog (Ex: <code>dse_2_250204_syslog</code>)</li> <li><code>Schema Registry Hostname</code>: The hostname of the master server in the Kafka Datahub (You have noted the hostname earlier in Step 1.6 in this exercise).</li> <li>Example: edu-ds-messaging-250204-master0.devops-5.fc0b-8n9t.a6.cloudera.site</li> </ul> <p>Click Next.</p> <p></p>"},{"location":"guide/02-08/index.html#59-define-sizing-and-scaling","title":"5.9. Define sizing and scaling","text":"<p>On the next page, define sizing and scaling details. </p> <ul> <li><code>Size</code>: <code>Extra Small</code></li> <li><code>Auto Scaling</code>: <code>Enabled</code></li> <li><code>Min Nodes</code>: <code>1</code></li> <li><code>Max Nodes</code>: <code>3</code></li> </ul> <p>Click Next.</p> <p></p>"},{"location":"guide/02-08/index.html#510-skip-the-kpi","title":"5.10. Skip the KPI","text":"<p>Skip the KPI page by clicking Next</p> <p></p>"},{"location":"guide/02-08/index.html#511-review-deployment","title":"5.11. Review deployment","text":"<p>Review your deployment. Then Click Deploy.</p> <p></p>"},{"location":"guide/02-08/index.html#512-proceed-to-the-cdf-pc-dashboard","title":"5.12. Proceed to the CDF-PC Dashboard","text":"<p>Proceed to the CDF-PC Dashboard and wait for your flow deployment to complete. A Green Check Mark will appear once complete, which might take a few minutes.</p> <p></p>"},{"location":"guide/02-08/index.html#513-review-deployment","title":"5.13. Review Deployment","text":"<p>When deployed, the flow will show up on the Data flow dashboard, as below.</p> <p></p>"},{"location":"guide/02-08/index.html#514-view-system-metrics","title":"5.14. View System Metrics","text":"<p>Click on System Metrics tab to view system metrics</p> <p></p>"},{"location":"guide/02-08/index.html#6-end-of-the-exercise","title":"6. End of the Exercise","text":""},{"location":"guide/02-09/index.html","title":"02-09 Managing KeyTabs","text":"<p>To run queries on the <code>SQL Stream Builder</code> you need to have your KeyTab <code>unlocked</code>. This is mainly for <code>authentication</code> purposes. As the credential you are using is sometimes reused as part of other people doing the same lab it is possible that your KeyTab is <code>already unlocked</code>. We have shared the steps for both the scenarios.</p>"},{"location":"guide/02-09/index.html#1-unlock-your-keytab","title":"1. Unlock your KeyTab","text":""},{"location":"guide/02-09/index.html#11-click-on-menu","title":"1.1. Click on Menu","text":"<p>Select the Menu option by clicking on the 9 dots. </p> <p></p>"},{"location":"guide/02-09/index.html#12-select-management-console","title":"1.2. Select Management Console","text":"<p>Navigate to the Management Console page by clicking the Management Console tile.</p> <p></p>"},{"location":"guide/02-09/index.html#13-select-environment","title":"1.3. Select Environment","text":"<p>Click on the environment. </p> <p></p>"},{"location":"guide/02-09/index.html#14-select-analytics-data-hub","title":"1.4. Select Analytics Data Hub","text":"<p>Click on the Data Hub cluster for stream analytics. (Ex: edu-ds-analytics-250204) </p> <p></p>"},{"location":"guide/02-09/index.html#15-click-streaming-sql-console","title":"1.5. Click Streaming SQL Console.","text":"<p>Open the SSB UI by clicking on <code>Streaming SQL Console</code>.</p> <p></p>"},{"location":"guide/02-09/index.html#16-click-on-the-user-name","title":"1.6. Click on the User name","text":"<p>Click on the User name (Ex: <code>dse_2_250204</code>) at the bottom left of the screen and select <code>Manage Keytab</code>. Make sure you are logged in as the username that was assigned to you.</p> <p></p>"},{"location":"guide/02-09/index.html#17-enter-credentials","title":"1.7. Enter Credentials","text":"<p>Enter your Workload Username under <code>Principal Name *</code> and workload password that you had set earlier in the <code>Password *</code> field.</p> <p>Click on Unlock Keytab.</p> <p></p>"},{"location":"guide/02-09/index.html#18-close","title":"1.8. Close","text":"<p>A message appears confirming 'Success KeyTab has been unclocked'. </p> <p>Click on X to close the window. </p> <p></p>"},{"location":"guide/02-09/index.html#2-reset-your-keytab","title":"2. Reset your KeyTab","text":""},{"location":"guide/02-09/index.html#21-click-on-the-user-name","title":"2.1. Click on the User name","text":"<p>Click on the User name (Ex: <code>dse_2_250204</code>) at the bottom left of the screen and select <code>Manage Keytab</code>. Make sure you are logged in as the username that was assigned to you.</p> <p></p>"},{"location":"guide/02-09/index.html#22-verify-keyab","title":"2.2. Verify Keyab","text":"<p>If you get the following dialog box it means that your Keytab is already <code>UNLOCKED</code>. </p> <p>Hence, it would be necessary to reset here by locking it and unlocking it again using your newly set workload password.</p> <p>Click on Lock Keytab.</p> <p></p>"},{"location":"guide/02-09/index.html#23-review-message","title":"2.3. Review message","text":"<p>A success message appears confirming Keytab has been locked.</p> <p>Click on X to close the window. </p> <p></p>"},{"location":"guide/02-09/index.html#24-click-on-the-user-name","title":"2.4. Click on the User name","text":"<p>Click on the User name (Ex: <code>dse_2_250204</code>) at the bottom left of the screen and select <code>Manage Keytab</code>.</p> <p></p>"},{"location":"guide/02-09/index.html#25-enter-credentials","title":"2.5. Enter Credentials","text":"<p>Enter your Workload Username under <code>Principal Name *</code> and workload password that you had set earlier in the <code>Password *</code> field.</p> <p>Click on Unlock Keytab.</p> <p></p>"},{"location":"guide/02-09/index.html#26-close","title":"2.6. Close","text":"<p>A message appears confirming 'Success KeyTab has been unclocked'.</p> <p>Click on X to close the window. </p> <p></p>"},{"location":"guide/02-09/index.html#3-end-of-the-exercise","title":"3. End of the Exercise","text":""},{"location":"guide/02-10/index.html","title":"02-10 Working on SQL Stream Builder Project","text":"<p>Note</p> <p>The purpose of this workshop is to demonstrate streaming analytic capabilities using SQL Stream Builder. We will leverage the NiFi Flow deployed in CDF-PC from the previous step and demonstrate how to query live data and subsequently sink it to another location. The SQL query will leverage the existing syslog schema in Schema Registry.</p>"},{"location":"guide/02-10/index.html#1-create-a-sql-stream-builder-ssb-project","title":"1. Create a SQL Stream Builder (SSB) Project","text":""},{"location":"guide/02-10/index.html#11-click-on-menu","title":"1.1. Click on Menu","text":"<p>Select the Menu option by clicking on the 9 dots. </p> <p></p>"},{"location":"guide/02-10/index.html#12-select-management-console","title":"1.2. Select Management Console","text":"<p>Navigate to the Management Console page by clicking the Management Console tile.</p> <p></p>"},{"location":"guide/02-10/index.html#13-select-environment","title":"1.3. Select Environment","text":"<p>Click on the environment. </p> <p></p>"},{"location":"guide/02-10/index.html#14-select-analytics-data-hub","title":"1.4. Select Analytics Data Hub","text":"<p>Click on the Data Hub cluster for stream analytics. (Ex: edu-ds-analytics-250204) </p> <p></p>"},{"location":"guide/02-10/index.html#15-click-streaming-sql-console","title":"1.5. Click Streaming SQL Console.","text":"<p>Open the SSB UI by clicking on <code>Streaming SQL Console</code>.</p> <p></p>"},{"location":"guide/02-10/index.html#16-create-a-new-project","title":"1.6. Create a new project","text":"<p>Create a SQL Stream Builder (SSB) Project by clicking New Project.</p> <p></p>"},{"location":"guide/02-10/index.html#17-fill-in-the-details","title":"1.7. Fill in the details","text":"<p>Use the following details.</p> <ul> <li><code>Name</code>: <code>{user_id}_hol_data_services</code>.</li> <li>(Ex: <code>dse_2_250204_data_services</code>).</li> <li><code>Description</code>: SSB Project to analyze streaming data. </li> </ul> <p>Click Create.</p> <p></p>"},{"location":"guide/02-10/index.html#18-review-project","title":"1.8. Review project","text":"<p>A success message appears confirming 'Project has been successfully created'. </p> <p></p>"},{"location":"guide/02-10/index.html#19-switch-project","title":"1.9. Switch project","text":"<p>Switch to the created project (Ex: <code>dse_2_250204_hol_data_services</code>). </p> <p>Click on Switch.</p> <p></p>"},{"location":"guide/02-10/index.html#110-select-switch-project","title":"1.10. Select Switch Project","text":"<p>If pop up comes select <code>Switch Project</code>.</p> <p></p>"},{"location":"guide/02-10/index.html#111-review-project","title":"1.11. Review Project","text":"<p>The project page appears. </p> <p></p>"},{"location":"guide/02-10/index.html#2-create-kafka-data-store","title":"2. Create Kafka Data Store","text":""},{"location":"guide/02-10/index.html#21-select-data-sources","title":"2.1. Select Data Sources","text":"<p>Create Kafka Data Store by selecting <code>Data Sources</code> in the left pane.</p> <p></p>"},{"location":"guide/02-10/index.html#22-click-on-ellipsis","title":"2.2. Click on ellipsis","text":"<p>Click on the three-dotted icon next to <code>Kafka</code></p> <p></p>"},{"location":"guide/02-10/index.html#23-select-new-kafka-data-source","title":"2.3. Select New Kafka Data Source.","text":""},{"location":"guide/02-10/index.html#24-fill-in-the-details","title":"2.4. Fill in the details","text":"<p>Add the Flow Parameters as below.</p> <ul> <li><code>Name</code>: <code>{user-id}_cdp_kafka</code>.</li> <li>(Ex: <code>dse_2_250204_cdp_kafka</code>)</li> <li><code>Kafka Broker Endpoint</code>: The list of Kafka Brokers previously noted, which is comma separated as shown below.</li> <li>Example: edu-ds-messaging-250204-corebroker2.devops-5.fc0b-8n9t.a6.cloudera.site:9093,edu-ds-messaging-250204-corebroker0.devops-5.fc0b-8n9t.a6.cloudera.site:9093,edu-ds-messaging-250204-corebroker1.devops-5.fc0b-8n9t.a6.cloudera.site:9093</li> <li><code>Protocol</code>: <code>SASL/SSL</code></li> </ul> <p></p>"},{"location":"guide/02-10/index.html#25-fill-in-the-details","title":"2.5. Fill in the details","text":"<ul> <li><code>SASL Username</code>: <code>workload-username</code>.</li> <li>(Ex: dse_2_250204).</li> <li><code>SASL Mechanism</code>: <code>PLAIN</code>.</li> <li><code>SASL Password</code>: Workload User password set by you earlier in exercise 02-03 Define Workload Password.</li> </ul> <p>Click on Validate to test the connections.</p> <p></p>"},{"location":"guide/02-10/index.html#26-click-create","title":"2.6. Click Create","text":"<p>A message appears confirming Data Source is Valid. </p> <p>Click Create.</p> <p></p>"},{"location":"guide/02-10/index.html#27-review-message","title":"2.7. Review message","text":"<p>A success message appears confirming the data source has been saved. </p> <p></p>"},{"location":"guide/02-10/index.html#3-create-kafka-table","title":"3. Create Kafka Table","text":""},{"location":"guide/02-10/index.html#31-click-on-new-kafka-table","title":"3.1. Click on New Kafka Table","text":"<p>Create Kafka Table, by selecting <code>Virtual Tables</code> in the left pane by clicking on the three-dotted icon (ellipsis) next to it.</p>"},{"location":"guide/02-10/index.html#_1","title":"02-10 Working on SQL Stream Builder Project","text":"<p>Then click on New Kafka Table.</p> <p></p>"},{"location":"guide/02-10/index.html#32-configure-kafka-table","title":"3.2. Configure Kafka Table","text":"<p>Configure the Kafka Table using the details below.</p> <ul> <li><code>Table Name</code>: {user-id}_syslog_data.</li> <li>(Ex: <code>dse_2_250204_syslog_data</code>)</li> <li><code>Kafka Cluster</code>: <code>select the Kafka data source you created previously</code>.</li> <li>(Ex: <code>dse_2_250204_cdp_kafka</code>)</li> <li><code>Data Format</code>: <code>JSON</code>.</li> <li><code>Topic Name</code>: <code>select the topic created in Schema Registry</code>.</li> </ul> <p>When you select Data Format as AVRO, you must provide the correct Schema Definition when creating the table for SSB to be able to successfully process the topic data. For JSON tables, though, SSB can look at the data flowing through the topic and try to infer the schema automatically, which is quite handy at times. Obviously, there must be data in the topic already for this feature to work correctly.</p> <p>Note</p> <p>SSB tries its best to infer the schema correctly, but this is not always possible and sometimes data types are inferred incorrectly. You should always review the inferred schemas to check if it\u2019s correctly inferred and make the necessary adjustments.</p> <p>Since you are reading data from a JSON topic, go ahead and click on <code>Detect Schema</code> to get the schema inferred. You should see the schema be updated in the <code>Schema Definition</code> tab.</p> <p>Click on **Detect Schema. **</p> <p></p>"},{"location":"guide/02-10/index.html#33-review-message","title":"3.3. Review message","text":"<p>Click OK to close the window.</p> <p></p>"},{"location":"guide/02-10/index.html#34-schema-is-invalid","title":"3.4. Schema is invalid","text":"<p>You will also notice that a \"Schema is invalid\" message appears upon the schema detection.</p> <p></p>"},{"location":"guide/02-10/index.html#35-review-info","title":"3.5. Review info","text":"<p>If you hover the mouse over the message, it shows the reason.</p> <p>We will fix this in the next step.</p> <p></p>"},{"location":"guide/02-10/index.html#36-enter-properties","title":"3.6. Enter Properties","text":"<p>Info</p> <p>Each record read from Kafka by SSB has an associated timestamp column of data type TIMESTAMP ROWTIME. By default, this timestamp is sourced from the internal timestamp of the Kafka message and is exposed through a column called eventTimestamp. However, if your message payload already contains a timestamp associated with the event (event time), you may want to use that instead of the Kafka internal timestamp.</p> <p>In this case, the syslog message has a field called <code>timestamp</code> that contains the timestamp you should use. You want to expose this field as the table\u2019s <code>event_time</code> column. To do this, click on the Event Time tab and enter the following properties.</p> <ul> <li><code>Use Kafka Timestamps</code>: <code>Disable</code>.</li> <li><code>Input Timestamp Column</code>: <code>timestamp</code>.</li> <li><code>Event Time Column</code>: <code>event_time</code>.</li> <li><code>Watermark Seconds</code>: <code>3</code>.</li> </ul> <p>Now that you have configured the event time column, click on Detect Schema again. </p> <p></p>"},{"location":"guide/02-10/index.html#37-review-message","title":"3.7. Review message","text":"<p>Click OK to close the window.</p> <p></p>"},{"location":"guide/02-10/index.html#38-schema-is-valid","title":"3.8. Schema is valid","text":"<p>You should see the schema turn valid.</p> <p></p>"},{"location":"guide/02-10/index.html#39-create-table","title":"3.9. Create Table","text":"<p>Click the Create and Review button to create the table.</p> <p></p>"},{"location":"guide/02-10/index.html#310-review-the-tables-ddl","title":"3.10. Review the table\u2019s DDL","text":"<p>A success message appears confirming table has been created. </p> <p>Review the table\u2019s DDL and click Close. </p> <p></p>"},{"location":"guide/02-10/index.html#4-create-a-flink-job","title":"4. Create a Flink Job","text":""},{"location":"guide/02-10/index.html#41-click-on-new-job","title":"4.1. Click on New Job","text":"<p>Create a Flink Job, by selecting</p> <p><code>Jobs</code> in the left pane, clicking on the three-dotted icon (ellipsis) next to it. </p> <p>Then click on New Job.</p> <p></p>"},{"location":"guide/02-10/index.html#42-name-the-job","title":"4.2. Name the Job","text":"<p>Give a unique job name (Ex: <code>dse_2_250204_flink_job</code>) </p> <p>Click Create.</p> <p></p>"},{"location":"guide/02-10/index.html#43-review-job","title":"4.3. Review job","text":"<p>A success message appears confirming 'Job has been created'. </p> <p></p>"},{"location":"guide/02-10/index.html#44-execute-select-query","title":"4.4. Execute SELECT query","text":"<p>Add the following SQL Statement in the Editor.</p> <pre><code>SELECT * FROM {user-id}_syslog_data WHERE severity &lt;=3\n</code></pre> <p>Replace user-id with your username.</p> <p>For ex: SELECT * FROM dse_2_250204_syslog_data WHERE severity &lt;=3</p> <p>Run the Streaming SQL Job by clicking Execute.</p> <p></p>"},{"location":"guide/02-10/index.html#45-view-output","title":"4.5. View output","text":"<p>Give. it a couple of minutes for the query to run. </p> <p></p>"},{"location":"guide/02-10/index.html#46-review-message","title":"4.6. Review message","text":"<p>A success message appears confirming 'Job has been started'. </p> <p></p>"},{"location":"guide/02-10/index.html#47-review-output","title":"4.7. Review Output","text":"<p>In the <code>Results</code> tab, you should see syslog messages with severity levels \u21d03.</p> <p></p>"},{"location":"guide/02-10/index.html#5-end-of-the-exercise","title":"5. End of the Exercise","text":""},{"location":"guide/03-01/index.html","title":"03-01 Design Principles for Kubernetes Clusters","text":""},{"location":"guide/03-01/index.html#1-introduction","title":"1. Introduction","text":""},{"location":"guide/03-01/index.html#2-the-importance-of-microservices","title":"2. The Importance of Microservices","text":""},{"location":"guide/03-01/index.html#3-orchestration-for-containerized-services","title":"3. Orchestration for Containerized Services","text":""},{"location":"guide/03-01/index.html#4-design-principles-of-kubernetes","title":"4. Design Principles of Kubernetes","text":""},{"location":"guide/03-01/index.html#5-open-container-initiave","title":"5. Open Container Initiave","text":""},{"location":"guide/03-01/index.html#6-kubernetes-documentation","title":"6. Kubernetes Documentation","text":""},{"location":"guide/03-01/index.html#7-summary-and-exercise-assignments","title":"7. Summary and Exercise Assignments","text":""},{"location":"guide/03-02/index.html","title":"03-02 Reviewing Kubernetes Documentation","text":"<p>The purpose of this exercise is to introduce Kubernetes documentation. </p>"},{"location":"guide/03-02/index.html#1-click-planet-scale","title":"1. Click \"Planet Scale\"","text":"<p>Open your browser and enter this URL.</p> <pre><code>https://kubernetes.io\n</code></pre> <p></p>"},{"location":"guide/03-02/index.html#2-click-kubernetes","title":"2. Click \"Kubernetes\"","text":"<p>Scroll down and review Kubernetes Features. This is a helpful overview of the primary capabilities of Kubernetes.</p> <p></p>"},{"location":"guide/03-02/index.html#3-open-kubernetes-documentation","title":"3. Open Kubernetes Documentation","text":"<p>Click on Documentation or enter this URL.</p> <pre><code>https://kubernetes.io/docs/home/\n</code></pre> <p>\u2139\ufe0f Bookmark</p> <p>This would be a good URL to bookmark</p> <p></p>"},{"location":"guide/03-02/index.html#4-click-kubernetes-documentation-kubernetes","title":"4. Click \"Kubernetes Documentation | Kubernetes\"","text":"<p>Kubernetes documentation is of high quality. The documentation is divided into:</p> <ul> <li>Documentation by version</li> <li>Getting Started</li> <li>Concepts</li> <li>Tasks</li> <li>tutorials</li> <li>Reference</li> </ul> <p>Open each of the arrow heads and review the subjects.</p> <p></p>"},{"location":"guide/03-02/index.html#5-click-kubernetes-documentation-kubernetes","title":"5. Click \"Kubernetes Documentation | Kubernetes\"","text":"<p>Click open the arrowhead for Tutorials. This class covers all of these subjects. Explore the tutorials subjects.</p> <p></p>"},{"location":"guide/03-02/index.html#6-click-search-results-kubernetes","title":"6. Click \"Search Results | Kubernetes\"","text":"<p>The Kubernetes is well indexed. Every page can be quickly found with a search.</p> <p>Enter Pods in the search.</p> <p></p>"},{"location":"guide/03-02/index.html#7-click-pods-kubernetes","title":"7. Click \"Pods | Kubernetes\"","text":"<p>Click on Pods | Kubernetes</p> <pre><code>https://kubernetes.io/docs/concepts/workloads/pods/\n</code></pre> <p>Kubernetes documentation is extensive. The Table of Contents in the right hand column is helpful in understanding each subject. The Table of Contents are logical and generally present a subject beginning with an explaination.</p> <p></p>"},{"location":"guide/03-02/index.html#8-click-desktop","title":"8. Click \"desktop\"","text":"<p>Scroll down the page to find Using Pods. Kubernetes presents many example command lines. Kubernetes also includes model code blocks in YAML format. There is a copy and paste icon in the upper right corner of the code block. It is common practice to copy in a code block into your IDE and to then edit it in your IDE.</p> <p></p>"},{"location":"guide/03-02/index.html#9-subjects-to-review","title":"9. Subjects to Review","text":"<p>A list of recommended subjects for those who are new to Kubernetes includes:</p> <ul> <li>Pods</li> <li>Service</li> <li>ConfigMaps</li> <li>Secrets</li> </ul> <p>Take a few minutes and look these subjects up. Scan through them. Watch for the code blocks as these will be helpful in writing manifest files.</p> <p></p>"},{"location":"guide/03-02/index.html#10-end-of-the-exercise","title":"10. End of the Exercise","text":""},{"location":"guide/03-03/index.html","title":"03-03 Building Your First CDE Airflow DAG","text":"<p>We will start with a simple Airflow DAG that executes a Spark CDE Job and a shell command.</p> <p>We will work with the following artifacts:</p> <ul> <li>A python file containing Airflow DAG. This is provided under cde_jobs/firstdag.py.</li> <li>A pythong file containing a PySpark job. This is provided under cde_jobs/sql.py.</li> </ul>"},{"location":"guide/03-03/index.html#1-create-a-spark-cde-job","title":"1. Create a Spark CDE Job","text":"<p>Let's create the first CDE Job. </p>"},{"location":"guide/03-03/index.html#11-click-on-administration","title":"1.1. Click on Administration","text":"<p>Navigate to the main menu and select Administration option from the panel. </p> <p></p>"},{"location":"guide/03-03/index.html#12-review-the-environment","title":"1.2. Review the environment","text":"<p>A CDE service [CDE-251701] and a virtual cluster [VC-251701]has been created for you. </p> <p>In your case, the name would reflect your class ID [CDE-(class ID), VC-(class ID)]. Please make a note of the environment names. </p> <p></p>"},{"location":"guide/03-03/index.html#13-click-on-view-jobs","title":"1.3. Click on View Jobs","text":"<p>Click on the View Jobs button in the Virtual Cluster assigned to you. </p> <p></p>"},{"location":"guide/03-03/index.html#14-click-on-create-jobs","title":"1.4. Click on Create Jobs","text":"<p>Let's create our first job. Click on Create Job button. </p> <p></p>"},{"location":"guide/03-03/index.html#15-fill-in-the-details","title":"1.5. Fill in the details","text":"<p>A Create job page opens. In the Create Job page, select the Job Type as Spark 3.2.3.</p> <p>Name the job as sparksql_YourCDPUsername.</p> <p>Attention</p> <p>Replace username with your actual username.</p> <p>For example: sparksql_dse_1_250204</p> <p>Select File option under Application File.</p> <p></p>"},{"location":"guide/03-03/index.html#16-click-on-upload","title":"1.6. Click on Upload","text":"<p>Click on the Upload button to furnish the script</p> <p></p>"},{"location":"guide/03-03/index.html#17-browse-sqlpy","title":"1.7. Browse sql.py","text":"<p>From the cde_job zip file downloaded in the previous exercise, browse and upload sql.pyscript as your Spark job file.</p> <p></p>"},{"location":"guide/03-03/index.html#18-create-a-resource","title":"1.8. Create a Resource","text":"<p>You will be prompted to select a resource. Create a new resource with name firstdag_YourCDPUsername.</p> <p>Attention</p> <p>Replace username with your actual username.</p> <p>For example: firstdag_dse_1_250204</p> <p>Resources are repositories inside the Virtual Cluster where you can store files, dependencies, and manage python environments.</p> <p>For more info on resources, please visit the CDE Documentation.</p> <p>Click on Upload</p> <p></p>"},{"location":"guide/03-03/index.html#19-select-create","title":"1.9. Select Create","text":"<p>At the bottom of the form, make sure to select \"Create\" rather than \"Create and Run\" by clicking on the down arrow first.</p> <p></p>"},{"location":"guide/03-03/index.html#110-review-message","title":"1.10. Review message","text":"<p>A success message appears confirming job has been created. </p> <p></p>"},{"location":"guide/03-03/index.html#111-review-job","title":"1.11. Review Job","text":"<p>The Spark CDE Job is now available in the Jobs UI.</p> <p></p>"},{"location":"guide/03-03/index.html#2-review-edit-the-airflow-dag","title":"2. Review &amp; Edit the Airflow DAG","text":"<p>Let's go over the code.</p>"},{"location":"guide/03-03/index.html#21-open-firstdagpy-file","title":"2.1. Open firstdag.py file","text":"<p>Open the firstdag.py file located in the cde_jobs folder downloaded in the previous exercises.</p> <p></p>"},{"location":"guide/03-03/index.html#22-import-python-modules","title":"2.2. Import Python modules","text":"<p>Between lines 2 and 7 we import the Python modules needed for the DAG. </p> <p>Notice that at line 6 and 7 we import the CDEJobRunOperator and BashOperator. The CDEJobRunOperator was created by Cloudera to support Spark CDE Job. The BashOperator is used to perform shell actions inside an Airflow DAG.</p> <pre><code>from datetime import datetime, timedelta\nfrom dateutil import parser\nimport pendulum\nfrom airflow import DAG\nfrom cloudera.cdp.airflow.operators.cde_operator import CDEJobRunOperator\nfrom airflow.operators.bash import BashOperator\n</code></pre> <p></p>"},{"location":"guide/03-03/index.html#23-declare-dag-and-arguments","title":"2.3. Declare DAG and arguments","text":"<p>Between lines 8 and 22 we declare the DAG and its arguments.</p> <p>The arguments dictionary includes options for scheduling, setting dependencies, and general execution. For a comprehensive list of DAG arguments please consult this page in the documentation.</p> <p>Once the arguments dictionary is complete it is passed as an argument to the DAG object instance.</p> <pre><code>default_args = {\n        'owner': 'YourCDPUsername',\n        'retry_delay': timedelta(seconds=5),\n        'depends_on_past': False,\n        'start_date': pendulum.datetime(2020, 1, 1, tz=\"Europe/Amsterdam\")\n        }\n\nfirstdag = DAG(\n        'airflow-pipeline-demo',\n        default_args=default_args,\n        schedule_interval='@daily',\n        catchup=False,\n        is_paused_upon_creation=False\n        )\n\nspark_step = CDEJobRunOperator(\n        task_id='sql_job_new',\n        dag=firstdag,\n    job_name='sparksql_YourCDPUsername'\n        )\n</code></pre> <p>\u26a0\ufe0f Replace Owner and Job Name</p> <p>Before moving on, make sure to open the DAG python file in your editor and replace the current value of the 'Owner' with your CDP Username in the default arguments dictionary. </p> <p>Also, replace the current value of the 'job_name' with sparksql_YourCDPUsername, set earlier in this exercise, in the spark_step object. </p> <p>No other changes are required at this time.</p> <p></p>"},{"location":"guide/03-03/index.html#24-declare-object","title":"2.4. Declare object","text":"<p>Between lines 24 and 28 an instance of the CDEJobRunOperator obect is declared with the following arguments:</p> <ul> <li>Task ID: This is the name used by the Airflow UI to recognize the node in the DAG.</li> <li>DAG: This has to be the name of the DAG object instance declared at line 16.</li> <li>Job Name: This has to be the name of the Spark CDE Job created in step 1 above.</li> </ul> <pre><code>spark_step = CDEJobRunOperator(\n    task_id='sql_job_new',\n    dag=firstdag,\n    job_name='sparksql_dse_1_250204'\n    )\n</code></pre> <p></p>"},{"location":"guide/03-03/index.html#25-declare-object","title":"2.5. Declare object","text":"<p>Between lines 30 and 34 we declare an instance of the BashOperator object with the following arguments:</p> <ul> <li>Task ID: as above, you can pick an arbitrary string value</li> <li>DAG: This has to be the name of the DAG object instance declared at line 16.</li> <li>Bash Command: the actual shell commands you want to execute.</li> </ul> <p>Notice that this is just a simple example. You can optionally add more complex syntax with Jinja templating, use DAG variables, or even trigger shell scripts. For more please visit the Airflow Bash Operator documentation.</p> <pre><code>shell = BashOperator(\n    task_id='bash',\n    dag=firstdag,\n    bash_command='echo \"Hello Airflow\" '\n    )\n</code></pre> <p></p>"},{"location":"guide/03-03/index.html#26-declare-task-dependencies","title":"2.6. Declare Task Dependencies","text":"<p>Finally, at line 36 we declare Task Dependencies. With this statement you can specify the execution sequence of DAG tasks.</p> <pre><code>spark_step &gt;&gt; shell \n</code></pre> <p>Important</p> <p>Before moving on, make sure to open the DAG python file in your editor and replace the current value of the 'Owner' with your CDP Username in the default arguments dictionary. Also, replace the current value of the 'job_name' with sparksql_YourCDPUsername, set earlier in this exercise, in the spark_step object. No other changes are required at this time.</p> <p></p>"},{"location":"guide/03-03/index.html#3-running-your-first-cde-airflow-dag","title":"3. Running Your First CDE Airflow DAG","text":""},{"location":"guide/03-03/index.html#31-click-on-create-job","title":"3.1. Click on Create Job","text":"<p>Click on Create Job to create a new CDE Job.</p> <p></p>"},{"location":"guide/03-03/index.html#32-fill-in-the-details","title":"3.2. Fill in the details","text":"<ul> <li>Job Type: Airflow</li> <li>Name: FirstDag_YourCDPUsername</li> <li>Dag File: File</li> </ul> <p>Click on Upload</p> <p></p>"},{"location":"guide/03-03/index.html#33-upload-file-to-a-resource","title":"3.3. Upload file to a resource","text":"<p>Upload firstdag.py to the firstdag CDE Resource you created earlier.</p> <p>Ensure that you have replaced the current value of the 'Owner' with your CDP Username in the default arguments dictionary. Please refer to Step 2.3 above in this exercise for details. </p> <p>Click on Upload.</p> <p></p>"},{"location":"guide/03-03/index.html#34-select-create-and-run","title":"3.4. Select 'Create and Run'","text":"<p>Select Create and Run button to trigger the job immediately.</p> <p></p>"},{"location":"guide/03-03/index.html#35-job-run-in-progress","title":"3.5. Job run in progress","text":"<p>Please wait for a min for job to run</p> <p></p>"},{"location":"guide/03-03/index.html#36-new-run-is-initiated","title":"3.6. New run is initiated","text":"<p>A success message appears confirming that a new run with ID has been initiated. </p> <p></p>"},{"location":"guide/03-03/index.html#37-review-the-jobs-in-progress","title":"3.7. Review the Jobs in progress","text":"<p>Navigate to the CDE Jobs Run page and notice that two CDE Jobs are now in progress. One of them is \"sparksql_dse_1_250204\" (Spark CDE Job) and the other is \"FirstDag_dse_1_250204\" (Airflow CDE Job). The former has been triggered by the execution of the latter. Wait a few moments and allow for the DAG to complete.</p> <p></p>"},{"location":"guide/03-03/index.html#38-click-on-the-firstdag-link","title":"3.8. Click on the FirstDag link","text":"<p>Once the Status column shows the Jobs run as succeeded, click on the \"FirstDag_dse_1_250204\" link to access the Job Run page.</p> <p></p>"},{"location":"guide/03-03/index.html#39-select-the-most-recent-run","title":"3.9. Select the most recent Run","text":"<p>This page shows each run along with associated logs, execution statistics, and the Airflow UI.</p> <p>Ensure to select the most recent Run (in the screenshot, the most recent Run ID number is 2)</p> <p></p>"},{"location":"guide/03-03/index.html#310-review-job-run","title":"3.10. Review Job Run","text":"<p>Review the Job Run. Notice the run id in the top section of the page. </p> <p>Click on the Airflow UI tab.</p> <p></p>"},{"location":"guide/03-03/index.html#311-review-airflow-ui-tab","title":"3.11. Review Airflow UI tab","text":"<p>The first landing page lists all tasks along with their status. Notice that the DAG ID, Task ID and Operator columns are populated with the values set in the DAG python files. </p> <p>Next, click on the back arrow on the left side of the screen to navigate to the Airflow UI DAGs view.</p> <p></p>"},{"location":"guide/03-03/index.html#312-explore-dags-view","title":"3.12. Explore DAGs view.","text":"<p>From the DAGs view you can:</p> <ul> <li>Pause/unpause a DAG</li> <li>Filter the list of DAGs to show active, paused, or all DAGs</li> <li>Trigger, refresh, or delete a DAG</li> <li>Navigate quickly to other DAG-specific pages</li> </ul> <p>The DAG name specified in the python file during DAG declaration is airflow-pipeline-demo. </p> <p>Click on it to open the Airflow UI DAG view to drill down with DAG-specific pages.</p> <p></p>"},{"location":"guide/03-03/index.html#313-review-dag","title":"3.13. Review DAG","text":"<p>Here, Airflow provides a number of tabs to increase job observability. Below is a brief explanation of the most important ones.</p>"},{"location":"guide/03-03/index.html#the-tree-view","title":"The Tree View","text":"<p>The Tree View tracks DAG tasks across time. Each column represents a DAG Run and each square is a task instance in that DAG Run. Task instances are color-coded depending on success of failure. DAG Runs with a black border represent scheduled runs while DAG Runs with no border are manually triggered.</p>"},{"location":"guide/03-03/index.html#the-graph-view","title":"The Graph View","text":"<p>The Graph View shows a simpler diagram of DAG tasks and their dependencies for the selected run. You can enable auto-refresh the view to see the status of the tasks update in real time.</p>"},{"location":"guide/03-03/index.html#the-calendar-view","title":"The Calendar View","text":"<p>The Calendar View shows the state of DAG Runs overlaid on a calendar.</p>"},{"location":"guide/03-03/index.html#the-code-view","title":"The Code View","text":"<p>The Code View shows the code that is used to generate the DAG. This is the DAG python file we used earlier.</p> <p></p>"},{"location":"guide/03-03/index.html#4-end-of-the-exercise","title":"4. End of the Exercise","text":""},{"location":"guide/03-04/index.html","title":"03-04 Beyond Airflow for Spark Jobs","text":""},{"location":"guide/03-04/index.html#using-the-cdwrunoperator","title":"Using the CDWRunOperator","text":"<p>Info</p> <p>The CDWRunOperator was contributed by Cloudera in order to orchestrate CDW queries with Airflow.</p>"},{"location":"guide/03-04/index.html#1-setup-cdw","title":"1. Setup CDW","text":"<p>Before we can use it in the DAG we need to connect Airflow to CDW. To complete these steps, you must have access to a CDW virtual warehouse. CDE currently supports CDW operations for ETL workloads in Apache Hive virtual warehouses. To determine the CDW hostname to use for the connection:</p>"},{"location":"guide/03-04/index.html#11-click-on-menu","title":"1.1. Click on Menu","text":"<p>Select the Menu option by clicking on the 9 dots. </p> <p></p>"},{"location":"guide/03-04/index.html#12-select-data-warehouse","title":"1.2. Select Data Warehouse","text":"<p>Navigate to the Cloudera Data Warehouse Overview page by clicking the Data Warehouse tile in the Cloudera management console.</p> <p></p>"},{"location":"guide/03-04/index.html#13-view-the-environment","title":"1.3. View the environment","text":"<p>In the Virtual Warehouses column, find the warehouse you want to connect to. In our case, it would be [vw-hive-{class-ID}].</p> <p></p>"},{"location":"guide/03-04/index.html#14-notice-the-status","title":"1.4. Notice the status","text":"<p>Virtual Warehouse has been configured to auto-suspend every 300 seconds and it will probably go in the stopped state. The stopped status of virtual warehouse will not affect the exercise. </p> <p></p>"},{"location":"guide/03-04/index.html#15-copy-jdbc-url","title":"1.5. Copy JDBC URL","text":"<p>Click the three-dot menu for the selected warehouse, and then click Copy JDBC URL.</p> <p></p>"},{"location":"guide/03-04/index.html#16-note-the-hostname","title":"1.6. Note the hostname","text":"<p>Paste the URL into a text editor, and make note of the hostname.</p> <p>For example, starting with the following URL the hostname is shown below:</p> <pre><code>Original URL: jdbc:hive2://hs2-vw-hive-250204.dw-devops-570-class-250204.fc0b-8n9t.cloudera.site/default;transportMode=http;httpPath=cliservice;socketTimeout=60;ssl=true;retries=3;\n\nHostname: hs2-vw-hive-250204.dw-devops-570-class-250204.fc0b-8n9t.cloudera.site/\n</code></pre> <p></p>"},{"location":"guide/03-04/index.html#2-setup-cde","title":"2. Setup CDE","text":""},{"location":"guide/03-04/index.html#21-click-on-menu","title":"2.1. Click on Menu","text":"<p>Select the Menu option by clicking on the 9 dots.</p> <p></p>"},{"location":"guide/03-04/index.html#22-select-data-engineering","title":"2.2. Select Data Engineering","text":"<p>Navigate to the Cloudera Data Engineering Overview page by clicking the Data Engineering tile in the Cloudera management console.</p> <p></p>"},{"location":"guide/03-04/index.html#23-select-administration","title":"2.3. Select Administration","text":"<p>Navigate to the main menu and select Administration option from the panel. </p> <p></p>"},{"location":"guide/03-04/index.html#24-click-on-cluster-details","title":"2.4. Click on Cluster Details","text":"<p>In the Virtual Clusters column, click Cluster Details for the virtual cluster.</p> <p></p>"},{"location":"guide/03-04/index.html#25-click-airflow-ui","title":"2.5.  Click AIRFLOW UI","text":""},{"location":"guide/03-04/index.html#26-click-the-connection-link","title":"2.6. Click the Connection link","text":"<p>From the Airflow UI, click the Connection link from the Admin menu.</p> <p></p>"},{"location":"guide/03-04/index.html#27-add-a-new-record","title":"2.7. Add a new record","text":"<p>Click the plus sign to add a new record.</p> <p></p>"},{"location":"guide/03-04/index.html#28-fill-in-the-fields","title":"2.8. Fill in the fields","text":"<p>Fill in the following details in the fields</p> <ul> <li>Conn Id: Create a unique connection identifier, such as \"cdw_connection_YourCDPUsername\".</li> <li>For ex: cdw_connection_dse_1_250204</li> <li>Conn Type: Select Hive Client Wrapper.</li> <li>Host: Enter the hostname from the JDBC connection URL. Do not enter the full JDBC URL.</li> <li>Schema: default</li> <li>Login: Enter your workload username</li> <li>Password: Enter your workload password.</li> </ul> <p>Click Save.</p> <p></p>"},{"location":"guide/03-04/index.html#29-review-record","title":"2.9. Review record","text":"<p>Success</p> <p>A success message appears confirming a new row being added. </p> <p></p>"},{"location":"guide/03-04/index.html#3-review-the-dag-python-file","title":"3. Review the DAG Python file","text":"<p>Now you are ready to use the CDWOperator in your Airflow DAG. In the cde_jobs/ directory, a copy of \"firstdag.py\" has been made and named \"cdw_dag.py\" with the following additions. </p>"},{"location":"guide/03-04/index.html#31-import-operator","title":"3.1. Import Operator","text":"<p>At the top, an Operator has been imported along with other import statements.</p> <pre><code>from cloudera.cdp.airflow.operators.cdw_operator import CDWOperator\n</code></pre> <p></p>"},{"location":"guide/03-04/index.html#32-add-object","title":"3.2. Add Object","text":"<p>At the bottom of the file an instance of the CDWOperator object has been added.</p> <pre><code>cdw_query = \"\"\"\nshow databases;\n\"\"\"\n\ndw_step3 = CDWOperator(\n  task_id='dataset-etl-cdw',\n  dag=example_dag,\n  cli_conn_id='cdw_connection_YourCDPUsername',\n  hql=cdw_query,\n  schema='default',\n  use_proxy_user=False,\n  query_isolation=True\n)\n</code></pre> <p>Notice that the SQL syntax run in the CDW Virtual Warehouse is declared as a separate variable and then passed to the Operator instance as an argument.</p> <p></p>"},{"location":"guide/03-04/index.html#33-update-task-dependencies","title":"3.3.  Update task dependencies","text":"<p>Task dependencies have been updated to include \"dw_step3\":</p> <pre><code>spark_step &gt;&gt; shell &gt;&gt; dw_step3\n</code></pre> <p></p>"},{"location":"guide/03-04/index.html#34-update-variables","title":"3.4. Update variables","text":"<p>DAG names are stored in Airflow and must be unique. Therefore, the variable name of the DAG object instance has been changed to \"airflow_cdw_dag\" and the DAG ID to \"dw_dag\" as shown below.</p> <pre><code>airflow_cdw_dag = DAG(\n    'dw_dag',\n    default_args=default_args,\n    schedule_interval='@daily',\n    catchup=False,\n    is_paused_upon_creation=False\n    )\n</code></pre> <p>Notice the new DAG variable needs to be updated in each Operator as well. The completed DAG file is included in the cde_jobs folder for your convenience.</p> <p></p>"},{"location":"guide/03-04/index.html#35-replace-owner","title":"3.5. Replace Owner","text":"<p>Before moving on, make sure to replace the current value of the 'Owner' with your YourCDPUsername in the default arguments dictionary.</p> <p></p>"},{"location":"guide/03-04/index.html#36-replace-spark-job-name","title":"3.6. Replace Spark job name","text":"<p>Before moving on, make sure to replace the current value of the 'job_name' with sparksql_YourCDPUsername in spark_step block.</p> <p></p>"},{"location":"guide/03-04/index.html#37-replace-cli-connection-id","title":"3.7. Replace CLI Connection ID","text":"<p>Before moving on, make sure to replace the current value of the 'cli_conn_id' with the value cdw_connection_YourCDPUsername used while creating the connection. No other changes are required at this time.</p> <p></p>"},{"location":"guide/03-04/index.html#4-create-a-new-airflow-cde-job","title":"4. Create a new Airflow CDE Job","text":""},{"location":"guide/03-04/index.html#41-select-jobs","title":"4.1. Select Jobs","text":"<p>Navigate to the Jobs page by clicking on the Jobs in the main panel.</p> <p></p>"},{"location":"guide/03-04/index.html#42-click-on-create-job","title":"4.2. Click on Create Job","text":"<p>Click on Create Job to create a new CDE Job.</p> <p></p>"},{"location":"guide/03-04/index.html#43-fill-in-the-details","title":"4.3. Fill in the details","text":"<ul> <li>Job Type: Airflow</li> <li>Name: CDWDag_YourCDPUsername</li> <li>Dag File: File</li> </ul> <p>Click on Upload</p> <p></p>"},{"location":"guide/03-04/index.html#44-upload-file-to-a-resource","title":"4.4. Upload file to a resource","text":"<p>Upload cdw_dag.py to the firstdag CDE Resource you created earlier.</p> <p>Ensure that you have replaced the current value of the 'Owner' with your CDP Username in the default arguments dictionary. Please refer to Step 2.3 in the previous exercise for details. </p> <p>Click on Upload.</p> <p></p>"},{"location":"guide/03-04/index.html#45-select-create-and-run","title":"4.5. Select 'Create and Run'","text":"<p>Select Create and Run button to trigger the job immediately.</p> <p></p>"},{"location":"guide/03-04/index.html#46-job-run-in-progress","title":"4.6. Job run in progress","text":"<p>Please wait for a min for job to run</p> <p></p>"},{"location":"guide/03-04/index.html#47-new-run-is-initiated","title":"4.7. New run is initiated","text":"<p>A success message appears confirming that a new run with ID has been initiated. </p> <p></p>"},{"location":"guide/03-04/index.html#48-review-the-jobs-in-progress","title":"4.8. Review the Jobs in progress","text":"<p>Notice that two CDE Jobs are now in progress. One of them is \"sparksql_dse_1_250204\" (Spark CDE Job) and the other is \"CDWDag_dse_1_250204\" (Airflow CDE Job). The former has been triggered by the execution of the latter. Wait a few moments and allow for the DAG to complete.</p> <p></p>"},{"location":"guide/03-04/index.html#49-click-on-the-dag-link","title":"4.9. Click on the Dag link","text":"<p>Once the Status column shows the Jobs run as succeeded, click on the \"CDWDag_dse_1_250204\" link to access the Job Run page.</p> <p></p>"},{"location":"guide/03-04/index.html#410-select-the-most-recent-run","title":"4.10. Select the most recent Run","text":"<p>This page shows each run along with associated logs, execution statistics, and the Airflow UI.</p> <p>Ensure to select the most recent Run (in the screenshot, the most recent Run ID number is 60)</p> <p></p>"},{"location":"guide/03-04/index.html#411-review-job-run","title":"4.11. Review Job Run","text":"<p>Review the Job Run. Notice the run id in the top section of the page. </p> <p>Click on the Airflow UI tab.</p> <p></p>"},{"location":"guide/03-04/index.html#412-review-airflow-ui-tab","title":"4.12. Review Airflow UI tab","text":"<p>The first landing page lists all tasks along with their status. Notice that the DAG ID, Task ID and Operator columns are populated with the values set in the DAG python files. </p> <p>Next, click on the back arrow on the left side of the screen to navigate to the Airflow UI DAGs view.</p> <p></p>"},{"location":"guide/03-04/index.html#413-explore-dags-view","title":"4.13. Explore DAGs view.","text":"<p>From the DAGs view you can:</p> <ul> <li>Pause/unpause a DAG</li> <li>Filter the list of DAGs to show active, paused, or all DAGs</li> <li>Trigger, refresh, or delete a DAG</li> <li>Navigate quickly to other DAG-specific pages</li> </ul> <p>The DAG name specified in the python file during DAG declaration is dw_dag.</p> <p>Click on it to open the Airflow UI DAG view to drill down with DAG-specific pages.</p> <p></p>"},{"location":"guide/03-04/index.html#414-review-dag","title":"4.14. Review DAG","text":"<p>Here, Airflow provides a number of tabs to increase job observability. Open the Tree View and validate that the job has succeeded.</p> <p></p>"},{"location":"guide/03-04/index.html#5-printing-context-variables-with-the-bashoperator","title":"5. Printing Context Variables with the BashOperator","text":"<p>Info</p> <p>When Airflow runs a task, it collects several variables and passes these to the context argument on the execute() method. These variables hold information about the current task.</p> <p>Open the \"bash_dag.py\" file and examine the contents. Notice that at lines 52-56 a new instance of the BashOperator has been declared with the following entries:</p> <pre><code>also_run_this = BashOperator(\n    task_id='also_run_this',\n    dag=bash_airflow_dag,\n    bash_command='echo \"yesterday={{ yesterday_ds }} | today={{ ds }}| tomorrow={{ tomorrow_ds }}\"',\n)\n</code></pre> <p>Above we printed the \"yesterday_ds\", \"ds\" and tomorrow_ds\" dates. There are many more and you can find the full list here.</p> <p>Variables can also be saved and reused by other operators. We will explore this in the section on XComs.</p> <p></p>"},{"location":"guide/03-04/index.html#6-review-the-dag-python-file","title":"6. Review the DAG python file","text":"<p>Now you are ready to use the CDWOperator in your Airflow DAG. In the cde_jobs/ directory, a copy of \"cdw_dag.py\" has been made and named \"py_dag.py\" with the following additions.</p>"},{"location":"guide/03-04/index.html#61-python-operator","title":"6.1. Python Operator","text":"<p>Lines 60-67 in \"py_dag.py\" show how to use the operator to print out all Conext Variables in one run.</p> <pre><code>def _print_context(**context):\n   print(context)\n\nprint_context = PythonOperator(\n    task_id=\"print_context\",\n    python_callable=_print_context,\n    dag=dag,\n)\n</code></pre> <p></p>"},{"location":"guide/03-04/index.html#62-replace-owner","title":"6.2. Replace Owner","text":"<p>Before moving on, make sure to replace the current value of the 'Owner' with your YourCDPUsername in the default arguments dictionary.</p> <p></p>"},{"location":"guide/03-04/index.html#63-replace-spark-job-name","title":"6.3. Replace Spark job name","text":"<p>Before moving on, make sure to replace the current value of the 'job_name' with sparksql_YourCDPUsername in spark_step block.</p> <p></p>"},{"location":"guide/03-04/index.html#64-replace-cli-connection-id","title":"6.4. Replace CLI Connection ID","text":"<p>Before moving on, make sure to replace the current value of the 'cli_conn_id' with the value cdw_connection_YourCDPUsername used while creating the connection. No other changes are required at this time.</p> <p></p>"},{"location":"guide/03-04/index.html#7-using-the-python-operator","title":"7. Using the Python Operator","text":"<p>Info</p> <p>The PythonOperator allows you to run Python code inside the DAG. This is particularly helpful as it allows you to customize your DAG logic in a variety of ways.</p> <p>The PythonOperator requires implementing a callable inside the DAG file. Then, the method is called from the operator.</p>"},{"location":"guide/03-04/index.html#71-select-jobs","title":"7.1. Select Jobs","text":"<p>Navigate to the Jobs page by clicking on the Jobs in the main panel.</p> <p></p>"},{"location":"guide/03-04/index.html#72-click-on-create-job","title":"7.2. Click on Create Job","text":"<p>Click on Create Job to create a new CDE Job.</p> <p></p>"},{"location":"guide/03-04/index.html#73-fill-in-the-details","title":"7.3. Fill in the details","text":"<ul> <li>Job Type: Airflow</li> <li>Name: pydag_YourCDPUsername</li> <li>Dag File: File</li> <li>Upload py_dag.py to the firstdag CDE Resource you created earlier.</li> </ul> <p>Select 'Create and Run' button to trigger the job immediately.</p> <p></p>"},{"location":"guide/03-04/index.html#74-job-run-in-progress","title":"7.4. Job run in progress","text":"<p>Please wait for a min for job to run</p> <p></p>"},{"location":"guide/03-04/index.html#75-new-run-is-initiated","title":"7.5. New run is initiated","text":"<p>A success message appears confirming that a new run with ID has been initiated. </p> <p></p>"},{"location":"guide/03-04/index.html#76-review-the-jobs-in-progress","title":"7.6. Review the Jobs in progress","text":"<p>Notice that two CDE Jobs are now in progress. One of them is \"sparksql_dse_1_250204\" (Spark CDE Job) and the other is \"pydag_dse_1_250204\" (Airflow CDE Job). The former has been triggered by the execution of the latter. Wait a few moments and allow for the DAG to complete.</p> <p></p>"},{"location":"guide/03-04/index.html#77-click-on-the-dag-link","title":"7.7. Click on the Dag link","text":"<p>Once the Status column shows the Jobs run as succeeded, click on the \"pydag_dse_1_250204\" link to access the Job Run page.</p> <p></p>"},{"location":"guide/03-04/index.html#78-select-the-most-recent-run","title":"7.8. Select the most recent Run","text":"<p>This page shows each run along with associated logs, execution statistics, and the Airflow UI.</p> <p>Ensure to select the most recent Run (in the screenshot, the most recent Run ID number is 68)</p> <p></p>"},{"location":"guide/03-04/index.html#79-review-job-run","title":"7.9. Review Job Run","text":"<p>Review the Job Run. Notice the run id in the top section of the page. </p> <p>Click on the Logs tab.</p> <p></p>"},{"location":"guide/03-04/index.html#710-select-dag-task","title":"7.10. Select DAG Task","text":"<p>Ensure to select the correct Airflow task which in this case is \"print_context\":</p> <p></p>"},{"location":"guide/03-04/index.html#711-review-dag-task","title":"7.11. Review DAG Task","text":"<p>Review the print_context task</p> <p>Scroll to the bottom and validate the output.</p> <p></p>"},{"location":"guide/03-04/index.html#8-end-of-the-exercise","title":"8. End of the Exercise","text":""},{"location":"guide/03-05/index.html","title":"03-05 More Airflow DAG Features","text":""},{"location":"guide/03-05/index.html#using-the-simplehttpoperator","title":"Using the SimpleHttpOperator","text":"<p>Info</p> <p>You can use the SimpleHttpOperator to call HTTP requests and get the response text back. The Operator can help when you need to interact with 3rd party systems, APIs, and perform actions based on complex control flow logic.</p> <p>In the following example we will send a request to the Chuck Norris API from the Airflow DAG. Before updating the DAG file, we need to set up a new Airflow Connection and Airflow Variables.</p>"},{"location":"guide/03-05/index.html#1-creating-an-http-airflow-connection","title":"1. Creating an HTTP Airflow Connection","text":""},{"location":"guide/03-05/index.html#11-select-administration","title":"1.1. Select Administration","text":"<p>Navigate to the main menu and select Administration option from the panel. </p> <p></p>"},{"location":"guide/03-05/index.html#12-click-on-cluster-details","title":"1.2. Click on Cluster Details","text":"<p>In the Virtual Clusters column, click Cluster Details for the virtual cluster.</p> <p></p>"},{"location":"guide/03-05/index.html#13-click-airflow-ui","title":"1.3.  Click AIRFLOW UI","text":""},{"location":"guide/03-05/index.html#14-click-the-connection-link","title":"1.4. Click the Connection link","text":"<p>From the Airflow UI, click the Connection link from the Admin menu.</p> <p></p>"},{"location":"guide/03-05/index.html#15-add-a-new-record","title":"1.5. Add a new record","text":"<p>Click the plus sign to add a new record.</p> <p></p>"},{"location":"guide/03-05/index.html#16-fill-in-the-fields","title":"1.6. Fill in the fields","text":"<p>Configure the connection by entering the following values for the following parameters. Leave the remaining entries blank.</p> <ul> <li>Connection ID: chuck_norris_connection_YourCDPUsername</li> <li>Connection Type: HTTP</li> <li>Host: https://matchilling-chuck-norris-jokes-v1.p.rapidapi.com</li> </ul> <p>Click Save.</p> <p></p>"},{"location":"guide/03-05/index.html#17-review-record","title":"1.7. Review record","text":"<p>A success message appears confirming a new row being added. </p> <p></p>"},{"location":"guide/03-05/index.html#2-creating-airflow-variables","title":"2. Creating Airflow Variables","text":"<p>Info</p> <p>Airflow Variables allow you to parameterize your operators. Airflow Variables are used as environment variables for the DAG. Therefore, if you are looking to temporarily store operator results in the DAG and pass values to downstream operators you should use XComs (shown in the next section).</p> <p>In our example, we will use them to pass an API KEY and HOST value to the SimpleHttpOperator below. To set up Airflow Variables, navigate back to the CDE Virtual Cluster Service Details page and open the Airflow UI.</p>"},{"location":"guide/03-05/index.html#21-click-the-connection-link","title":"2.1. Click the Connection link","text":"<p>From the Airflow UI, click on the \"Variables\" tab under the \"Admin\" drop down at the top of the page.</p> <p></p>"},{"location":"guide/03-05/index.html#22-add-a-new-record","title":"2.2. Add a new record","text":"<p>Click the plus sign to add a new record.</p> <p></p>"},{"location":"guide/03-05/index.html#23-create-first-variable","title":"2.3. Create first variable","text":"<p>Create a variable with the following entries:</p> <ul> <li>Key: rapids_api_host_YourCDPUsername</li> <li>Value: matchilling-chuck-norris-jokes-v1.p.rapidapi.com</li> </ul> <p>Click Save</p> <p></p>"},{"location":"guide/03-05/index.html#24-review-new-row","title":"2.4. Review new row","text":"<p>Success</p> <p>A success message appears confirming that a row has been added. </p> <p>Click the plus sign to add a new record.</p> <p></p>"},{"location":"guide/03-05/index.html#25-create-second-variable","title":"2.5. Create second variable","text":"<p>Create a new variable with the following entries:</p> <ul> <li>Key: rapids_api_key_YourCDPUsername</li> <li>Value: f16c49e390msh7e364a479e33b3dp10fff7jsn6bc84b000b75</li> </ul> <p>Click Save</p> <p></p>"},{"location":"guide/03-05/index.html#26-review-second-row","title":"2.6. Review second row","text":"<p>Success</p> <p>A success message appears confirming that the new row has been added.</p> <p></p>"},{"location":"guide/03-05/index.html#3-working-with-the-dag","title":"3. Working with the DAG","text":"<p>Next, open \"http_dag.py\" and familiarize yourself with the code. The code relevant to the new operator is used between lines 71 and 92.</p>"},{"location":"guide/03-05/index.html#31-import-variable","title":"3.1. Import Variable","text":"<p>Notice that at line 11 we are importing the Variable type from the airflow.models module.</p> <p></p>"},{"location":"guide/03-05/index.html#32-create-airflow-variables","title":"3.2. Create Airflow Variables","text":"<p>We are then creating two Airflow Variables at lines 71 and 72.</p> <p></p>"},{"location":"guide/03-05/index.html#33-map-parameter","title":"3.3. Map parameter","text":"<p>The \"http_conn_id\" parameter is mapped to the Connection ID you configured in the prior step.</p> <p></p>"},{"location":"guide/03-05/index.html#34-validate-response","title":"3.4. Validate response","text":"<p>The \"response_check\" parameter allows you to specify a python method to validate responses. This is the \"handle_response\" method declared at line 74.</p> <pre><code>api_host = Variable.get(\"rapids_api_host\")\napi_key = Variable.get(\"rapids_api_key\")\n\ndef handle_response(response):\n    if response.status_code == 200:\n        print(\"Received 200 Ok\")\n        return True\n    else:\n        print(\"Error\")\n        return False\n\nhttp_task = SimpleHttpOperator(\n    task_id=\"chuck_norris_task\",\n    method=\"GET\",\n    http_conn_id=\"chuck_norris_connection\",\n    endpoint=\"/jokes/random\",\n    headers={\"Content-Type\":\"application/json\",\n            \"X-RapidAPI-Key\": api_key,\n            \"X-RapidAPI-Host\": api_host},\n    response_check=lambda response: handle_response(response),\n    dag=http_dag\n)\n</code></pre> <p></p>"},{"location":"guide/03-05/index.html#35-replace-owner","title":"3.5. Replace Owner","text":"<p>Make sure to replace the current value of the 'Owner' with your YourCDPUsername in the default arguments dictionary.</p> <p></p>"},{"location":"guide/03-05/index.html#36-replace-spark-job-name","title":"3.6. Replace Spark job name","text":"<p>Make sure to replace the current value of the 'job_name' with sparksql_YourCDPUsername in spark_step block.</p> <p></p>"},{"location":"guide/03-05/index.html#37-replace-cli-connection-id","title":"3.7. Replace CLI Connection ID","text":"<p>Make sure to replace the current value of the 'cli_conn_id' with the value cdw_connection_YourCDPUsername used while creating the connection.</p> <p></p>"},{"location":"guide/03-05/index.html#38-replace-airflow-variables","title":"3.8. Replace Airflow Variables","text":"<p>Make sure to replace the current value of the 'api_host' and api_key with the value rapids_api_host_YourCDPUsername and rapids_api_key_YourCDPUsername used while creating the variables.</p> <p></p>"},{"location":"guide/03-05/index.html#39-replace-http-cli-connection-id","title":"3.9. Replace HTTP CLI Connection ID","text":"<p>Make sure to replace the current value of the 'http_conn_id' with the value chuck_norris_connection_YourCDPUsername used while creating the connection.</p> <p></p>"},{"location":"guide/03-05/index.html#4-create-a-new-airflow-cde-job","title":"4. Create a new Airflow CDE Job","text":""},{"location":"guide/03-05/index.html#41-select-jobs","title":"4.1. Select Jobs","text":"<p>Navigate to the Jobs page by clicking on the Jobs in the main panel.</p> <p></p>"},{"location":"guide/03-05/index.html#42-click-on-create-job","title":"4.2. Click on Create Job","text":"<p>Click on Create Job to create a new CDE Job.</p> <p></p>"},{"location":"guide/03-05/index.html#43-fill-in-the-details","title":"4.3. Fill in the details","text":"<ul> <li>Job Type: Airflow</li> <li>Name: ChuckNorris_YourCDPUsername</li> <li>Dag File: File</li> <li>Upload http_dag.py to the firstdag CDE Resource you created earlier.</li> </ul> <p>Select 'Create and Run' button to trigger the job immediately.</p> <p></p>"},{"location":"guide/03-05/index.html#44-job-run-in-progress","title":"4.4. Job run in progress","text":"<p>Please wait for a min for job to run</p> <p></p>"},{"location":"guide/03-05/index.html#45-new-run-is-initiated","title":"4.5. New run is initiated","text":"<p>Success</p> <p>A success message appears confirming that a new run with ID has been initiated. </p> <p></p>"},{"location":"guide/03-05/index.html#46-review-the-jobs-in-progress","title":"4.6. Review the Jobs in progress","text":"<p>Notice that two CDE Jobs are now in progress. One of them is \"sparksq_dse_1_250204\" (Spark CDE Job) and the other is \"ChuckNorris_dse_1_250204\" (Airflow CDE Job). The former has been triggered by the execution of the latter. Wait a few moments and allow for the DAG to complete.</p> <p></p>"},{"location":"guide/03-05/index.html#47-click-on-the-dag-link","title":"4.7. Click on the Dag link","text":"<p>Once the Status column shows the Jobs run as succeeded, click on the \"ChuckNorris_dse_1_250204\" link to access the Job Run page.</p> <p></p>"},{"location":"guide/03-05/index.html#48-select-the-most-recent-run","title":"4.8. Select the most recent Run","text":"<p>This page shows each run along with associated logs, execution statistics, and the Airflow UI.</p> <p>Ensure to select the most recent Run (in the screenshot, the most recent Run ID number is 72)</p> <p></p>"},{"location":"guide/03-05/index.html#49-review-job-run","title":"4.9. Review Job Run","text":"<p>Review the Job Run. Notice the run id in the top section of the page. </p> <p>Click on the Logs tab.</p> <p></p>"},{"location":"guide/03-05/index.html#410-select-dag-task","title":"4.10. Select DAG Task","text":"<p>Ensure to select the correct Airflow task which in this case is \"chuck_norris_task\":</p> <p></p>"},{"location":"guide/03-05/index.html#411-review-dag-task","title":"4.11.  Review DAG Task","text":"<p>Review the chuck_norris_task task</p> <p>Scroll to the bottom and validate the output.</p> <p></p>"},{"location":"guide/03-05/index.html#5-using-xcoms","title":"5. Using XComs","text":"<p>Although the request in the prior step was successful the operator did not actually return the response to the DAG. XComs (short for \u201ccross-communications\u201d) are a mechanism that let Tasks talk to each other, as by default Tasks are entirely isolated and may be running on entirely different machines.</p> <p>Note</p> <p>Practically XComs allow your operators to store results into a governed data structure and then reuse the values within the context of different operators. An XCom is identified by a key (essentially its name), as well as the task_id and dag_id it came from. They are only designed for small amounts of data; do not use them to pass around large values, like dataframes.</p> <p>Open \"xcom_dag.py\" and familiarize yourself with the code. Notice the following changes in the code between lines 82 and 103:</p>"},{"location":"guide/03-05/index.html#51-add-an-argument","title":"5.1. Add an argument","text":"<p>At line 92 we added a \"do_xcom_push=True\" argument. This allows the response to be temporarily saved in the DAG.</p> <p></p>"},{"location":"guide/03-05/index.html#52-add-python-method","title":"5.2. Add python method","text":"<p>At line 95 we introduced a new Python method \"print_chuck_norris_quote\" and at line 96 we use the built-in \"xcom_pull\" method to retrieve the temporary value from the SimpleHttpOperator task.</p> <p></p>"},{"location":"guide/03-05/index.html#53-declare-operator","title":"5.3. Declare operator","text":"<p>At line 62, we declared a new Python Operator running the method above.</p> <pre><code>http_task = SimpleHttpOperator(\n    task_id=\"chuck_norris_task\",\n    method=\"GET\",\n    http_conn_id=\"chuck_norris_connection\",\n    endpoint=\"/jokes/random\",\n    headers={\"Content-Type\":\"application/json\",\n            \"X-RapidAPI-Key\": api_key,\n            \"X-RapidAPI-Host\": api_host},\n    response_check=lambda response: handle_response(response),\n    dag=xcom_dag,\n    do_xcom_push=True\n)\n\ndef _print_chuck_norris_quote(**context):\n    return context['ti'].xcom_pull(task_ids='chuck_norris_task')\n\nreturn_quote = PythonOperator(\n    task_id=\"print_quote\",\n    python_callable=_print_chuck_norris_quote,\n    dag=xcom_dag\n)\n</code></pre> <p></p>"},{"location":"guide/03-05/index.html#54-replace-owner","title":"5.4. Replace Owner","text":"<p>Make sure to replace the current value of the 'Owner' with your YourCDPUsername in the default arguments dictionary.</p> <p></p>"},{"location":"guide/03-05/index.html#55-replace-spark-job-name","title":"5.5. Replace Spark job name","text":"<p>Make sure to replace the current value of the 'job_name' with sparksql_YourCDPUsername in spark_step block.</p> <p></p>"},{"location":"guide/03-05/index.html#56-replace-cli-connection-id","title":"5.6. Replace CLI Connection ID","text":"<p>Make sure to replace the current value of the 'cli_conn_id' with the value cdw_connection_YourCDPUsername used while creating the connection.</p> <p></p>"},{"location":"guide/03-05/index.html#57-replace-airflow-variables","title":"5.7. Replace Airflow Variables","text":"<p>Make sure to replace the current value of the 'api_host' and api_key with the value rapids_api_host_YourCDPUsername and rapids_api_key_YourCDPUsername used while creating the variables.</p> <p></p>"},{"location":"guide/03-05/index.html#58-replace-http-cli-connection-id","title":"5.8. Replace HTTP CLI Connection ID","text":"<p>Make sure to replace the current value of the 'http_conn_id' with the value chuck_norris_connection_YourCDPUsername used while creating the connection.</p> <p></p>"},{"location":"guide/03-05/index.html#6-create-a-new-airflow-cde-job","title":"6. Create a new Airflow CDE Job","text":""},{"location":"guide/03-05/index.html#61-select-jobs","title":"6.1. Select Jobs","text":"<p>Navigate to the Jobs page by clicking on the Jobs in the main panel.</p> <p></p>"},{"location":"guide/03-05/index.html#62-click-on-create-job","title":"6.2. Click on Create Job","text":"<p>Click on Create Job to create a new CDE Job.</p> <p></p>"},{"location":"guide/03-05/index.html#63-fill-in-the-details","title":"6.3. Fill in the details","text":"<ul> <li>Job Type: Airflow</li> <li>Name: XcomDag_YourCDPUsername</li> <li>Dag File: File</li> <li>Upload xcom_dag.py to the firstdag CDE Resource you created earlier.</li> </ul> <p>Select 'Create and Run' button to trigger the job immediately.</p> <p></p>"},{"location":"guide/03-05/index.html#64-job-run-in-progress","title":"6.4. Job run in progress","text":"<p>Please wait for a min for job to run</p> <p></p>"},{"location":"guide/03-05/index.html#65-new-run-is-initiated","title":"6.5. New run is initiated","text":"<p>Success</p> <p>A success message appears confirming that a new run with ID has been initiated. </p> <p></p>"},{"location":"guide/03-05/index.html#66-review-the-jobs-in-progress","title":"6.6. Review the Jobs in progress","text":"<p>Notice that two CDE Jobs are now in progress. One of them is \"sparksql_dse_1_250204\" (Spark CDE Job) and the other is \"XcomDags_dse_1_250204\" (Airflow CDE Job). The former has been triggered by the execution of the latter. Wait a few moments and allow for the DAG to complete.</p> <p></p>"},{"location":"guide/03-05/index.html#67-click-on-the-firstdag-link","title":"6.7. Click on the FirstDag link","text":"<p>Once the Status column shows the Jobs run as succeeded, click on the \"XcomDags_dse_1_250204\" link to access the Job Run page.</p> <p></p>"},{"location":"guide/03-05/index.html#68-select-the-most-recent-run","title":"6.8. Select the most recent Run","text":"<p>This page shows each run along with associated logs, execution statistics, and the Airflow UI.</p> <p>Ensure to select the most recent Run (in the screenshot, the most recent Run ID number is 98)</p> <p></p>"},{"location":"guide/03-05/index.html#69-review-job-run","title":"6.9. Review Job Run","text":"<p>Review the Job Run. Notice the run id in the top section of the page. </p> <p>Click on the Logs tab.</p> <p></p>"},{"location":"guide/03-05/index.html#610-select-dag-task","title":"6.10. Select DAG Task","text":"<p>Ensure to select the correct Airflow task which in this case is \"print_quote\":</p> <p></p>"},{"location":"guide/03-05/index.html#611-review-dag-task","title":"6.11. Review DAG Task","text":"<p>Review the print_quote task</p> <p>Scroll all the way down and validate that a Chuck Norris quote has been printed out. Which one did you get?</p> <p></p>"},{"location":"guide/03-05/index.html#7-end-of-the-exercise","title":"7. End of the Exercise","text":""},{"location":"guide/04-01/index.html","title":"04-01 Access and Interact","text":""},{"location":"guide/04-01/index.html#1-introduction","title":"1. Introduction","text":""},{"location":"guide/04-01/index.html#2-the-types-of-kubernetes-clusters","title":"2. The Types of Kubernetes Clusters","text":""},{"location":"guide/04-01/index.html#3-install-minikube","title":"3. Install minikube","text":""},{"location":"guide/04-01/index.html#4-kubernetes-dashboard","title":"4. Kubernetes Dashboard","text":""},{"location":"guide/04-01/index.html#5-kubernetes-command-line-interface","title":"5. Kubernetes Command Line Interface","text":""},{"location":"guide/04-01/index.html#6-summary-and-exercise-assignments","title":"6. Summary and Exercise Assignments","text":""},{"location":"guide/04-02/index.html","title":"04-02 Installing minikube for macOS","text":"<p>The purpose of this exercise is to install minikube on a Mac. Minikube will be used as the classroom environment to teach Kubernetes</p>"},{"location":"guide/04-02/index.html#1-recommend-an-ide","title":"1. Recommend an IDE","text":"<p>It is recommended to install an IDE. The purpose overtime is to build up examples and working code. These exercises will show images of Visual Studio Code. However, the commands in these exercises can be run from a terminal on a desktop.</p>"},{"location":"guide/04-02/index.html#2-install-a-docker-desktop","title":"2. Install a Docker Desktop","text":"<p>There are a large number of container environments to select from and each technician will have their preferered software. This is an example of installing Docker Desktop for a Mac.</p>"},{"location":"guide/04-02/index.html#21-click-mac-docker-docs","title":"2.1. Click \"Mac | Docker Docs\"","text":"<p>Open the Install Docker Desktop on Mac web page. This can be found with an Internet search or by opening this URL.</p> <pre><code>https://docs.docker.com/desktop/setup/install/mac-install/\n</code></pre> <p></p>"},{"location":"guide/04-02/index.html#22-click-mac-docker-docs","title":"2.2. Click \"Mac | Docker Docs\"","text":"<p>Review the instructions for Install interactively. This instance will be only used for educational purposes and thus meets the Docker Subscription Service Agreement</p> <p></p>"},{"location":"guide/04-02/index.html#23-click-system-requirements","title":"2.3. Click \"System requirements\"","text":"<p>Click to download the correct process architecture. </p> <ul> <li>Docker Desktop for Mac with Apple silicon</li> <li>Docker Desktop for Mac with Intel chip</li> </ul> <p></p>"},{"location":"guide/04-02/index.html#24-click-docker","title":"2.4. Click \"Docker\"","text":"<p>Click on the Docker.dmg to open. Drag and drop the Docker application into the Applications folder.</p> <p></p>"},{"location":"guide/04-02/index.html#25-accept","title":"2.5. Accept","text":"<p>Review the license agreement. Click to Accept to continue.</p>"},{"location":"guide/04-02/index.html#26-use-recommended-setting","title":"2.6. Use recommended setting","text":"<p>Select use recommended settings. This will require your user password to install and to configure.</p>"},{"location":"guide/04-02/index.html#3-start-docker-desktop","title":"3. Start Docker Desktop","text":""},{"location":"guide/04-02/index.html#31-open-applications","title":"3.1. Open Applications","text":"<p>Select Go &gt; Applications</p> <p>Locate Docker icon. </p> <p>For ease of access drag the Docker icon onto your toolbar</p> <p></p>"},{"location":"guide/04-02/index.html#32-click-desktop","title":"3.2. Click \"desktop\"","text":"<p>Click on the icon for Docker. Enter your password if required. The Docker Desktop is now running. We will explore it later after we have installed minikube.</p> <p></p>"},{"location":"guide/04-02/index.html#4-install-minikube","title":"4. Install minikube","text":""},{"location":"guide/04-02/index.html#41-click-minikube-start-minikube","title":"4.1. Click \"minikube start | minikube\"","text":"<p>Open the minikube install page. This can be found with an Internet search or using this URL.</p> <pre><code>https://minikube.sigs.k8s.io/docs/start/?arch=%2Fmacos%2Fx86-64%2Fstable%2Fbinary+download\n</code></pre> <p></p>"},{"location":"guide/04-02/index.html#42-click-minikube-start-minikube","title":"4.2. Click \"minikube start | minikube\"","text":"<p>minikube is lightweight but full featured. It runs as a Docker container. A container environment is required. </p> <p></p>"},{"location":"guide/04-02/index.html#43-click-minikube-start-minikube","title":"4.3. Click \"minikube start | minikube\"","text":"<p>Scroll down to the Installation.  Select the functions to set up your installation path. In this example we have set the following:</p> <ul> <li>Operating system: macOS</li> <li>Architecture: ARM64</li> <li>Release type: Stable</li> <li>Installer type: homebrew</li> </ul> <p></p>"},{"location":"guide/04-02/index.html#44-install-minikube","title":"4.4. Install minikube","text":"<p>Use Homebrew Package Manager to install minikube</p> <pre><code>brew install minikube\n</code></pre> <p></p>"},{"location":"guide/04-02/index.html#45-review-install","title":"4.5. Review Install","text":"<p>Verify the install of minikube was successful.</p> <p></p>"},{"location":"guide/04-02/index.html#5-end-of-exercise","title":"5. End of Exercise","text":""},{"location":"guide/04-03/index.html","title":"Step 3: Cloudera Data Warehouse - Raw Layer (Direct Cloud Object Storage Access)","text":"<p>The objective of this step is to create External tables on top of raw CSV files sitting in cloud storage (In this case it has been stored in AWS S3 by the instructor) and then run few queries to access the data via SQL using HUE.</p>"},{"location":"guide/04-03/index.html#31-open-hue-for-cdw-virtual-warehouse-vw-hive","title":"3.1 Open Hue for CDW Virtual Warehouse - <code>vw-hive</code>","text":"<ul> <li> <p>Click on the  button on the right upper     corner of <code>vw-hive</code> as shown in the screenshot below.     </p> </li> <li> <p>Create new databases. Enter the following query and then make sure     that you enter the user assigned to you. In the screenshot the user     is <code>wuser00</code>.</p> </li> </ul> <pre><code>CREATE DATABASE ${user_id}_airlines_raw;\n\nCREATE DATABASE ${user_id}_airlines;\n</code></pre> <p></p> <ul> <li>There may be many databases, look for the 2 that start with your     <code>&lt;user_id&gt;</code>. Run the following SQL to see the 2 databases that     you created just now.</li> </ul> <pre><code>SHOW DATABASES;\n</code></pre> <p></p>"},{"location":"guide/04-03/index.html#32-run-the-following-ddl-in-hue-for-the-cdw-virtual-warehouse-vw-hive","title":"3.2 Run the following DDL in HUE for the CDW Virtual Warehouse - <code>vw-hive</code>","text":"<p>This will create External Tables on CSV Data Files that have been uploaded previously by your instructor in AWS S3. This provides a fast way to allow SQL layer on top of data in cloud storage.</p> <ul> <li>Copy paste the following into HUE.</li> </ul> <pre><code>drop table if exists ${user_id}_airlines_raw.flights_csv;\nCREATE EXTERNAL TABLE ${user_id}_airlines_raw.flights_csv(month int, dayofmonth int, dayofweek int, deptime int, crsdeptime int, arrtime int, crsarrtime int, uniquecarrier string, flightnum int, tailnum string, actualelapsedtime int, crselapsedtime int, airtime int, arrdelay int, depdelay int, origin string, dest string, distance int, taxiin int, taxiout int, cancelled int, cancellationcode string, diverted string, carrierdelay int, weatherdelay int, nasdelay int, securitydelay int, lateaircraftdelay int, year int)\nROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\n'\nSTORED AS TEXTFILE LOCATION 's3a://${storage}/course-data/meta-cdw-workshop/airlines-raw/airlines-csv/flights' tblproperties(\"skip.header.line.count\"=\"1\");\n\ndrop table if exists ${user_id}_airlines_raw.planes_csv;\nCREATE EXTERNAL TABLE ${user_id}_airlines_raw.planes_csv(tailnum string, owner_type string, manufacturer string, issue_date string, model string, status string, aircraft_type string, engine_type string, year int)\nROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\n'\nSTORED AS TEXTFILE LOCATION 's3a://${storage}/course-data/meta-cdw-workshop/airlines-raw/airlines-csv/planes' tblproperties(\"skip.header.line.count\"=\"1\");\n\ndrop table if exists ${user_id}_airlines_raw.airlines_csv;\nCREATE EXTERNAL TABLE ${user_id}_airlines_raw.airlines_csv(code string, description string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\n'\nSTORED AS TEXTFILE LOCATION 's3a://${storage}/course-data/meta-cdw-workshop/airlines-raw/airlines-csv/airlines' tblproperties(\"skip.header.line.count\"=\"1\");\n\ndrop table if exists ${user_id}_airlines_raw.airports_csv;\nCREATE EXTERNAL TABLE ${user_id}_airlines_raw.airports_csv(iata string, airport string, city string, state DOUBLE, country string, lat DOUBLE, lon DOUBLE)\nROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\\n'\nSTORED AS TEXTFILE LOCATION 's3a://${storage}/course-data/meta-cdw-workshop/airlines-raw/airlines-csv/airports' tblproperties(\"skip.header.line.count\"=\"1\");\n</code></pre> <p>Notice the following screenshot corresponding to the above SQL.  Makre sure to replace user_id &amp; storage.</p> <p></p> <ul> <li>Check all the 4 tables were created.</li> </ul> <pre><code>USE ${user_id}_airlines_raw;\n\nSHOW TABLES;\n</code></pre> <p>Make sure that 4 tables (<code>airlines_csv</code>, <code>airports_csv</code>, <code>flights_csv</code>, <code>planes_csv</code>) are created as shown below.</p> <p></p>"},{"location":"guide/04-03/index.html#33-run-the-following-ddl-in-hue-for-the-cdw-virtual-warehouse-vw-impala","title":"3.3 Run the following DDL in HUE for the CDW Virtual Warehouse - <code>vw-impala</code>.","text":"<ul> <li>Go to the page where now you will access HUE of an Impala virtual     warehouse. Click on <code>HUE</code> for <code>vw-impala</code> as shown in the     screenshot below. </li> </ul> <p>Now, copy paste the following in the HUE browser and click on Run as shown below.</p> <pre><code>select count(*) from ${user_id}_airlines_raw.flights_csv;\n</code></pre> <p></p> <p>Notice that while the query is executing, continue to the next step. Once the query returns you will see the following in the Results - <code>the flights_csv table has over 86 million records</code>. </p> <ul> <li>Go back to the CDP Console and observe the Impala Virtual Warehouse     <code>vw-impala</code>.    </li> </ul> <p>Here, you'll notice that the warehouse is now at a state where it is not executing any queries and hence, the node count would be low and as the users will run their queries it will scale up or down depending upon the need of resources or lack of it when queries are not run.</p> <p>Note</p> <pre><code>Since this workshop has many users logged in and the virtual impala warehouse is always ON at this point, the actual behavior might differ from what you see in the screenshot. The idea is to convey that the virtual warehouse scales up and scales down.`\n</code></pre> <ul> <li>Run the following query to start analyzing the data - \"Find the     needle in the haystack\" query.</li> </ul> <pre><code>SELECT model,\n       engine_type\nFROM ${user_id}_airlines_raw.planes_csv\nWHERE planes_csv.tailnum IN\n    (SELECT tailnum\n     FROM\n       (SELECT tailnum,\n               count(*),\n               avg(depdelay) AS avg_delay,\n               max(depdelay),\n               avg(taxiout),\n               avg(cancelled),\n               avg(weatherdelay),\n               max(weatherdelay),\n               avg(nasdelay),\n               max(nasdelay),\n               avg(securitydelay),\n               max(securitydelay),\n               avg(lateaircraftdelay),\n               max(lateaircraftdelay),\n               avg(airtime),\n               avg(actualelapsedtime),\n               avg(distance)\n        FROM ${user_id}_airlines_raw.flights_csv\n        WHERE tailnum IN ('N194JB',\n                          'N906S',\n                          'N575ML',\n                          'N852NW',\n                          'N000AA')\n        GROUP BY tailnum) AS delays);\n</code></pre> <p></p> <ul> <li> <p>Go back to the CDP console to observe the behavior of scaling     up/down of virtual warehouses.     </p> </li> <li> <p>Check in the Hue browser and the query shows the result as follows.     Observe the amount of time taken to run this query.     </p> </li> </ul>"},{"location":"guide/04-04/index.html","title":"04-04 Running minikube","text":"<p>The purpose of this exercise is to start up minicube and to learn basic commands for minikube.</p>"},{"location":"guide/04-04/index.html#1-open-minikube","title":"1. Open minikube","text":""},{"location":"guide/04-04/index.html#11-open-docker-desktop","title":"1.1. Open Docker Desktop","text":"<p>minikkube runs in a container. It requires a container environment. Start up your container environment. In this case the image is of Docker Desktop running on macOS.</p> <p></p>"},{"location":"guide/04-04/index.html#12-open-a-terminal-or-ide","title":"1.2. Open a Terminal or IDE","text":"<p>Open a terminal or an IDE. This exercise will use Visual Studio Code. Note the normal color setting for VSC is white on black. For the purposes of readability on a printed page this has been changed to black on white.</p> <p></p>"},{"location":"guide/04-04/index.html#13-start-minikube","title":"1.3. Start minikube","text":"<p>Start up minikube and track progress.</p> <pre><code>minikube start\n</code></pre> <p>Clear the screen.</p> <pre><code>clear\n</code></pre> <p></p>"},{"location":"guide/04-04/index.html#14-allow-access","title":"1.4. Allow Access","text":"<p>If required allow VSC to access data from other apps.</p> <p></p>"},{"location":"guide/04-04/index.html#2-basic-minikube-commands","title":"2. Basic minikube Commands","text":""},{"location":"guide/04-04/index.html#21-minikube-usage","title":"2.1. minikube Usage","text":"<pre><code>minikube --help\n</code></pre>"},{"location":"guide/04-04/index.html#22-pause-and-unpause","title":"2.2. Pause and Unpause","text":"<p>Pause and unpause minikube.</p> <pre><code>minikube pause\n</code></pre> <pre><code>minikube unpause\n</code></pre> <p></p>"},{"location":"guide/04-04/index.html#23-start-and-stop","title":"2.3. Start and Stop","text":"<pre><code>minikube stop\n</code></pre> <pre><code>minikube start\n</code></pre>"},{"location":"guide/04-04/index.html#24-logs-for-minikube","title":"2.4. Logs for minikube","text":"<pre><code>minikube logs\n</code></pre>"},{"location":"guide/04-04/index.html#25-list-addons","title":"2.5. List Addons","text":"<p>Review the addons list. Addons will be configured as required.</p> <pre><code>minikube addons list\n</code></pre> <p></p>"},{"location":"guide/04-04/index.html#26-enable-dashboard","title":"2.6. Enable Dashboard","text":"<pre><code>minikube addons enable dashboard\n</code></pre>"},{"location":"guide/04-04/index.html#27-open-kubernetes-dashboard","title":"2.7. Open Kubernetes Dashboard","text":"<p>Open the Kubernetes dashboard. A later exercise will review this dashboard.</p> <pre><code>minikube dashboard\n</code></pre> <p>When you are done close the tab for the Kubernetes dashboard. Use Ctrl-C to exit the command.</p> <p></p>"},{"location":"guide/04-04/index.html#3-deleting-minikube","title":"3. Deleting minikube","text":""},{"location":"guide/04-04/index.html#31-delete-minikube-clusters","title":"3.1. Delete minikube clusters","text":"<p>\u274c DO NOT DELETE</p> <p>Do not delete minikube at this time. </p> <p>To delete all minikube clusters use this command.</p> <pre><code>xx minikube delete --all\n</code></pre> <p></p>"},{"location":"guide/04-04/index.html#32-the-docker-desktop-warning","title":"3.2. The Docker Desktop Warning","text":"<p>Do not delete minikube from the Docker Desktop container view. This will result in a start up error for minikube. Always delete from the command line.</p> <p></p>"},{"location":"guide/04-04/index.html#4-verify-kubernetes","title":"4. Verify Kubernetes","text":""},{"location":"guide/04-04/index.html#41-test-for-the-kubectl-command","title":"4.1. Test for the kubectl Command","text":"<p>The kubectl command should automatically be available. If it does not appear ask your instructor for an assist.</p> <pre><code>kubectl get pods -A\n</code></pre> <p></p>"},{"location":"guide/04-04/index.html#42-run-test-commands-to-create-a-deployment","title":"4.2. Run Test Commands to Create a Deployment","text":"<p>Create a Deployment with a simple image.</p> <pre><code>kubectl create deployment hello-minikube --image=kicbase/echo-server:1.0\n</code></pre> <pre><code>kubectl expose deployment hello-minikube --type=NodePort --port=8080\n</code></pre> <p>These commands are also found in the = Install minikube web page.</p> <p></p>"},{"location":"guide/04-04/index.html#43-list-deployment-and-service","title":"4.3. List Deployment and Service","text":"<p>Inspect the deployment</p> <pre><code>kubectl get deployment hello-minikube\n</code></pre> <pre><code>kubectl get service hello-minikube\n</code></pre> <p></p>"},{"location":"guide/04-04/index.html#44-open-the-application","title":"4.4. Open the Application","text":"<p>\u2139\ufe0f Terminal Status</p> <p>The terminal must be left open to run the web page.</p> <p>Use minkube to open the browser.</p> <pre><code>minikube service hello-minikube\n</code></pre> <p></p>"},{"location":"guide/04-04/index.html#45-review-the-web-page","title":"4.5. Review the Web Page","text":"<p>Review the web page. When done close the tab. In the IDE use Ctrl-C to escape the minikube service tunnel. </p> <pre><code>clear\n</code></pre> <p></p>"},{"location":"guide/04-04/index.html#46-delete-deployment-and-service","title":"4.6. Delete Deployment and Service","text":"<pre><code>kubectl delete deployment hello-minikube\n</code></pre> <pre><code>kubectl delete service hello-minikube\n</code></pre>"},{"location":"guide/04-04/index.html#47-verification-complete","title":"4.7. Verification Complete","text":"<p>The minikube cluster is now tested and ready for further exercises.</p> <p></p>"},{"location":"guide/04-04/index.html#5-review-minikube-documentation","title":"5. Review minikube Documentation","text":""},{"location":"guide/04-04/index.html#51-open-minikube-web-ui","title":"5.1. Open minikube Web UI","text":"<p>Open the minikube handbook. Scan through it to see the full range of commands and capabilities. The handbook could be used as a tutorial.</p> <pre><code>https://minikube.sigs.k8s.io/docs/handbook/\n</code></pre> <p></p>"},{"location":"guide/04-04/index.html#52-review-documentation","title":"5.2. Review Documentation","text":"<p>Select Addons &gt; Ingress DNS. This is a good example of the value of this documentation. It provides a clear explaination with excellent example commands. The commands are demostrated for Linux, macOS, and Windows.</p> <p></p>"},{"location":"guide/04-04/index.html#6-end-of-exercise","title":"6. End of Exercise","text":""},{"location":"guide/04-05/index.html","title":"Step 5: Performance Optimizations &amp; Table maintenance Using Impala VW","text":"<p>In this Step we will look at some of the performance optimization and table maintenance tasks that can be performed to ensure the best possible TCO, while ensuring the best performance.</p>"},{"location":"guide/04-05/index.html#51-iceberg-in-place-partition-evolution-performance-optimization","title":"5.1 Iceberg in-place Partition Evolution [Performance Optimization]","text":"<ul> <li> <p>Open HUE for the CDW <code>Hive</code> Virtual Warehouse - <code>vw-hive</code> </p> </li> <li> <p>One of the key features for Iceberg tables is the ability to evolve     the partition that is being used over time.</p> </li> </ul> <pre><code>ALTER TABLE ${user_id}_airlines.flights\nSET PARTITION spec ( year, month );\n\nSHOW CREATE TABLE ${user_id}_airlines.flights;\n</code></pre> <p></p> <ul> <li> <p>Check for the following where now the partition is by     <code>year, month</code>.     </p> </li> <li> <p>Load new data into the flights table using the NEW partition     definition. <code>This query will take a while to run</code>.</p> </li> </ul> <pre><code>INSERT INTO ${user_id}_airlines.flights\nSELECT * FROM ${user_id}_airlines_raw.flights_csv\nWHERE year = 2007;\n</code></pre> <p></p> <ul> <li> <p>Open HUE for the CDW <code>Impala</code> Virtual Warehouse - <code>impala-vw</code>.     </p> <p></p> </li> <li> <p>Copy/paste the following in the HUE Editor, but <code>DO NOT</code> execute     the query.</p> </li> </ul> <pre><code>SELECT year, month, count(*)\nFROM ${user_id}_airlines.flights\nWHERE year = 2006 AND month = 12\nGROUP BY year, month\nORDER BY year desc, month asc;\n</code></pre> <ul> <li> <p>Run <code>Explain Plans</code> against some typical analytic queries we might     run to see what happens with this new Partition.     </p> <p></p> </li> <li> <p>Copy/paste the following in the HUE Editor, but <code>DO NOT</code> execute     the query.</p> </li> </ul> <pre><code>SELECT year, month, count(*)\nFROM ${user_id}_airlines.flights\nWHERE year = 2007 AND month = 12\nGROUP BY year, month\nORDER BY year desc, month asc;\n</code></pre> <ul> <li>Run <code>Explain Plans</code> against some typical analytic queries we might     run to see what happens with this new Partition.     </li> </ul> <p>In the output notice the amount of data that needs to be scanned for this query, about 11 MB, is significantly less than that of the first, 138 MB. This shows an important capability of Iceberg, Partition Pruning. Meaning that much less data is scanned for this query and only the selected month of data needs to be processed. This should result in much faster query execution times. </p>"},{"location":"guide/04-05/index.html#52-iceberg-snapshots-table-maintenance","title":"5.2 Iceberg Snapshots [Table Maintenance]","text":"<ul> <li>In the previous steps we have loaded data into the <code>flights</code> iceberg     table. We will insert more data into it. Each time we add (update or     delete) data a <code>snapshot</code> is captured. The snapshot is important for     <code>eventual consistency</code> &amp; to allow multiple read/writes concurrently     (from various engines or the same engine).</li> </ul> <pre><code>INSERT INTO ${user_id}_airlines.flights\nSELECT * FROM ${user_id}_airlines_raw.flights_csv\nWHERE year &gt;= 2008;\n</code></pre> <ul> <li>To see snapshots, execute the following SQL.</li> </ul> <pre><code>DESCRIBE HISTORY ${user_id}_airlines.flights;\n</code></pre> <p>In the output there should be 3 Snapshots, described below. Note that we have been reading/writing data from/to the Iceberg table from both Hive &amp; Impala. It is an important aspect of Iceberg Tables that they support <code>multi-function analytics</code> - ie. many engines can work with Iceberg tables (<code>Cloudera Data Warehouse [Hive &amp; Impala]</code>, <code>Cloudera Data Engineering [Spark]</code>, <code>Cloudera Machine Learning [Spark]</code>, <code>Cloudera DataFlow [NiFi]</code>, and <code>DataHub Clusters</code>).</p> <ul> <li> <p>Get the details of the <code>snapshots</code> and store it in a notepad.     </p> <p></p> </li> </ul>"},{"location":"guide/04-05/index.html#53-iceberg-time-travel-table-maintenance","title":"5.3 Iceberg Time Travel [Table Maintenance]","text":"<ul> <li>Copy/paste the following data into the Impala Editor, but     <code>DO NOT</code> execute.</li> </ul> <pre><code>-- SELECT DATA USING TIMESTAMP FOR SNAPSHOT\nSELECT year, count(*)\nFROM ${user_id}_airlines.flights\n  FOR SYSTEM_TIME AS OF '${create_ts}'\nGROUP BY year\nORDER BY year desc;\n\n-- SELECT DATA USING SNAPSHOT ID FOR SNAPSHOT\nSELECT year, count(*)\nFROM ${user_id}_airlines.flights\n  FOR SYSTEM_VERSION AS OF ${snapshot_id}\nGROUP BY year\nORDER BY year desc;\n</code></pre> <ul> <li> <p>After copying you will see 2 parameters as below.     </p> </li> <li> <p>From the notepad just copy the first value of the timestamp. It     could be the date or the timestamp. Paste it in the <code>create_ts</code> box.     In our case the value was <code>2025-02-07 11:06:25.905000000</code>. Then     execute the highlighted query only (1st query).      </p> </li> <li> <p>From the notepad just copy the second value of the snapshot id. In     our case the value was <code>1796099219985023033</code>. Paste it in the     <code>snapshot_id</code> box. Then execute the highlighted query only (2nd     query).      </p> </li> </ul>"},{"location":"guide/04-05/index.html#54-dont-run-fyi-only-iceberg-rollback-table-maintenance","title":"5.4 (Don't Run, FYI ONLY) - Iceberg Rollback [Table Maintenance]","text":"<ul> <li>Sometimes data can be loaded incorrectly, due to many common     issues - missing fields, only part of the data was loaded, bad data,     etc. In situations like this data would need to be removed,     corrected, and reloaded. Iceberg can help with the Rollback command     to remove the \"unwanted\" data. This leverages Snapshot IDs to     perform this action by using a simple ALTER TABLE command as     follows. We will <code>NOT RUN</code> this command in this lab.</li> </ul> <pre><code>-- ALTER TABLE ${user_id}_airlines.flights EXECUTE ROLLBACK(${snapshot_id});\n</code></pre>"},{"location":"guide/04-05/index.html#55-dont-run-fyi-only-iceberg-rollback-table-maintenance","title":"5.5 (Don't Run, FYI ONLY) - Iceberg Rollback [Table Maintenance]","text":"<ul> <li>As time passes it might make sense to expire old Snapshots, instead     of the Snapshot ID you use the Timestamp to expire old Snapshots.     You can do this manually by running a simple ALTER TABLE command as     follows. We will <code>NOT RUN</code> this command in this lab.</li> </ul> <pre><code>-- Expire Snapshots up to the specified timestamp\n-- BE CAREFUL: Once you run this you will not be able to Time Travel for any Snapshots that you Expire ALTER TABLE ${user_id}_airlines.flights\n-- ALTER TABLE ${user_id}_airlines_maint.flights EXECUTE expire_snapshots('${create_ts}');\n</code></pre>"},{"location":"guide/04-05/index.html#56-materialized-views-performance-optimization","title":"5.6 Materialized Views [Performance Optimization]","text":"<ul> <li>This can be used for both Iceberg tables and Hive Tables to improve     performance. Go to the Cloudera console and look for <code>hive-vw</code>.     Click on the <code>Hue</code> button on the right upper corner of <code>hive-vw</code> as     shown in the screenshot below.</li> </ul> <ul> <li>Up until this point we had <code>airlines</code> table which was (Hive + orc).     Now, we shall create the airlines table which is (Iceberg + orc).     Copy/paste the following, make sure to highlight the entire block,     and execute the following.</li> </ul> <pre><code>SET hive.query.results.cache.enabled=false;\n\ndrop table  if exists ${user_id}_airlines.airlines;\nCREATE EXTERNAL TABLE ${user_id}_airlines.airlines (code string, description string) STORED BY ICEBERG STORED AS ORC TBLPROPERTIES ('format-version' = '2');\n\nINSERT INTO ${user_id}_airlines.airlines SELECT * FROM ${user_id}_airlines_raw.airlines_csv;\n\nSELECT airlines.code AS code,  MIN(airlines.description) AS description,\n          flights.month AS month,\n          sum(flights.cancelled) AS cancelled\nFROM ${user_id}_airlines.flights flights , ${user_id}_airlines.airlines airlines\nWHERE flights.uniquecarrier = airlines.code\ngroup by airlines.code, flights.month;\n</code></pre> <p>Note: Hive has built in performance improvements, such as a Query Cache that stores results of queries run so that similar queries don't have to retrieve data, they can just get the results from the cache. In this step we are turning that off using the SET statement, this will ensure when we look at the query plan, we will not retrieve the data from the cache. Note: With this query you are combining an Iceberg Table Format (<code>flight</code> table) with a Hive Table Format (<code>airlines ORC</code> table) in the same query.</p> <ul> <li>Let's look at the Query Plan that was used to execute this query. On     the left side click on <code>Jobs</code>, as shown in the screenshot below.</li> </ul> <p></p> <ul> <li>Then click on <code>Hive Queries</code>. This is where an Admin will go when he     wants to investigate the queries. In our case for this lab, we'd     like to look at the query we just executed to see how it ran and the     steps taken to execute the query. Administrators would also be able     to perform other monitoring and maintenance tasks for what is     running (or has been run). Monitoring and maintenance tasks could     include cancel (kill) queries, see what is running, analyze whether     queries that have been executed are optimized, etc.</li> </ul> <p></p> <ul> <li> <p>Click on the first query as shown below. Make sure that this is the     latest query. You can look at the <code>Start Time</code> field here to know if     it's the latest or not.     </p> </li> <li> <p>This is where you can analyze queries at a deep level. For this lab     let's take a look at the explain details, by clicking on     <code>Visual Explain</code> tab. It might take a while to appear, please click     on refresh.     </p> </li> <li> <p>This plan shows that this query needs to read <code>flights</code> (86M rows)     and <code>airlines</code> (1.5K rows) with hash join, group, and sort. This is     a lot of data processing and if we run this query constantly it     would be good to reduce the time this query takes to execute.     </p> </li> <li> <p>Click on the <code>Editor</code> option on the left side as shown.     </p> </li> <li> <p>Create Materialized View (MV) - Queries will transparently be     rewritten, when possible, to use the MV instead of the base tables.     Copy/paste the following, highlight the entire block, and execute.</p> </li> </ul> <pre><code>DROP MATERIALIZED VIEW IF EXISTS ${user_id}_airlines.traffic_cancel_airlines;\nCREATE MATERIALIZED VIEW ${user_id}_airlines.traffic_cancel_airlines\nas SELECT airlines.code AS code,  MIN(airlines.description) AS description,\n          flights.month AS month,\n          sum(flights.cancelled) AS cancelled,\n          count(flights.diverted) AS diverted\nFROM ${user_id}_airlines.flights flights JOIN ${user_id}_airlines.airlines airlines ON (flights.uniquecarrier = airlines.code)\ngroup by airlines.code, flights.month;\n\n-- show MV\nSHOW MATERIALIZED VIEWS in ${user_id}_airlines;\n</code></pre> <p></p> <ul> <li>Run Dashboard Query again to see usage of the MV - Copy/paste the     following, make sure to highlight the entire block, and execute the     following. This time an <code>order by</code> was added to make this query must     do more work.</li> </ul> <pre><code>SET hive.query.results.cache.enabled=false;\nSELECT airlines.code AS code,  MIN(airlines.description) AS description,\n          flights.month AS month,\n          sum(flights.cancelled) AS cancelled\nFROM ${user_id}_airlines.flights flights , ${user_id}_airlines.airlines airlines\nWHERE flights.uniquecarrier = airlines.code\ngroup by airlines.code, flights.month\norder by airlines.code;\n</code></pre> <p></p> <ul> <li>Let's look at the Query Plan that was used to execute this query. On     the left menu select <code>Jobs</code>. On the Jobs Browser - select the     <code>Queries</code> tab to the right of the <code>Job</code> browser header. Hover over &amp;     click on the Query just executed (should be the first row). Click on     the <code>Visual Explain</code> tab. With query rewrite the materialized view     is used and the new plan just reads the MV and sorts the data vs.     reading <code>flights (86M rows)</code> and <code>airlines (1.5K rows)</code> with hash     join, group and sorts. This results in significant reduction in run     time for this query.</li> </ul> <p></p>"},{"location":"guide/04-06/index.html","title":"Step 6: Cloudera Data Visualization","text":"<p>In this step you will build a Logistics Dashboard using Cloudera Data Visualization. The Dashboard will include details about flight delays and cancellations. But first we will start with Data Modeling.</p>"},{"location":"guide/04-06/index.html#step-6a-data-modeling","title":"Step 6(a): Data Modeling","text":"<ul> <li> <p>If you are not on the CDP home page, then go there and click on the     following CDW icon to go into Cloudera Data Warehouse.     </p> </li> <li> <p>Then click on the Data Visualization option in the left window pane. You\u2019ll see an option Data VIZ next to the data-viz application with the name dataviz. It should open a new window.+</p> </li> </ul> <p></p> <ul> <li> <p>There are 4 areas of CDV - <code>HOME, SQL, VISUALS, DATA</code> - these are     the tabs at the top of the screen in the black bar to the right of     the Cloudera Data Visualization banner. </p> </li> <li> <p>Build a Dataset (aka. Metadata Layer or Data Model) - click on     <code>DATA</code> in the top banner. A Dataset is a Semantic Layer where you     can create a business view on top of the data - data is not copied;     this is just a logical layer.     </p> </li> <li> <p>Create a connection - click on the NEW CONNECTION button on the left     menu. Enter the details as shown in the screenshot and click on     <code>TEST</code>.</p> <p>Connection type: Select <code>CDW Impala</code>.</p> <p>Connection name: <code>&lt;user_id&gt;-airlines-lakehouse</code> (Ex-<code>wuser00-airlines-lakehouse</code>).</p> <p>CDW Warehouse: <code>Make Sure you select the warehouse that is associated with your &lt;user_id&gt;</code>.</p> <p>Hostname or IP address: Gets automatically selected.</p> <p>Port: <code>Gets automatically filled up</code>.</p> <p>Username: <code>Gets automatically filled up</code>.</p> <p>Password: <code>Blank</code></p> </li> </ul> <p> </p> <ul> <li> <p>Click on <code>CONNECT</code>.     </p> </li> <li> <p>You will see your connection in the list of connections on the left     menu. On the right side of the screen you will see Datasets and the     Connection Explorer. Click on <code>NEW DATASET</code>.      </p> </li> <li> <p>Fill the details as follows and click <code>CREATE</code>. <code>airline_logistics</code>     gets created</p> <p>Dataset title - <code>airline_logistics</code>.</p> <p>Dataset Source - Select <code>From Table</code> (however, you could choose to directly enter a SQL statement instead).</p> <p>Select Database - <code>&lt;user_id&gt;_airlines</code> (Make Sure you select the database that is associated with your \\&lt;user_id&gt;).</p> <p>Select Table - <code>flights</code>.</p> </li> </ul> <p></p> <ul> <li> <p>Edit the Dataset - click on <code>airline_logistics</code> on the right of the     screen. This will open the details page, showing you information     about the Dataset, such as connection details, and options that are     set. Click on <code>Fields</code> option in the left window pane.      </p> </li> <li> <p>Click on <code>Data Model</code> - for our Dataset we need to join additional     data to the flights table including the <code>planes</code>, <code>airlines</code>, and     <code>airports</code> tables.     </p> </li> <li> <p>Click on <code>EDIT DATA MODEL</code>.     </p> </li> <li> <p>Click on the <code>+</code> icon next to the <code>flights</code> table option.     </p> </li> <li> <p>Select the appropriate <code>Database Name</code> based on your user id (Ex:     <code>wuser00_airlines</code>) and table name <code>planes</code>.     </p> </li> <li> <p>Then click on the  <code>join</code> icon     and see that there are 2 join options <code>tailnum</code> &amp; <code>year</code>. </p> <p>Click on <code>EDIT JOIN</code> and then remove the <code>year</code> join by clicking the little <code>-</code> (minus) icon to the right next to the <code>year</code> column. </p> <p>Click on <code>APPLY</code>.</p> <p> </p> </li> <li> <p>Now we will create a join between another table. </p> <p>Click on <code>+</code> icon next to <code>flights</code> as shown below.</p> <p>Select the appropriate <code>Database Name</code> based on your &lt;user_id&gt; (Ex: <code>wuser00_airlines</code>) and table name <code>airlines</code>.  </p> </li> <li> <p>Make sure you select the column <code>uniquecarrier</code> from <code>flights</code> and     column <code>code</code> from the <code>airlines</code> table. Click <code>APPLY</code>.      </p> </li> <li> <p>Click on <code>+</code> icon next to <code>flights</code> as shown below. Select the     appropriate <code>Database Name</code> based on your &lt;user_id&gt; (Ex:     <code>wuser00_airlines</code>) and table name <code>airports</code>.     </p> </li> <li> <p>Make sure you select the column <code>origin</code> from <code>flights</code> and column     <code>iata</code> from the <code>airports</code> table. Click <code>APPLY</code>.     </p> </li> <li> <p>Click on <code>+</code> icon next to <code>flights</code> as shown below. Select the     appropriate <code>Database Name</code> based on your &lt;user_id&gt; (Ex:     <code>wuser00_airlines</code>) and table name <code>airports</code>.     </p> </li> <li> <p>Make sure you select the column <code>dest</code> from <code>flights</code> and column     <code>iata</code> from the <code>airports</code> table. Click <code>APPLY</code>. Then click on     <code>SAVE</code>.      </p> </li> <li> <p>Verify that you have the joins which are as following. </p> <p>You can do so by clicking the  <code>join</code> icon.</p> <p><code>flights.tailnum</code>\u2009---\u2009<code>planes.tailnum</code></p> <p><code>flights.uniquecarrier</code>\u2009---\u2009<code>airlines.code</code></p> <p><code>flights.origin</code>\u2009---\u2009<code>airports.iata</code></p> <p><code>flights.dest</code>\u2009---\u2009<code>airports_1.iata</code></p> </li> <li> <p>Click on <code>SHOW DATA</code>. And then click on <code>SAVE</code>.  </p> </li> <li> <p>Click on the <code>Fields</code> column on the left window pane. Then click on     <code>EDIT FIELDS</code>. Make sure that you click on the highlighted area to     change <code>#</code> (measures icon) next to each column to <code>Dim</code> (dimension     icon). The columns are as follows.</p> <p>a.  <code>flights</code> table: Columns (<code>month</code>, <code>dayofmonth</code>, <code>dayofweek</code>,     <code>deptime</code>, <code>crsdeptime</code>, <code>arrtime</code>, <code>crsarrtime</code>, <code>flightnum</code> &amp;     <code>year</code>)</p> <p>b.  <code>planes</code> table: <code>All columns</code></p> <p>c.  <code>airports</code> table: <code>All columns</code></p> <p>d.  <code>airports_1</code> table: <code>All columns</code></p> </li> </ul> <p> </p> <ul> <li> <p>Click on <code>TITLE CASE</code>. And notice that the column names changes to     be <code>Camel case</code>. Click on the <code>pencil</code> icon next to the <code>Depdelay</code>     icon.      </p> </li> <li> <p>Change the <code>Default Aggregation</code> to <code>Average</code>. Click on the     <code>Display Format</code> and then change <code>Category</code> to be <code>Integer</code>. Check     mark the box next to the <code>Use 1000 separator</code>. Click on <code>APPLY</code>.      </p> </li> <li> <p>Click on the <code>down arrow</code> shown against the <code>Origin</code> column and then     click on <code>Clone</code>. A column <code>Copy of Origin</code> is created. Click on the     <code>pencil</code> icon next to it.      </p> </li> <li> <p>Change the <code>Display Name</code> to <code>Route</code>. Then click on <code>Expression</code> and     enter the following in the <code>Expression</code> editor. Click on <code>APPLY</code>.</p> </li> </ul> <pre><code>concat([Origin], '-', [Dest])\n</code></pre> <p> </p> <ul> <li>Click on <code>SAVE</code>. We have completed the step of data modeling and now     we will create data visualization.     </li> </ul>"},{"location":"guide/04-06/index.html#step-6b-creating-dashboard","title":"Step 6(b): Creating Dashboard","text":"<ul> <li>Now we will create a dashboard page based on the data model that we     just created. Click on <code>NEW DASHBOARD</code>.</li> </ul> <ul> <li> <p>You will see the following. </p> </li> <li> <p>A quick overview of the screen that you are seeing is as follows.</p> <p>On the right side of the screen there will be a VISUALS menu.</p> <p>At the top of this menu, there is a series of Visual Types to choose from.</p> <p>There will be 30+ various visuals to choose from.</p> <p>Below the Visual Types you will see what are called Shelves. The Shelves that are present depend on the Visual Type that is selected. Shelves with a <code>*</code> are required, all other Shelves are optional. </p> <p>On the far right of the page there is a DATA menu, which identifies the Connection &amp; Dataset used for this visual.  Underneath that is the Fields from theDataset broken down by Dimensions and Measures. With each of these Categories you can see that it is subdivided by each Table in the Dataset.  </p> </li> <li> <p>Let's build the 1st visual - <code>Top 25 Routes by Avg Departure Delay</code>.     CDV will add a Table visual displaying a sample of the data from the     Dataset as the default visualization when you create a new Dashboard     or new Visuals on the Dashboard (see New Dashboard screen above).     The next step is to modify (Edit) the default visualization to suit     your needs.</p> </li> <li> <p>Pick the Visual Type - Select the <code>Stacked Bar</code> chart visual on the     top right as shown below. Make sure <code>Build</code> is selected for it to     appear on the right side.     </p> </li> <li> <p>Find <code>Route</code> under <code>Dimensions \u2192 flights</code>. Drag to <code>X-Axis</code>.     Similarly, find <code>DepDelay</code> under <code>Measures \u2192 flights</code>. Drag to     <code>Y-Axis</code>. By default the aggregation selected is average and hence     you would see <code>avg(Depdelay)</code>.     </p> </li> <li> <p>Click on the arrow next to <code>avg(Depdelay)</code>. Enter <code>50</code> against the     text box labeled <code>Top K</code>. Click on <code>REFRESH VISUAL</code>.      </p> </li> <li> <p>Click <code>enter title</code> and enter the title based on your user id as -     <code>&lt;user_id&gt;- Routes with Highest Avg. Departure Delays</code>. Then click     on <code>SAVE</code>. </p> </li> </ul>"},{"location":"guide/04-06/index.html#_1","title":"Step 6: Cloudera Data Visualization","text":""},{"location":"guide/05-01/index.html","title":"05-01 Microservices","text":""},{"location":"guide/05-01/index.html#1-introduction","title":"1. Introduction","text":""},{"location":"guide/05-01/index.html#2-containerized-applications","title":"2. Containerized Applications","text":""},{"location":"guide/05-01/index.html#3-build-and-run-applications","title":"3. Build and Run Applications","text":""},{"location":"guide/05-01/index.html#4-repositories-and-registries","title":"4. Repositories and Registries","text":""},{"location":"guide/05-01/index.html#5-docker","title":"5. Docker","text":""},{"location":"guide/05-01/index.html#6-summary-and-exercise-assignments","title":"6. Summary and Exercise Assignments","text":""},{"location":"guide/05-02/index.html","title":"05-02 Running Docker Containers","text":"<p>The purpose of this exercise is to learn the basic Docker commands for managing images and containers.</p>"},{"location":"guide/05-02/index.html#1-review-docker-hub","title":"1. Review Docker Hub","text":""},{"location":"guide/05-02/index.html#11-open-docker-hub","title":"1.1. Open Docker Hub","text":"<p>Docker Hub is one of the primary public registries for Docker images. In your browser click + to add a new tab. Insert the</p> <pre><code>https://hub.docker.com\n</code></pre> <p>Click** return. **</p> <p></p>"},{"location":"guide/05-02/index.html#12-search-for-software","title":"1.2. Search for Software","text":"<p>Search for nginx. This page lists the available downloads. The official version is approved by both Docker and the nginx community.</p> <p></p>"},{"location":"guide/05-02/index.html#13-open-nginx-repo","title":"1.3. Open nginx Repo","text":"<p>Click on nginx Docker Official Image. Review the list of versions. Identify one, for example 1.27.4. </p> <p>\u2139\ufe0f Latest</p> <p>The latest version can always be downloaded with nginx:latest</p> <p></p>"},{"location":"guide/05-02/index.html#14-open-a-version","title":"1.4. Open a Version","text":"<p>Open a version. This generally will take you to GitHub. This is where the source code is located. </p> <p></p>"},{"location":"guide/05-02/index.html#15-clean-up","title":"1.5. Clean Up","text":"<p>Close out of the browser tabs.</p>"},{"location":"guide/05-02/index.html#2-pull-docker-images","title":"2. Pull Docker Images","text":""},{"location":"guide/05-02/index.html#21-list-available-images","title":"2.1. List Available Images","text":"<p>This syntax lists the current available docker images. There are two Dockers images at the moment. These are prebuilt for the classroom environment.</p> <pre><code>docker images\n</code></pre> <p>There may be additional docker images, but these may be ignored at this point in time.</p> <p></p>"},{"location":"guide/05-02/index.html#22-list-available-images-on-docker-hub","title":"2.2. List Available Images on Docker Hub","text":"<p>This syntax will list the available versions for the nginx software on Docker Hub.</p> <pre><code>docker search nginx\n</code></pre> <p></p>"},{"location":"guide/05-02/index.html#23-pull-nginx-image","title":"2.3. Pull nginx Image","text":"<p>The nginx software is a populate a web server. It can also be used a reverse proxy, load balancer, and HTTP cache. This syntax will pull the latest version.</p> <pre><code>docker pull nginx\n</code></pre> <p>Notice the download comes in layers, each a separate download.</p> <p></p>"},{"location":"guide/05-02/index.html#24-list-images","title":"2.4. List images","text":"<p>The Docker image list now shows nginx.</p> <pre><code>docker images\n</code></pre> <p></p>"},{"location":"guide/05-02/index.html#25-pull-a-specific-software-version","title":"2.5. Pull a Specific Software Version","text":"<p>Select a version from the search list and then pull the version down. Images are pulled into a local cache.</p> <pre><code>docker pull nginx:1.23\n</code></pre> <p></p>"},{"location":"guide/05-02/index.html#26-list-the-available-images","title":"2.6. List the Available Images","text":"<p>Notice the listing of the different versions of the software. Compare the two versions of nginx. This is one of the powers of using docker images.</p> <pre><code>docker images\n</code></pre> <p></p>"},{"location":"guide/05-02/index.html#3-manage-images","title":"3. Manage Images","text":""},{"location":"guide/05-02/index.html#31-delete-an-image","title":"3.1. Delete an Image","text":"<p>Delete the older version of nginx from the local cache. Copy and paste the IMAGE ID from the terminal.</p> <pre><code>docker images\n</code></pre> <pre><code>docker rmi &lt;image_id&gt;\n</code></pre> <p></p>"},{"location":"guide/05-02/index.html#4-run-an-image-as-a-container","title":"4. Run an Image as a Container","text":""},{"location":"guide/05-02/index.html#41-view-running-docker-containers","title":"4.1. View Running Docker Containers","text":"<p>This syntax lists current running docker containers. There are no containers running at this moment.</p> <pre><code>docker ps\n</code></pre> <p></p>"},{"location":"guide/05-02/index.html#42-start-a-docker-container","title":"4.2. Start a Docker Container","text":"<p>When running a Docker container it will run in the foreground and capture the terminal. </p> <pre><code>docker run nginx\n</code></pre> <p></p>"},{"location":"guide/05-02/index.html#43-exit-the-running-container","title":"4.3. Exit the Running Container","text":"<p>Review the output from the start up of nginx. This can be useful for troubleshooting. Exit with a Crtl-C.</p> <p></p>"},{"location":"guide/05-02/index.html#44-run-a-docker-container-in-detach-mode","title":"4.4. Run a Docker Container in Detach Mode","text":"<p>The option -d | --detach will run the container in the background of the terminal. Notice the image ID is a long HEX number.</p> <p>docker run -d | --detach</p> <pre><code>docker run -d nginx\n</code></pre> <p></p>"},{"location":"guide/05-02/index.html#45-list-the-running-docker-containers","title":"4.5. List the Running Docker Containers","text":"<p>Docker ps shows running containers. </p> <pre><code>docker ps\n</code></pre> <p>The table for container has:</p> <ul> <li>Container ID</li> <li>Image used to create the container</li> <li>Command to start the application</li> <li>Created time</li> <li>Status</li> <li>Ports</li> <li>Useful Name (not displayed in snapshot.</li> </ul> <p></p>"},{"location":"guide/05-02/index.html#5-start-and-stop-containers","title":"5. Start and Stop Containers","text":""},{"location":"guide/05-02/index.html#51-stop-a-running-container","title":"5.1. Stop a Running Container","text":"<p>Copy and paste the CONTAINER ID to stop the container.</p> <pre><code>docker stop &lt;container_id&gt;\n</code></pre> <p></p>"},{"location":"guide/05-02/index.html#52-list-all-containers","title":"5.2. List All Containers","text":"<p>This lists both the running and the stopped containers. Notice the status of Exited. This means the container is stopped.</p> <pre><code>docker ps -a\n</code></pre> <p></p>"},{"location":"guide/05-02/index.html#53-start-a-container","title":"5.3. Start a Container","text":"<p>Copy and paste the CONTAINER ID to start a nginx container. </p> <pre><code>docker start &lt;container_id&gt;\n</code></pre> <p></p>"},{"location":"guide/05-02/index.html#54-start-a-second-container","title":"5.4. Start a Second Container","text":"<p>Copy and paste the NAME to start another nginx container. You may have to scroll to the end of the line. Docker uses a convention of adverb and name to create random titles for containers.</p> <pre><code>docker start &lt;container_name&gt;\n</code></pre> <p></p>"},{"location":"guide/05-02/index.html#55-list-running-containers","title":"5.5. List Running Containers","text":"<p>Both containers are running.</p> <pre><code>docker ps\n</code></pre> <p></p>"},{"location":"guide/05-02/index.html#6-delete-a-container","title":"6. Delete a Container","text":"<p>Docker containers must be stopped before you can delete the container.</p>"},{"location":"guide/05-02/index.html#61-stop-a-running-container","title":"6.1. Stop a Running Container","text":"<pre><code>docker stop &lt;container_id&gt;\n</code></pre>"},{"location":"guide/05-02/index.html#62-delete-a-stopped-container","title":"6.2. Delete a Stopped Container","text":"<pre><code>docker rm &lt;container_id&gt;\n</code></pre>"},{"location":"guide/05-02/index.html#63-verify-the-container-is-removed","title":"6.3. Verify the Container is Removed","text":"<pre><code>docker ps -a\n</code></pre>"},{"location":"guide/05-02/index.html#7-add-a-name-to-a-container","title":"7. Add a Name to a Container","text":"<p>Docker adds a random name to every containers. It is generally two part name, one an adverb and the other a noun, such as kickass-mcnulty.</p>"},{"location":"guide/05-02/index.html#71-start-a-container-with-a-name","title":"7.1. Start a Container with a Name","text":"<p>The option -n | --name will add a specific name to a container.</p> <pre><code>docker run -d --name app-frontend  nginx\n</code></pre> <p></p>"},{"location":"guide/05-02/index.html#72-list-running-containers","title":"7.2. List Running Containers","text":"<p>Verify the name of the container is app-frontend.  You may have to scroll to the end of the line. Assigning a name will make managing containers significantly easier.</p> <pre><code>docker ps\n</code></pre> <p></p>"},{"location":"guide/05-02/index.html#73-stop-nginx-containers","title":"7.3. Stop nginx Containers","text":"<p>Stop all running containers. One will use the assigned name and the other will use the CONTAINER ID.</p> <pre><code>docker stop app-frontend &lt;container_id&gt;\n</code></pre> <p></p>"},{"location":"guide/05-02/index.html#74-remove-nginx-containers","title":"7.4. Remove nginx Containers","text":"<p>Delete all stopped containers.</p> <pre><code>sudo docker rm app-frontend &lt;container_id&gt;\n</code></pre> <p></p>"},{"location":"guide/05-02/index.html#8-map-a-port-to-a-container","title":"8. Map a Port to a Container","text":""},{"location":"guide/05-02/index.html#81-map-a-port-to-a-container","title":"8.1. Map a Port to a Container","text":"<p>Container start on an internal Docker network. There is no direct access. The option -p | --publish : will map a port to a container. This will provide access to the container. <p>docker run --name  -d | --detach -p | --publish  image:id <pre><code>docker run --detach --name app-frontend --publish 8080:80 nginx\n</code></pre> <p></p>"},{"location":"guide/05-02/index.html#82-list-running-containers","title":"8.2. List Running Containers","text":"<p>Notice the PORTS addressing and the NAMES.</p> <pre><code>docker ps\n</code></pre> <p></p>"},{"location":"guide/05-02/index.html#9-open-nginx","title":"9. Open nginx","text":""},{"location":"guide/05-02/index.html#91-open-nginx","title":"9.1. Open nginx","text":"<p>Open a broswer tab, click +, and enter the URL:</p> <pre><code>http://localhost:8080\n</code></pre> <p></p>"},{"location":"guide/05-02/index.html#10-view-container-logs","title":"10. View Container Logs","text":""},{"location":"guide/05-02/index.html#101-view-the-container-logs","title":"10.1. View the Container Logs","text":"<p>View the docker logs for the web container. Notice the comments about the docker enterpoint.</p> <pre><code>docker logs app-frontend\n</code></pre> <p></p>"},{"location":"guide/05-02/index.html#11-open-shell-in-the-container","title":"11. Open Shell in the Container","text":""},{"location":"guide/05-02/index.html#111-open-shell-in-the-container","title":"11.1. Open Shell in the Container","text":"<p>A useful capability is to log into the container. Run a Docker container with /bin/bash to open a terminal within the container. The -i option attaches stdin and stdout. The -t option allocates a terminal.</p> <pre><code>docker run --name nginx-shell -i -t nginx /bin/bash\n</code></pre> <p></p>"},{"location":"guide/05-02/index.html#112-run-shell-commands-in-container","title":"11.2. Run Shell Commands in Container","text":"<p>The CentOS container has started and the bash shell is open.</p> <pre><code>ls /etc\n</code></pre> <pre><code>cat /etc/bash.bashrc\n</code></pre> <p></p>"},{"location":"guide/05-02/index.html#113-exit-container-shell","title":"11.3. Exit Container Shell","text":"<p>To disconnect from the container type exit.</p> <pre><code>exit\n</code></pre> <p></p>"},{"location":"guide/05-02/index.html#12-clean-up","title":"12. Clean Up","text":""},{"location":"guide/05-02/index.html#121-stop-containers","title":"12.1. Stop containers","text":"<p>List all of the running containers.</p> <pre><code>docker ps \n</code></pre> <p>Stop all nginx containers.</p> <pre><code>docker stop &lt;nginx_container_id&gt;\n</code></pre> <p></p>"},{"location":"guide/05-02/index.html#122-delete-all-nginx-containers","title":"12.2. Delete All nginx Containers","text":"<p>List all containers, running and stop.</p> <pre><code>docker ps -a\n</code></pre> <p>Delete all stopped containers with the prune option.</p> <pre><code>docker container prune\n</code></pre> <p>\u26a0\ufe0f Warning</p> <p>Do not delete the minikube container</p> <p></p>"},{"location":"guide/05-02/index.html#123-verify-clean-up","title":"12.3. Verify Clean Up","text":"<pre><code>docker ps -a\n</code></pre>"},{"location":"guide/05-02/index.html#13-end-of-exercise","title":"13. End of Exercise","text":""},{"location":"guide/05-03/index.html","title":"05-03 Building a Microservice","text":"<p>The purpose of this exercise is to use a Flask to create a simply containerized web application. A Dockerfile is used to create the directives for building the Docker image. And the image is used to start up a Docker container running a simple python application.</p>"},{"location":"guide/05-03/index.html#1-review-the-python-script","title":"1. Review the Python Script","text":""},{"location":"guide/05-03/index.html#11-change-directories","title":"1.1. Change Directories","text":"<pre><code>cd app-frontend\n</code></pre> <pre><code>ls\n</code></pre>"},{"location":"guide/05-03/index.html#12-review-the-python-script","title":"1.2. Review the Python Script","text":"<p>The python script imports Flask and a method render_template. It will set a route to the directory templates and search for index.html. It sets to the localhost at 0.0.0.0.</p> <pre><code>cat app.py\n</code></pre> <p></p>"},{"location":"guide/05-03/index.html#13-review-the-requirments","title":"1.3. Review the Requirments","text":"<p>The requirements file sets packages and libraries. This is a standard file for Python.</p> <pre><code>cat requirements.txt\n</code></pre> <p></p>"},{"location":"guide/05-03/index.html#2-review-the-dockerfile","title":"2. Review the Dockerfile","text":""},{"location":"guide/05-03/index.html#21-review-the-dockerfile","title":"2.1. Review the Dockerfile","text":"<pre><code>cat Dockerfile\n</code></pre> <ul> <li>FROM pulls in a python image running in Alpine Linux</li> <li>RUN updates the pip</li> <li>WORKDIR sets the working directory in the application to /app</li> <li>ADD copies in all of the files from the local directory to /app</li> <li>RUN installs the packages and libraries in the requirements.txt file</li> <li>CMD executes python to run the app.py script</li> </ul>"},{"location":"guide/05-03/index.html#22-description-of-the-dockerfile","title":"2.2. Description of the Dockerfile","text":"<ul> <li>The standard file name is always Dockerfile</li> <li>Dockerfiles builds images in layers</li> <li>The first layer is the base image, which is a pre-build docker image. Generally a application support such as java or an OS, such as Ubuntu or CentOS.</li> <li>The directive FROM will pull a base image from Docker Hub. In this example the node image with a specific image.</li> <li>The directive COPY will copy from a localhost and copy them into the container. In this example both the package.json and the src for the java script are called into a folder called /app.</li> <li>The directive WORKDIR sets a path as the default directory inside of the container. In this example this is the location as the java script and the dependency file.</li> <li>The directive RUN will run a command in a shell inside the container. In this example the npm install to install dependencies.</li> <li>The last directive is always CMD. This is the command to start the application in the container. In this example we are start the node command and passing in the java script.</li> </ul>"},{"location":"guide/05-03/index.html#3-build-an-image","title":"3. Build an Image","text":""},{"location":"guide/05-03/index.html#31-list-images","title":"3.1. List Images","text":"<pre><code>docker images\n</code></pre>"},{"location":"guide/05-03/index.html#32-build-the-image","title":"3.2. Build the Image","text":"<p>Follow the directives of the Dockerfile to see the image layers being built. This Dockerfile directorive begins by downloading node:20-alpine. The -t or --tag option tags the image.</p> <p>docker build -t | --tag tag_name:version path_to_Dockerfile</p> <pre><code>docker build -t app-frontend .\n</code></pre> <p></p>"},{"location":"guide/05-03/index.html#33-list-images","title":"3.3. List images","text":"<p>There are two new images. One for node:20-apline and the other for the Clouddev application.</p> <pre><code>docker images\n</code></pre> <p></p>"},{"location":"guide/05-03/index.html#4-run-a-container","title":"4. Run a Container","text":""},{"location":"guide/05-03/index.html#41-run-the-container","title":"4.1. Run the Container","text":"<p>This syntax will start a container named app-frontend. The name of the container is app-frontend. It maps host port 8040 to the container port for Flask, which is 5000. The Docker image is also app-frontend.</p> <pre><code>docker run -d --name app-frontend -p 8040:5000 app-frontend\n</code></pre> <p></p>"},{"location":"guide/05-03/index.html#42-list-running-containers","title":"4.2. List Running Containers","text":"<pre><code>docker ps\n</code></pre>"},{"location":"guide/05-03/index.html#43-review-logs","title":"4.3. Review Logs","text":"<pre><code>docker logs app-frontend\n</code></pre>"},{"location":"guide/05-03/index.html#5-verify-the-web-app","title":"5. Verify the Web App","text":""},{"location":"guide/05-03/index.html#51-check-the-url","title":"5.1. Check the URL","text":"<p>Open a tab on Firefox and insert this URL.</p> <pre><code>http://localhost:8040\n</code></pre> <p></p>"},{"location":"guide/05-03/index.html#6-clean-up","title":"6. Clean Up","text":""},{"location":"guide/05-03/index.html#61-remove-container","title":"6.1. Remove Container","text":"<pre><code>docker ps\n</code></pre> <pre><code>docker stop app-frontend\n</code></pre> <pre><code>docker rm app-frontend\n</code></pre>"},{"location":"guide/05-03/index.html#62-remove-image","title":"6.2. Remove Image","text":"<pre><code>docker images\n</code></pre> <pre><code>docker rmi app-frontend\n</code></pre>"},{"location":"guide/05-03/index.html#63-change-directories","title":"6.3. Change Directories","text":"<pre><code>cd ..\n</code></pre>"},{"location":"guide/05-03/index.html#7-end-of-exercise","title":"7. End of Exercise","text":""},{"location":"guide/06-01/index.html","title":"06-01 Architecture for Kubernetes Resources","text":""},{"location":"guide/06-01/index.html#1-introduction","title":"1. Introduction","text":""},{"location":"guide/06-01/index.html#2-kubernetes-resources","title":"2. Kubernetes Resources","text":""},{"location":"guide/06-01/index.html#3-namespaces","title":"3. Namespaces","text":""},{"location":"guide/06-01/index.html#4-pods","title":"4. Pods","text":""},{"location":"guide/06-01/index.html#5-deployments","title":"5. Deployments","text":""},{"location":"guide/06-01/index.html#6-summary-and-exercise-assignments","title":"6. Summary and Exercise Assignments","text":""},{"location":"guide/06-02/index.html","title":"06-02 Architecture for Kubernetes Resources","text":""},{"location":"guide/06-02/index.html#1-introduction","title":"1. Introduction","text":""},{"location":"guide/06-02/index.html#2-kubernetes-resources","title":"2. Kubernetes Resources","text":""},{"location":"guide/06-02/index.html#3-configurations","title":"3. Configurations","text":""},{"location":"guide/06-02/index.html#4-networking","title":"4. Networking","text":""},{"location":"guide/06-02/index.html#5-access-control","title":"5. Access Control","text":""},{"location":"guide/06-02/index.html#6-storage","title":"6. Storage","text":""},{"location":"guide/06-02/index.html#7-summary-and-exercise-assignments","title":"7. Summary and Exercise Assignments","text":""},{"location":"guide/06-03/index.html","title":"06-03 Running Pods and Services","text":"<p>The purpose of this exercise is to run Kubernetes CLI tools, which includes kubectl and K9s.</p>"},{"location":"guide/06-03/index.html#1-review-kubectl-usage","title":"1. Review kubectl Usage","text":""},{"location":"guide/06-03/index.html#11-review-kubectl-usage","title":"1.1. Review kubectl Usage","text":"<p>Page through the help pages to overview kubectl. Identify the basic commands, such as get, edit, explain and delete. Take your time to review the help page. </p> <pre><code>kubectl --help \n</code></pre> <p></p>"},{"location":"guide/06-03/index.html#12-review-kubectl-usage","title":"1.2. Review kubectl Usage","text":"<p>The kubectl command has excellent usage statements. Review this usage statement to review the various example commands. Review this page for the syntax. </p> <pre><code>kubectl get pods -h\n</code></pre> <p></p>"},{"location":"guide/06-03/index.html#2-kubernetes-pods","title":"2. Kubernetes Pods","text":""},{"location":"guide/06-03/index.html#21-view-all-pods","title":"2.1. View All Pods","text":"<p>Get a list of all Pods in the Default namespace. Currently there are no Pods running.</p> <pre><code>kubectl get pods\n</code></pre> <p></p>"},{"location":"guide/06-03/index.html#22-start-a-pod","title":"2.2. Start a Pod","text":"<p>Start up a Pod for nginx. The option --image has a default to pull from Docker Hub.</p> <pre><code>kubectl run web-pod --image nginx\n</code></pre> <pre><code>kubectl get pods\n</code></pre> <p></p>"},{"location":"guide/06-03/index.html#23-inspect-the-pod","title":"2.3. Inspect the Pod","text":"<p>Inspect the Pod. Scan through to find the image name. What Port is assigned to the Pod?</p> <pre><code>kubectl describe pod web-pod\n</code></pre> <p></p>"},{"location":"guide/06-03/index.html#24-review-the-logs","title":"2.4. Review the Logs","text":"<pre><code>kubectl logs web-pod\n</code></pre>"},{"location":"guide/06-03/index.html#25-start-another-pod","title":"2.5. Start another Pod","text":"<p>Start up a Pod for postgresql</p> <pre><code>kubectl run db-pod --image postgres\n</code></pre> <pre><code>kubectl get pods\n</code></pre> <p></p>"},{"location":"guide/06-03/index.html#26-list-pods-for-more-details","title":"2.6. List Pods for More Details","text":"<p>Get a list of pods using the -o wide option.</p> <pre><code>kubectl get pods -o wide\n</code></pre> <p></p>"},{"location":"guide/06-03/index.html#27-view-logs","title":"2.7. View Logs","text":"<p>A CrashLoopBackOff is a failure to start. View the logs for the failing Pod.</p> <pre><code>kubectl logs pgsql-pod\n</code></pre> <p>This container needs configuration. It needs to have a ConfigMap with input variables.</p> <p></p>"},{"location":"guide/06-03/index.html#28-delete-a-pod","title":"2.8. Delete a Pod","text":"<p>There is an issue with one of the Pods. Delete the Pod in CrashLoopBackOff.</p> <pre><code>kubectl delete pod db-pod\n</code></pre> <pre><code>kubectl get pods\n</code></pre> <p></p>"},{"location":"guide/06-03/index.html#3-kubernetes-services","title":"3. Kubernetes Services","text":""},{"location":"guide/06-03/index.html#31-review-the-usage-for-service","title":"3.1. Review the Usage for service","text":"<p>Review the usage statement for the service.</p> <pre><code>kubectl expose -h\n</code></pre> <p></p>"},{"location":"guide/06-03/index.html#32-expose-the-ports-for-nginx","title":"3.2. Expose the ports for nginx","text":"<p>The expose command will assign ports to allow access to nginx. Port 80 is for the nginx Pod and the target port opens a listener on the host.</p> <pre><code>kubectl expose pod web-pod --name web-svc --type NodePort --port 8080\n</code></pre> <pre><code>kubectl get svc\n</code></pre> <p></p>"},{"location":"guide/06-03/index.html#33-set-up-service-tunnel","title":"3.3. Set up service tunnel","text":"<p>Minikube is running as a Docker container. The service is only exposed from the Pod to the minikube container. A service tunnel must be opened to allow access from the local host to the minikube container. The terminal needs to remain open to access the url. The --url will output the correct URL to access the web page.</p> <pre><code>minikube service web-svc --url\n</code></pre> <p></p>"},{"location":"guide/06-03/index.html#34-verify-access-to-nginx","title":"3.4. Verify Access to nginx","text":"<p>Open a tab in your browser and verify access to nginx. Copy and paste in the URL.</p> <pre><code>http://localhost:8888\n</code></pre> <p>When done close the tab.</p> <p></p>"},{"location":"guide/06-03/index.html#35-exit-the-url","title":"3.5. Exit the URL","text":"<p>Exit the minikube service tunnel with Ctrl-C.</p> <p></p>"},{"location":"guide/06-03/index.html#4-load-a-microservice-in-minikube","title":"4. Load a Microservice in minikube","text":"<p>These steps will build the app-frontend image and then load the image into minikube.</p>"},{"location":"guide/06-03/index.html#41-change-directory-to-dockerfile","title":"4.1. Change Directory to Dockerfile","text":"<pre><code>cd app-flask\n</code></pre>"},{"location":"guide/06-03/index.html#42-build-an-image","title":"4.2. Build an Image","text":"<pre><code>docker build -t app-flask .\n</code></pre>"},{"location":"guide/06-03/index.html#43-list-images","title":"4.3. List Images","text":"<pre><code>docker images\n</code></pre>"},{"location":"guide/06-03/index.html#44-change-directory","title":"4.4. Change Directory","text":"<pre><code>cd ..\n</code></pre>"},{"location":"guide/06-03/index.html#45-verify-docker-enviroment","title":"4.5. Verify Docker Enviroment","text":"<pre><code>minikube docker-env\n</code></pre>"},{"location":"guide/06-03/index.html#46-set-variable","title":"4.6. Set Variable","text":"<pre><code>eval $(minikube docker-env)\n</code></pre>"},{"location":"guide/06-03/index.html#47-load-image-into-minikube","title":"4.7. Load Image into Minikube","text":"<pre><code>minikube image load app-flask\n</code></pre> <pre><code>minikube image list\n</code></pre>"},{"location":"guide/06-03/index.html#5-kubernetes-namespaces","title":"5. Kubernetes Namespaces","text":""},{"location":"guide/06-03/index.html#51-view-namespaces","title":"5.1. View Namespaces","text":"<p>View all of the current namespaces.</p> <pre><code>kubectl get namespaces\n</code></pre> <p></p>"},{"location":"guide/06-03/index.html#52-create-a-namespace","title":"5.2. Create a Namespace","text":"<pre><code>kubectl create namespace app\n</code></pre> <pre><code>kubectl get namespaces\n</code></pre>"},{"location":"guide/06-03/index.html#53-list-the-resources-in-the-namespace","title":"5.3. List the Resources in the Namespace","text":"<pre><code>kubectl -n app get all\n</code></pre>"},{"location":"guide/06-03/index.html#6-run-the-microservice-in-the-namespace","title":"6. Run the Microservice in the Namespace","text":""},{"location":"guide/06-03/index.html#61-create-a-pod-with-the-image","title":"6.1. Create a Pod with the Image","text":"<pre><code>kubectl -n app run flask-pod --image app-flask --image-pull-policy=Never --port 5000 \n</code></pre> <ul> <li>Namespace: app</li> <li>Pod name: app-pod</li> <li>Image: app-frontend</li> <li>Image pull policy: pulls the image from the local cache and not the default Docker Hub</li> <li>Port: opens the application port</li> </ul> <pre><code>kubectl -n app get pods\n</code></pre>"},{"location":"guide/06-03/index.html#62-inspect-the-pod","title":"6.2. Inspect the Pod","text":"<pre><code>kubectl -n app describe pod flask-pod\n</code></pre> <p>Find the Port for the Pod. It should be set to 5000/TCP.</p> <p></p>"},{"location":"guide/06-03/index.html#63-create-the-service-for-the-pod","title":"6.3. Create the Service for the Pod","text":"<pre><code>kubectl -n app expose pod flask-pod --name flask-svc --type NodePort --port 5000 --target-port 5000\n</code></pre> <ul> <li>Namespace: app</li> <li>Pod: app-pod</li> <li>Service Name: app-svc</li> <li>Type: NodePort, this will open a listener on the minikube host</li> <li>Port: The port for the service</li> <li>Target Port: The port for the Pod</li> </ul> <pre><code>kubectl -n app get all\n</code></pre>"},{"location":"guide/06-03/index.html#64-inspect-the-service","title":"6.4. Inspect the Service","text":"<pre><code>kubectl -n app describe svc flask-svc\n</code></pre>"},{"location":"guide/06-03/index.html#65-list-the-endpoint","title":"6.5. List the Endpoint","text":"<p>This is the IP address and Port number to reach the app service. The app service will route forward to the Pod.</p> <pre><code>kubectl -n app get endpoints\n</code></pre> <p></p>"},{"location":"guide/06-03/index.html#66-open-the-minikube-service-tunnel","title":"6.6. Open the minikube Service Tunnel","text":"<pre><code>minikube -n app service flask-svc --url\n</code></pre> <p>Copy the http://IP_address:Port Number to a clipboard.</p> <p></p>"},{"location":"guide/06-03/index.html#67-verify-access-to-the-application","title":"6.7. Verify Access to the Application","text":"<p>http://IP_address:Port_Number</p> <p></p>"},{"location":"guide/06-03/index.html#7-clean-up","title":"7. Clean Up","text":""},{"location":"guide/06-03/index.html#71-clean-up","title":"7.1. Clean Up","text":"<p>Close the tab on the browser.</p> <p>Use Ctrl-C to exit the minikube service tunnel. Leave the application running.</p> <p>\u2139\ufe0f Leave the Pods Running</p> <p></p>"},{"location":"guide/06-03/index.html#8-end-of-exercise","title":"8. End of Exercise","text":""},{"location":"guide/06-04/index.html","title":"06-04 Inspecting the Kubernetes Dashboard","text":"<p>The purpose of this exercise is to tour the Kubernetes web UI and to learn the functions for reviewing logs in pods.</p>"},{"location":"guide/06-04/index.html#1-open-kubernetes-dashboard","title":"1. Open Kubernetes Dashboard","text":""},{"location":"guide/06-04/index.html#11-open-the-kubernetes-dashboard","title":"1.1. Open the Kubernetes Dashboard","text":"<p>The minikube dashboard command will automatically open a web page to the Kubernetes Dashboard. You must leave the terminal open.</p> <pre><code>minikube dashboard\n</code></pre> <p></p>"},{"location":"guide/06-04/index.html#12-review-the-kubernetes-dashboard","title":"1.2. Review the Kubernetes Dashboard","text":"<p>The dashboard opens to the default namespace. It shows an overview of the current workloads. Here we observe two Pods are running. You may have different Pods open.</p> <p></p>"},{"location":"guide/06-04/index.html#13-review-the-menu","title":"1.3. Review the Menu","text":"<p>The left hand column is a list of filters for the Kubernetes resources. Scroll update down this list. Many were discussed in the class.</p> <p></p>"},{"location":"guide/06-04/index.html#2-open-a-namespace","title":"2. Open a Namespace","text":""},{"location":"guide/06-04/index.html#21-open-a-namespace","title":"2.1. Open a Namespace","text":"<p>Click on default. </p> <p></p>"},{"location":"guide/06-04/index.html#22-search-for-a-namespace","title":"2.2. Search for a Namespace","text":"<p>This lists all of the current running namespaces. </p> <p>Select the namespace for app.</p> <p></p>"},{"location":"guide/06-04/index.html#3-inspect-the-pod","title":"3. Inspect the Pod","text":""},{"location":"guide/06-04/index.html#31-select-the-pod","title":"3.1. Select the Pod","text":"<p>Click on web-pod.</p> <p></p>"},{"location":"guide/06-04/index.html#32-inspect-the-pod-details","title":"3.2. Inspect the Pod Details","text":"<p>Scroll up and down the page to inspect the Pod details.  At the bottom of the page you will find the information about the container.</p> <p></p>"},{"location":"guide/06-04/index.html#33-open-the-logs","title":"3.3. Open the Logs","text":"<p>Click on the icon for logs. </p> <p></p>"},{"location":"guide/06-04/index.html#34-review-the-logs","title":"3.4. Review the Logs","text":"<p>Review the logs. When done click on the breadcrum trail to return to flask-pod.</p> <p></p>"},{"location":"guide/06-04/index.html#35-open-the-shell","title":"3.5. Open the Shell","text":"<p>Click on the icon for shell. If a terminal is available this will execute into the pod.</p> <p></p>"},{"location":"guide/06-04/index.html#36-inspect-the-container","title":"3.6. Inspect the Container","text":"<pre><code>ls\n</code></pre> <pre><code>cat Dockerfile\n</code></pre> <p>When done click on the breadcrum trail to return to flask-pod.</p> <p></p>"},{"location":"guide/06-04/index.html#37-open-the-configuration","title":"3.7. Open the Configuration","text":"<p>Click on the icon for configuration. This will display the mainifest file for the Pod.</p> <p></p>"},{"location":"guide/06-04/index.html#38-review-the-manifest-file","title":"3.8. Review the Manifest File","text":"<p>The mainifest file can be formatted as YAML or as JSON. Review both.</p> <p>Click Cancel when done.</p> <p></p>"},{"location":"guide/06-04/index.html#4-inspect-the-service","title":"4. Inspect the Service","text":""},{"location":"guide/06-04/index.html#41-open-services","title":"4.1. Open Services","text":"<p>Click on Services in the left hand table of contents. Click on the flask-svc.</p> <p></p>"},{"location":"guide/06-04/index.html#42-review-the-service","title":"4.2. Review the Service","text":"<p>Scroll up and down the page to review the service. </p> <p></p>"},{"location":"guide/06-04/index.html#43-review-the-service-mainifest-file","title":"4.3. Review the Service Mainifest File","text":"<p>Click on the edit icon to review the manifest file. Scroll to find the properties for the ports. The host port is set to 32082, the service port is set to 8040, and the Pod port is set to 5000.</p> <p>Click Cancel when done.</p> <p></p>"},{"location":"guide/06-04/index.html#5-clean-up","title":"5. Clean Up","text":""},{"location":"guide/06-04/index.html#51-close-dashboard","title":"5.1. Close Dashboard","text":"<p>Close the tab for the Kubernetes Dashboard. Use Ctrl-C to exit the minikube service tunnel.</p> <p></p>"},{"location":"guide/06-04/index.html#52-delete-flask-pods-and-services","title":"5.2. Delete Flask Pods and Services","text":"<p>\u26a0\ufe0f Do Not Delete service/kubernetes</p> <p>Do not delete the service kubernetes in the default namespace.</p> <pre><code>kubectl -n app delete pod flask-pod\n</code></pre> <pre><code>kubectl -n app delete service flask-svc\n</code></pre> <p></p>"},{"location":"guide/06-04/index.html#53-delete-web-pods-and-services","title":"5.3. Delete Web Pods and Services","text":"<pre><code>kubectl delete pod web-pod\n</code></pre> <pre><code>kubectl delete service web-svc\n</code></pre>"},{"location":"guide/06-04/index.html#6-end-of-exercise","title":"6. End of Exercise","text":""},{"location":"guide/07-01/index.html","title":"07-01 Kubernetes Manifest Files","text":""},{"location":"guide/07-01/index.html#1-introduction","title":"1. Introduction","text":""},{"location":"guide/07-01/index.html#2-yaml-files","title":"2. YAML Files","text":""},{"location":"guide/07-01/index.html#3-manifest-files","title":"3. Manifest Files","text":""},{"location":"guide/07-01/index.html#4-imperative-vs-declarative","title":"4. Imperative vs Declarative","text":""},{"location":"guide/07-01/index.html#5-summary-and-exercise-assignments","title":"5. Summary and Exercise Assignments","text":""},{"location":"guide/07-02/index.html","title":"07-02 Creating Deployments","text":""},{"location":"guide/07-02/index.html#1-set-up-the-ingress-controller","title":"1. Set Up the Ingress Controller","text":""},{"location":"guide/07-02/index.html#11-add-the-ingress-controller","title":"1.1. Add the Ingress Controller","text":"<p>An Ingress Controller is required to route traffic according to the rules in the Ingress.</p> <pre><code>minikube addons enable ingress\n</code></pre> <p></p>"},{"location":"guide/07-02/index.html#2-apply-the-deployment","title":"2. Apply the Deployment","text":""},{"location":"guide/07-02/index.html#21-change-directory","title":"2.1. Change directory","text":"<pre><code>cd web-deploy\n</code></pre> <pre><code>ls\n</code></pre>"},{"location":"guide/07-02/index.html#22-open-the-deployment","title":"2.2. Open the Deployment","text":"<pre><code>cat web-deploy.yaml\n</code></pre> <p>If you are using a IDE then open the file in the editor.</p> <p></p>"},{"location":"guide/07-02/index.html#23-review-the-deployment","title":"2.3. Review the Deployment","text":"<p>The required properties are all present:</p> <ul> <li>apiVersion: apps/v1</li> <li>kind: Deployment</li> <li>metadata:</li> <li>spec:</li> </ul> <p>Additional properties</p> <ul> <li>name: web-deploy</li> <li>namespace: app</li> <li>labels: name and tier</li> </ul> <p>The spec sets a template which includes the spec for the container.</p> <ul> <li>Pod name: web-pod</li> <li>name: web-nginx</li> <li>image: nginx</li> <li>ports: http:80</li> </ul> <p></p>"},{"location":"guide/07-02/index.html#24-apply-the-deployment","title":"2.4. Apply the Deployment","text":"<pre><code>kubectl apply -f web-deploy.yaml\n</code></pre> <pre><code>kubectl -n app get all\n</code></pre>"},{"location":"guide/07-02/index.html#3-apply-the-service","title":"3. Apply the Service","text":""},{"location":"guide/07-02/index.html#31-open-the-service","title":"3.1. Open the Service","text":"<pre><code>cat web-svc.yaml\n</code></pre> <p>If you are using a IDE then open the file in the editor.</p> <p></p>"},{"location":"guide/07-02/index.html#32-review-the-service","title":"3.2. Review the Service","text":"<p>The required properties are all present:</p> <ul> <li>apiVersion: v1</li> <li>kind: Service</li> <li>metadata:</li> <li>spec:</li> </ul> <p>Additional properties</p> <ul> <li>name: app-srv</li> <li>namespace: app</li> <li>ports: map the container port 80, be default the service port will also be 80</li> </ul> <p></p>"},{"location":"guide/07-02/index.html#33-apply-the-server","title":"3.3. Apply the Server","text":"<pre><code>kubectl apply -f web-svc.yaml\n</code></pre> <pre><code>kubectl -n app get all\n</code></pre>"},{"location":"guide/07-02/index.html#4-test-service","title":"4. Test Service","text":""},{"location":"guide/07-02/index.html#41-open-a-minikube-service-tunnel","title":"4.1. Open a minikube Service Tunnel","text":"<pre><code>minikube -n app service web-svc --url\n</code></pre>"},{"location":"guide/07-02/index.html#42-validate-access","title":"4.2. Validate Access","text":"<p>Copy and paste the URL into a tab in the browser. You can also use the curl command from another terminal.</p> <pre><code>curl http://127.0.0.1:55017\n</code></pre> <p></p>"},{"location":"guide/07-02/index.html#43-close-the-minikube-service-tunnel","title":"4.3. Close the minikube Service Tunnel","text":"<p>Use Ctrl-C to exit the minikube service tunnel.</p> <p></p>"},{"location":"guide/07-02/index.html#5-apply-the-ingress","title":"5. Apply the Ingress","text":""},{"location":"guide/07-02/index.html#51-open-the-ingress","title":"5.1. Open the Ingress","text":"<pre><code>cat web-ing.yaml\n</code></pre> <p>If you are using a IDE then open the file in the editor.</p> <p></p>"},{"location":"guide/07-02/index.html#52-review-the-ingress","title":"5.2. Review the Ingress","text":"<p>The required properties are all present:</p> <ul> <li>apiVersion: networking.k8s.io/v1</li> <li>kind: Ingress</li> <li>metadata:</li> <li>spec:</li> </ul> <p>Additional properties</p> <ul> <li>name: app-ing</li> <li>namespace: app</li> </ul> <p>Setting rules</p> <ul> <li>host: identify the hostname</li> <li>path: the path within the html</li> <li>backend service: identify the service the ingress is mapping for routing</li> </ul> <p></p>"},{"location":"guide/07-02/index.html#53-apply-the-ingress","title":"5.3. Apply the Ingress","text":"<pre><code>kubectl apply -f web-ing.yaml\n</code></pre> <pre><code>kubectl -n app get ingress web-ing\n</code></pre>"},{"location":"guide/07-02/index.html#6-test-ingress","title":"6. Test Ingress","text":"<p>Close the browser tab and use Ctrl-C to close the minikube tunnel. Leave the application running.</p>"},{"location":"guide/07-02/index.html#61-open-the-minikube-tunnel","title":"6.1. Open the minikube tunnel","text":"<pre><code>minikube tunnel\n</code></pre> <p>Notice the line \"Starting tunnel for service app-ing\".</p> <p></p>"},{"location":"guide/07-02/index.html#62-verify-with-curl","title":"6.2. Verify with curl","text":"<pre><code>curl --resolve \"app.example:80:127.0.0.1\" -i http://app.example\n</code></pre> <p>The --resolve option is mapping app.example:80 to the localhost IP address of 127.0.0.1 and then passing in http://app/example</p> <p>The ingress is accepting this host URL and mapping it to the service app-svc.</p> <p></p>"},{"location":"guide/07-02/index.html#63-test-in-browser-optional","title":"6.3. Test in Browser (Optional)","text":"<p>\u26a0\ufe0f Remove the Host Entry</p> <p>When you are done with this exercise do not forget to remove this entry from your /etc/hosts file.</p> <p>\u2139\ufe0f Require sudo Access to root</p> <p>This step requires sudo access to root. If you do not have this access you can not perform this step.</p> <p>If you have access to root on your laptop you can make a change to the /etc/hosts file to verify access from the browser.</p> <pre><code>sudo echo 127.0.0.1 app.example  &gt;&gt; /etc/hosts\n</code></pre> <p></p>"},{"location":"guide/07-02/index.html#64-validate-in-the-browser","title":"6.4. Validate in the Browser","text":"<pre><code>http://app.example\n</code></pre>"},{"location":"guide/07-02/index.html#clean-up","title":"Clean Up","text":""},{"location":"guide/07-02/index.html#7-end-of-exercise","title":"7. End of Exercise","text":""},{"location":"guide/08-01/index.html","title":"08-01 Architecture for Kubernetes Cluster","text":""},{"location":"guide/08-01/index.html#1-introduction","title":"1. Introduction","text":""},{"location":"guide/08-01/index.html#2-node-architecture","title":"2. Node Architecture","text":""},{"location":"guide/08-01/index.html#3-cluster-components","title":"3. Cluster Components","text":""},{"location":"guide/08-01/index.html#4-cluster-networking","title":"4. Cluster Networking","text":""},{"location":"guide/08-01/index.html#5-containerd","title":"5. ContainerD","text":""},{"location":"guide/08-01/index.html#6-summary-and-exercise-assignments","title":"6. Summary and Exercise Assignments","text":""},{"location":"guide/08-02/index.html","title":"08-02 Inspecting a Kubernetes Cluster","text":"<p>The purpose of this exercise is to run Kubernetes CLI tools, which includes kubectl and K9s.</p>"},{"location":"guide/08-02/index.html#1-start-up-a-multi-node-cluster","title":"1. Start Up a Multi Node Cluster","text":""},{"location":"guide/08-02/index.html#11-terminate-minikube-clusters","title":"1.1. Terminate minikube Clusters","text":"<p>Shutdown the 1 node minikube cluster.</p> <pre><code>minikube delete --all\n</code></pre> <p></p>"},{"location":"guide/08-02/index.html#12-start-up-a-multinode-cluster","title":"1.2. Start Up a multinode Cluster","text":"<p>\u26a0\ufe0f Caution Regarding Local Memory</p> <p>A 4 node minikube cluster use slightly over 10 Gigabyte of RAM. If your laptop will not support this you must reduce the node count to 2.</p> <p>Start a 4 node cluster, the -p | --profile flag creates a profile name for this custom cluster</p> <pre><code>minikube start --nodes 4 -p minikube-app\n</code></pre> <p></p>"},{"location":"guide/08-02/index.html#2-inspect-nodes","title":"2. Inspect Nodes","text":""},{"location":"guide/08-02/index.html#21-list-nodes","title":"2.1. List nodes","text":"<pre><code>kubectl get nodes\n</code></pre>"},{"location":"guide/08-02/index.html#22-list-nodes-with-details","title":"2.2. List Nodes with Details","text":"<p>List the nodes but also show the host IP address and the Operating System.</p> <pre><code>kubectl get nodes -o wide\n</code></pre> <p></p>"},{"location":"guide/08-02/index.html#23-inspect-a-node","title":"2.3. Inspect a Node","text":"<pre><code>kubectl describe node minikube-app\n</code></pre>"},{"location":"guide/08-02/index.html#24-check-status-with-minikube","title":"2.4. Check Status with minikube","text":"<pre><code>minikube status -p minikube-app\n</code></pre>"},{"location":"guide/08-02/index.html#3-review-kubelet","title":"3. Review kubelet","text":""},{"location":"guide/08-02/index.html#31-inspect-kubelet","title":"3.1. Inspect kubelet","text":"<p>kubelet is a Linux process running on every node. It is not a container. We can list it to find various configuration files.</p> <pre><code>minikube ssh \"ps aux | grep -i kubelet\" -p minikube-app\n</code></pre> <p></p>"},{"location":"guide/08-02/index.html#32-inspect-ip-tables-for-busy-pod","title":"3.2. Inspect IP tables for busy-pod","text":"<pre><code>minikube ssh \"sudo iptables -L -t nat | grep app-svc\" -p minikube-app\n</code></pre> <p>The output for iptables is a set of rules for routing traffic. nat stands for Network Address Translation. These rules route traffic to 10.244.3.13:80. This is the Pod IP address.</p> <p></p>"},{"location":"guide/08-02/index.html#33-list-pod-ip-address","title":"3.3. List Pod IP Address","text":"<pre><code>kubectl -n app get pods -o wide\n</code></pre>"},{"location":"guide/08-02/index.html#34-list-the-endpoints","title":"3.4. List the endpoints","text":"<pre><code>kubectl -n app get endpoints\n</code></pre>"},{"location":"guide/08-02/index.html#4-add-metrics","title":"4. Add Metrics","text":""},{"location":"guide/08-02/index.html#41-add-the-metrics-server","title":"4.1. Add the Metrics Server","text":"<pre><code>minikube addons enable metrics-server -p minikube-app\n</code></pre>"},{"location":"guide/08-02/index.html#42-list-nodes-for-cpu-and-memory","title":"4.2. List Nodes for CPU and Memory","text":"<pre><code>kubectl top node\n</code></pre>"},{"location":"guide/08-02/index.html#43-list-pods-for-cpu-and-memory","title":"4.3. List Pods for CPU and Memory","text":"<pre><code>kubectl -n kube-system top pod\n</code></pre>"},{"location":"guide/08-02/index.html#5-start-up-deployment","title":"5. Start Up Deployment","text":""},{"location":"guide/08-02/index.html#51-change-directories","title":"5.1. Change Directories","text":"<pre><code>cd app-busy\n</code></pre> <pre><code>ls\n</code></pre>"},{"location":"guide/08-02/index.html#52-display-the-manifest-file","title":"5.2. Display the Manifest File","text":"<pre><code>cat busy-deploy.yaml\n</code></pre> <p>If you are using a IDE then open the file in the editor.</p> <p></p>"},{"location":"guide/08-02/index.html#53-review-the-manifest-file","title":"5.3. Review the Manifest File","text":"<p>There is a lot going on in this manifest file. The important properties are:</p> <ul> <li>replicas is set to 4</li> <li>podAntiAffinity is set to ensure pods will be on separate nodes</li> <li>image is busybox, a lightweight Linux box used for testing</li> <li>command is set to sleep for 60 seconds, the pods will terminate in 1 min after start up</li> </ul> <p>\u2139\ufe0f Restarting the Deployment</p> <p>If you take longer then 60 seconds to complete the tasks you can delete the deployment and run it again. Or you can edit the deployment and increase the sleep count.</p> <p></p>"},{"location":"guide/08-02/index.html#54-start-up-the-deployment","title":"5.4. Start Up the Deployment","text":"<pre><code>kubectl apply -f busy-deploy.yaml\n</code></pre> <pre><code>kubectl get all\n</code></pre>"},{"location":"guide/08-02/index.html#55-inspect-rollout-status","title":"5.5. Inspect Rollout Status","text":"<pre><code>kubectl rollout status deployment.apps/busy-deploy\n</code></pre>"},{"location":"guide/08-02/index.html#6-test-resilience-of-replicas","title":"6. Test Resilience of Replicas","text":""},{"location":"guide/08-02/index.html#61-delete-pods","title":"6.1. Delete Pods","text":"<pre><code>kubectl get pods\n</code></pre> <p>Use copy and paste to copy in two pods to the delete command.</p> <pre><code>kubectl delete pods busy-deploy-545467b6cf-gfv6m busy-deploy-545467b6cf-xgr8p\n</code></pre> <p></p>"},{"location":"guide/08-02/index.html#62-list-the-pods","title":"6.2. List the Pods","text":"<p>Notice RESTARTS is now 1 for two of the Pods. The Pod NAME has also changed for two of the Pods. This demonstrates resilience.</p> <pre><code>kubectl get pods\n</code></pre> <p></p>"},{"location":"guide/08-02/index.html#7-change-the-replicas","title":"7. Change the Replicas","text":""},{"location":"guide/08-02/index.html#71-change-the-replica-count","title":"7.1. Change the Replica Count","text":"<pre><code>kubectl scale deployment busy-deploy --replicas 2\n</code></pre> <pre><code>kubectl get pods\n</code></pre>"},{"location":"guide/08-02/index.html#8-end-of-exercise","title":"8. End of Exercise","text":""},{"location":"guide/09-01/index.html","title":"09-01 Architecture for Kubernetes Control Plane","text":""},{"location":"guide/09-01/index.html#1-introduction","title":"1. Introduction","text":""},{"location":"guide/09-01/index.html#2-control-plane","title":"2. Control Plane","text":""},{"location":"guide/09-01/index.html#3-kube-apiserver","title":"3. kube-apiserver","text":""},{"location":"guide/09-01/index.html#4-kube-controller-manager","title":"4. kube-controller-manager","text":""},{"location":"guide/09-01/index.html#5-kube-scheduler","title":"5. kube-scheduler","text":""},{"location":"guide/09-01/index.html#6-etcd","title":"6. etcd","text":""},{"location":"guide/09-01/index.html#7-control-plane-configuration-files","title":"7. Control Plane Configuration Files","text":""},{"location":"guide/09-01/index.html#8-control-plane-operations","title":"8. Control Plane Operations","text":""},{"location":"guide/09-01/index.html#9-summary-and-exercise-assignments","title":"9. Summary and Exercise Assignments","text":""},{"location":"guide/09-02/index.html","title":"09-02 Inspecting the Control Plane","text":""},{"location":"guide/09-02/index.html#1-inspect-the-pods-in-kube-system","title":"1. Inspect the Pods in kube-system","text":"<p>The namespace kube-system contains all of the Pods for the control plane.</p> <pre><code>kubectl -n kube-system get pods\n</code></pre> <p>Locate:</p> <ul> <li>api-server</li> <li>controller-manager</li> <li>scheduler</li> <li>etcd</li> </ul> <p></p>"},{"location":"guide/09-02/index.html#2-display-the-minikube-profile","title":"2. Display the minikube Profile","text":"<pre><code>minikube profile list\n</code></pre>"},{"location":"guide/09-02/index.html#3-list-the-manifest-directory","title":"3. List the Manifest Directory","text":"<p>List the manifest directory for the control plane. The control plane pods are unique in that they are static pods. They are started directly by kubelet. </p> <pre><code>minikube ssh -p minikube-app -- \"ls /etc/kubernetes/manifests\"\n</code></pre> <p></p>"},{"location":"guide/09-02/index.html#4-display-the-api-server-manifest-file","title":"4. Display the api-server Manifest File","text":"<pre><code>minikube ssh -p minikube-app -- \"sudo cat /etc/kubernetes/manifests/kube-apiserver.yaml\"\n</code></pre>"},{"location":"guide/09-02/index.html#5-end-of-exercise","title":"5. End of Exercise","text":""},{"location":"guide/10-01/index.html","title":"10-01 Next Steps","text":""},{"location":"guide/10-01/index.html#1-introduction","title":"1. Introduction","text":""},{"location":"guide/10-01/index.html#2-course-summary","title":"2. Course Summary","text":""},{"location":"guide/10-01/index.html#3-next-steps","title":"3. Next Steps","text":""},{"location":"guide/10-01/index.html#4-thank-you","title":"4. Thank you","text":""},{"location":"guide/A-01/index.html","title":"A-01 Cloudera Support","text":"<p>The purpose of this chapter is to summarize Cloudera Support and Professional Services for Cloudera Data Platform.</p>"},{"location":"guide/A-01/index.html#1-deploy-a-cloud-operating-system","title":"1. Deploy a Cloud Operating System","text":""},{"location":"guide/A-01/index.html#2-team-work-wins","title":"2. Team Work Wins","text":""},{"location":"guide/A-01/index.html#3-engagement-model","title":"3. Engagement Model","text":""},{"location":"guide/A-01/index.html#4-supportability","title":"4. Supportability","text":""},{"location":"guide/A-01/index.html#5-summary","title":"5. Summary","text":""},{"location":"guide/A-02/index.html","title":"A-02 Bookmarking Cloudera Resources","text":"<p>The purpose of this exercise is to bookmark Cloudera resources available to a Cloudera Data Platform Administrator. This exercise is best run on your own browser. You will create a folder and bookmark a number of sites for future reference.</p>"},{"location":"guide/A-02/index.html#1-open-your-browser","title":"1. Open Your Browser","text":""},{"location":"guide/A-02/index.html#11-open-create-folder","title":"1.1. Open Create Folder","text":"<p>On your local browser's bookmark bar create a folder.</p> <p></p>"},{"location":"guide/A-02/index.html#12-create-folder","title":"1.2. Create Folder","text":"<p>Create a Cloudera folder in the bookmarks.</p> <pre><code>Cloudera\n</code></pre> <p></p>"},{"location":"guide/A-02/index.html#2-tour-cloudera-resources","title":"2. Tour Cloudera Resources","text":""},{"location":"guide/A-02/index.html#21-bookmark-cloudera-resources","title":"2.1. Bookmark Cloudera Resources","text":"<p>This page is an excellent front end to all of Cloudera web sites of value to the CDP Administrator. Add bookmark:</p> <pre><code>https://www.cloudera.com/users.html\n</code></pre> <p></p>"},{"location":"guide/A-02/index.html#22-tour-demo-videos","title":"2.2. Tour Demo Videos","text":"<p>Click Videos. There are a number of excellent demonstrations of Cloudera products. These two to five minute overviews lead a user through the fundamentals of Cloudera products. Scan the list and select one to follow.</p> <p></p>"},{"location":"guide/A-02/index.html#3-tour-cloudera-documentation","title":"3. Tour Cloudera Documentation","text":""},{"location":"guide/A-02/index.html#31-bookmark-cloudera-docs","title":"3.1. Bookmark Cloudera Docs","text":"<p>Cloudera documentation is constantly improving and is a valuable resource. Add bookmark:</p> <pre><code>https://docs.cloudera.com/cdp-private-cloud/latest/index.html\n</code></pre> <p></p>"},{"location":"guide/A-02/index.html#32-review-landing-page","title":"3.2. Review Landing Page","text":"<p>Cloudera documentation is divided into CDP Public Cloud and CDP Private Cloud. This is to account for the Data Services in Private Cloud following a delay in rollout behind CDP Public Cloud. Document is presented with a primary menu in black to the left side. There are two submenus. One on each side of the page.</p> <p></p>"},{"location":"guide/A-02/index.html#33-cdp-private-cloud-base","title":"3.3. CDP Private Cloud Base","text":"<p>Click Private Cloud Base.</p> <p>CDP Private Cloud Base is the CDP Runtime built on hosts or virtual machines. It is the \"base\" for building up Embedded Container Services. Having prior experience and knowledge of CDP Private Cloud Base is a prerequisite for the ADMIN-335 Running Cloudera Private Cloud.</p> <p></p>"},{"location":"guide/A-02/index.html#34-open-installation-instructions","title":"3.4. Open Installation Instructions","text":"<p>Select Getting Started &gt; Installation &gt; Installing CDP Private Cloud Data Services.</p> <p></p>"},{"location":"guide/A-02/index.html#35-review-cdp-private-cloud-data-services","title":"3.5. Review CDP Private Cloud Data Services","text":"<p>Note at the top of the page the version of Data Services. This is version 1.5.0 (latest). Clicking the up arrowhead will present a menu of previous versions.</p> <p>The left hand table of contents presents the primary subjects. A recommended reading to start is the Overview.</p> <p></p>"},{"location":"guide/A-02/index.html#36-open-installing-on-the-embedded-container-service","title":"3.6. Open Installing on the Embedded Container Service","text":"<p>Click Installing on the Embedded Container Service.</p> <p></p>"},{"location":"guide/A-02/index.html#37-scan-through-the-requirements-and-the-installation-pages","title":"3.7. Scan through the Requirements and the Installation Pages","text":"<p>Take a few minutes and scan through pages for requirements and installation. Many of these topics will be discussed in lectures and supported with exercises in this course. Being familiar with documentation is always important to a CDP administrator.</p> <p></p>"},{"location":"guide/A-02/index.html#4-tour-cloudera-blog","title":"4. Tour Cloudera Blog","text":""},{"location":"guide/A-02/index.html#41-open-cloudera-blog","title":"4.1. Open Cloudera Blog","text":"<p>Add bookmark:</p> <pre><code>blog.cloudera.com\n</code></pre> <p>Click Technical.</p> <p></p>"},{"location":"guide/A-02/index.html#42-search-for-data-services","title":"4.2. Search for Data Services","text":"<p>Click checkbox CDP Private Cloud. Search \"data services\". A number of technical articles will be listed. These are curated articles written by technical leadings in Cloudera. Scroll through and select one to read.</p> <p></p>"},{"location":"guide/A-02/index.html#5-tour-cloudera-community","title":"5. Tour Cloudera Community","text":"<p>Cloudera community is where the industry learns, shares, and collaborates on the use of Cloudera products.</p> <p>Click Take a Tour of the Community</p>"},{"location":"guide/A-02/index.html#51-open-cloudera-community","title":"5.1. Open Cloudera Community","text":"<p>Add bookmark:</p> <pre><code>https://community.cloudera.com\n</code></pre> <p></p>"},{"location":"guide/A-02/index.html#52-review-the-tour","title":"5.2. Review the Tour","text":"<p>Step through the size screens for touring Cloudera Community.</p> <p>!!! important    All CDP Administrators should have a single sign on account with Cloudera, in particular to make full use of the community. If you do not have a login please take a moment and subscribe now.</p> <p></p>"},{"location":"guide/A-02/index.html#53-hunting-resolutions","title":"5.3. Hunting Resolutions","text":"<p>Search \"Pods are crashing in namespace\". Select \"Pods are Crashing in Namespace shared-services\"</p> <p></p>"},{"location":"guide/A-02/index.html#6-tour-cloudera-educational-services","title":"6. Tour Cloudera Educational Services","text":""},{"location":"guide/A-02/index.html#61-add-bookmark-for-education","title":"6.1. Add Bookmark for Education","text":"<p>Cloudera Education offers educational paths for every role in the Cloudera Data Lifecycle. This is intended to guide a students career progression in Big Data and in Big Compute. Add bookmark:</p> <pre><code>https://www.cloudera.com/about/training.html\n</code></pre> <p></p>"},{"location":"guide/A-02/index.html#7-tour-cloudera-tutorials","title":"7. Tour Cloudera Tutorials","text":""},{"location":"guide/A-02/index.html#71-add-bookmark","title":"7.1. Add Bookmark","text":"<p>The Cloudera tutorials are interactive short videos to provide an overview of all of Cloudera's products. Add bookmark:</p> <pre><code>https://www.cloudera.com/tutorials.html\n</code></pre> <p></p>"},{"location":"guide/A-02/index.html#8-tour-cloudera-on-youtube","title":"8. Tour Cloudera on YouTube","text":""},{"location":"guide/A-02/index.html#81-subscribe-to-cloudera-on-youtube","title":"8.1. Subscribe to Cloudera on YouTube","text":"<p>Search \"Discover Cloudera Data Platform\". This provides a great overview of CDP.</p> <p>Cloudera recommends for all CDP Administrators to subscribe to this youtube channel. Add bookmark:</p> <pre><code>https://www.youtube.com/@ClouderaInc\n</code></pre> <p></p>"},{"location":"guide/A-02/index.html#9-review-bookmarks","title":"9. Review Bookmarks","text":""},{"location":"guide/A-02/index.html#91-review-the-cloudera-bookmarks","title":"9.1. Review the Cloudera Bookmarks","text":""},{"location":"guide/A-02/index.html#10-end-of-exercise","title":"10. End of Exercise","text":""},{"location":"tests/index.html","title":"Test Suite for Predicting with Cloudera Machine Learning","text":""},{"location":"tests/cml/index.html","title":"Environment Test Suite","text":""}]}